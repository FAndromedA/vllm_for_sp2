nohup: ignoring input
WARNING 02-27 07:41:42 [vllm.py:1403] Current vLLM config is not set.
INFO 02-27 07:41:42 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:41:42 [api_server.py:1351] vLLM API server version 0.13.0
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:41:42 [utils.py:253] non-default args: {'model_tag': '/mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2/', 'port': 8711, 'model': '/mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2/', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 65536, 'enforce_eager': True, 'served_model_name': ['SSE_SWA_MOBA'], 'block_size': 128, 'gpu_memory_utilization': 0.65}
[0;36m(APIServer pid=193648)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:06 [model.py:514] Resolved architecture: SSESWAMoBAForCausalLM
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:06 [model.py:2002] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:06 [model.py:1661] Using max model len 65536
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:07 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:07 [config.py:312] Disabling cascade attention since it is not supported for hybrid models.
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:07 [config.py:439] Setting attention block size to 336 tokens to ensure that attention page size is >= mamba page size.
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:07 [config.py:463] Padding mamba page size by 5.00% to ensure that mamba page size and attention page size are exactly equal.
[0;36m(APIServer pid=193648)[0;0m WARNING 02-27 07:42:07 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:07 [vllm.py:722] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=193648)[0;0m The tokenizer you are loading from '/mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[0;36m(EngineCore_DP0 pid=193965)[0;0m WARNING 02-27 07:42:30 [vllm.py:1403] Current vLLM config is not set.
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:30 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:30 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2/', speculative_config=None, tokenizer='/mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=SSE_SWA_MOBA, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:30 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.119.16.124:49127 backend=nccl
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:30 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:31 [gpu_model_runner.py:3562] Starting to load model /mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2/...
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:33 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=193965)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=193965)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.47s/it]
[0;36m(EngineCore_DP0 pid=193965)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.71s/it]
[0;36m(EngineCore_DP0 pid=193965)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.82s/it]
[0;36m(EngineCore_DP0 pid=193965)[0;0m 
[0;36m(EngineCore_DP0 pid=193965)[0;0m [2026-02-27 07:42:37] INFO modeling_sse_swa_moba.py:519: æ‰€æœ‰æ¨¡åž‹å‚æ•°å‡å·²æˆåŠŸåŠ è½½.
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:37 [default_loader.py:308] Loading weights took 3.70 seconds
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:37 [gpu_model_runner.py:3659] Model loading took 6.7054 GiB memory and 4.961293 seconds
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=16.0, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=253.0, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=0.99609375, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=7.3125, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=99.5, min=3.62396240234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=0.59765625, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=19.625, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=148.0, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=0.76953125, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=13.75, min=0.00019168853759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=64.5, min=5.0067901611328125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=2.78125, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=11.8125, min=6.389617919921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=70.0, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=2.046875, min=0.00159454345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=16.625, min=5.054473876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=65.5, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=3.046875, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=10.25, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=53.75, min=4.8160552978515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=2.375, min=0.0021820068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=15.4375, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=48.75, min=1.9550323486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=2.6875, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=16.625, min=4.100799560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=23.75, min=1.2516975402832031e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=3.109375, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=14.9375, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=33.25, min=1.087784767150879e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=10.25, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=16.625, min=5.698204040527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=56.0, min=5.453824996948242e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=5.0625, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=8.25, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=18.375, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=5.375, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=7.21875, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=43.25, min=4.76837158203125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=6.21875, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=16.375, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=11.125, min=4.976987838745117e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=7.15625, min=0.00020122528076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=14.0, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=31.25, min=4.1484832763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=8.1875, min=0.004791259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=6.875, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=61.75, min=0.00011396408081054688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=13.25, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=7.59375, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=18.5, min=0.0004787445068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=13.25, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=25.0, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=36.75, min=7.390975952148438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=18.125, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=9.9375, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=12.0, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=25.25, min=0.004180908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=9.75, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=11.5625, min=1.800060272216797e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=42.0, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=10.5625, min=7.331371307373047e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=16.5, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=38.75, min=0.01251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=8.5625, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=12.1875, min=4.1425228118896484e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=44.25, min=0.01239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=12.0, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=8.625, min=3.337860107421875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=47.75, min=0.0179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=7.46875, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=21.75, min=4.284083843231201e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=65.5, min=0.031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=6.4375, min=2.7894973754882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=17.875, min=5.841255187988281e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=67.5, min=0.029052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=5.84375, min=1.5497207641601562e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=18.25, min=1.4424324035644531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=97.5, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=5.875, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=11.5625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=97.5, min=0.032958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(2048, 2048), max=8.875, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(2048, 1024), max=26.875, min=7.927417755126953e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(2048, 1024), max=105.0, min=0.04296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(2048, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:38 [gpu_worker.py:375] Available KV cache memory: 44.69 GiB
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:38 [kv_cache_utils.py:1291] GPU KV cache size: 418,320 tokens
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:38 [kv_cache_utils.py:1296] Maximum concurrency for 65,536 tokens per request: 6.35x
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=16.0, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=253.0, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=0.99609375, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=7.3125, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=100.0, min=2.3126602172851562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=0.6015625, min=5.841255187988281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=19.625, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=148.0, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=0.765625, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=13.75, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=64.5, min=1.601874828338623e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=2.78125, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=11.75, min=1.1801719665527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=70.0, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=2.046875, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=16.625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=65.5, min=2.4199485778808594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=3.015625, min=5.3882598876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=10.3125, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=54.0, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=2.375, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=15.375, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=48.75, min=1.621246337890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=2.6875, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=16.625, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=23.75, min=8.821487426757812e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=3.15625, min=0.00016117095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=14.9375, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=33.25, min=1.990795135498047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=10.5625, min=0.00185394287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=16.5, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=56.5, min=5.751848220825195e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=5.15625, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=8.25, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=18.375, min=2.0712614059448242e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=5.3125, min=0.0003757476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=7.34375, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=43.5, min=8.083879947662354e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=6.0625, min=0.002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=16.875, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=11.0625, min=1.5616416931152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=6.90625, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=13.8125, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=31.5, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=8.25, min=0.004852294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=6.75, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=62.75, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=12.3125, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=7.34375, min=0.000728607177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=18.375, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=13.3125, min=0.0113525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=24.125, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=36.75, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=18.0, min=0.007659912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=9.875, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=12.4375, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=25.375, min=0.007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=9.75, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=11.9375, min=1.341104507446289e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=40.25, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=10.375, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=16.875, min=0.00015926361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=38.25, min=0.033447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=8.0, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=12.875, min=3.4570693969726562e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=42.75, min=0.0264892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=12.75, min=7.12275505065918e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=9.0, min=1.3262033462524414e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=49.0, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=7.53125, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=21.0, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=67.5, min=0.01904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=6.4375, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=18.625, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=66.5, min=0.00592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=6.75, min=2.6971101760864258e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=17.25, min=2.849102020263672e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=111.5, min=0.00186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=5.8125, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=10.375, min=2.8014183044433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=109.0, min=0.03369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'NoneType'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(256, 2048), max=8.4375, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(256, 1024), max=24.0, min=5.662441253662109e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(256, 1024), max=101.0, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(256, 2048), max=0.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:38 [core.py:259] init engine (profile, create kv cache, warmup model) took 1.42 seconds
[0;36m(EngineCore_DP0 pid=193965)[0;0m The tokenizer you are loading from '/mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[0;36m(EngineCore_DP0 pid=193965)[0;0m WARNING 02-27 07:42:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=193965)[0;0m INFO 02-27 07:42:39 [vllm.py:722] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [api_server.py:1099] Supported tasks: ['generate']
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [api_server.py:1425] Starting vLLM API server 0 on http://0.0.0.0:8711
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:42:39 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=193648)[0;0m INFO:     Started server process [193648]
[0;36m(APIServer pid=193648)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=193648)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=193648)[0;0m The tokenizer you are loading from '/mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:45:24 [chat_utils.py:590] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=28.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=320.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=2.90625, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=2.90625, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=8.125, min=1.2814998626708984e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=118.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=1.1484375, min=8.404254913330078e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=1.015625, min=1.8924474716186523e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=2.703125, min=7.264316082000732e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=24.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=165.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=2.171875, min=5.21540641784668e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=2.171875, min=2.4557113647460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=15.8125, min=9.775161743164062e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=19.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=85.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=2.921875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=2.015625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=2.265625, min=3.993511199951172e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=15.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=92.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=2.953125, min=3.7670135498046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=2.859375, min=1.430511474609375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=3.78125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=26.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=84.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=3.4375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=3.125, min=1.9222497940063477e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=4.59375, min=1.5974044799804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=81.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=2.796875, min=7.808208465576172e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=2.796875, min=1.3897079043090343e-09
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=3.921875, min=7.987022399902344e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=57.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=3.609375, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=2.734375, min=1.909211277961731e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=9.0, min=2.115964889526367e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=18.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=31.5, min=5.513429641723633e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=4.375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=4.0625, min=2.5779008865356445e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=4.21875, min=3.6656856536865234e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=35.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=5.53125, min=2.0503997802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=4.125, min=7.808208465576172e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=14.75, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=22.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=66.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=4.78125, min=9.953975677490234e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=4.75, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=6.15625, min=2.86102294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=27.5, min=1.519918441772461e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=8.75, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=7.09375, min=5.513429641723633e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=20.625, min=9.611248970031738e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=59.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=8.625, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=6.0, min=5.7220458984375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=10.8125, min=1.2695789337158203e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=24.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=23.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=11.875, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=9.625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=8.625, min=2.2530555725097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=23.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=37.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=10.5, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=8.6875, min=9.47713851928711e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=11.375, min=8.404254913330078e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=69.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=18.0, min=3.8623809814453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=18.0, min=6.079673767089844e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=10.5, min=5.7697296142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=9.0625, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=35.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=18.125, min=3.457069396972656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=18.125, min=7.063150405883789e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=15.1875, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=26.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=41.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=28.0, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=22.75, min=1.3172626495361328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=19.25, min=1.8358230590820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=9.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=28.5, min=1.6298145055770874e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=26.25, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=22.5, min=3.427267074584961e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=28.0, min=3.4809112548828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=11.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=28.75, min=5.960464477539063e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=34.75, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=33.5, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=31.375, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=26.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=53.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=47.75, min=1.3172626495361328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=47.25, min=0.00022220611572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=8.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=31.125, min=3.073364496231079e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=63.5, min=1.2040138244628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=40.5, min=2.753734588623047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=59.0, min=9.965896606445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=23.75, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=21.75, min=9.357929229736328e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=68.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=44.5, min=5.125999450683594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=61.25, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=10.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=26.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=66.5, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=66.5, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=64.0, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=51.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=130.0, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=76.5, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=98.5, min=4.6253204345703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=7.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=34.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=114.5, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=65.5, min=9.760260581970215e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=97.0, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=8.75, min=2.86102294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=34.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=127.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=70.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=206.0, min=8.344650268554688e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(15, 2048), max=13.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(15, 1024), max=36.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(15, 1024), max=76.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(15, 2048), max=60.5, min=9.012222290039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(15, 2048), max=51.5, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=249.0, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.66796875, min=9.870529174804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.515625, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.0004863739013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=103.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6015625, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=2.753734588623047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=1.6450881958007812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=137.0, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.77734375, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=9.655952453613281e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.71875, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=3.9577484130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.5, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.1875, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.4375, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=3.218650817871094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.140625, min=0.0003948211669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.53125, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5, min=0.00016117095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5703125, min=1.1920928955078125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.203125, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.515625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.40625, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=5.811452865600586e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.09375, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=6.198883056640625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.8125, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.65625, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.15625, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.3125, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=0.0002422332763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.59375, min=0.00176239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.875, min=0.0037994384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=8.630752563476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.0003871917724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.00518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.5, min=0.002471923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.000675201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.0, min=0.00555419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0228271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.0159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.003936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=92.5, min=0.0118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.002410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=113.5, min=0.00167083740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.5, min=0.0096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=0.0052490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=244.0, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5703125, min=3.314018249511719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5703125, min=3.314018249511719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.265625, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=110.0, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6171875, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.91015625, min=5.9604644775390625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=133.0, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.88671875, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.984375, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.875, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=1.9073486328125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.21875, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=8.440017700195312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=89.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.609375, min=3.4809112548828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=2.1904706954956055e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=4.315376281738281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.1875, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9453125, min=0.00011301040649414062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9453125, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.4375, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.40625, min=0.00014591217041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=5.221366882324219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.671875, min=0.0004367828369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0625, min=0.0004253387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.1875, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.96875, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.03125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=0.0004482269287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.625, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.1875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.71875, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0625, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.375, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.65625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.000713348388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=0.0003795623779296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.8125, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.6875, min=0.00142669677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=4.7206878662109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.004180908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.375, min=0.0098876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=7.82012939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.00010442733764648438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.00188446044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=2.956390380859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.875, min=0.01611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.017578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.0, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.0064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.5, min=0.007568359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.125, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.0263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=64.0, min=0.00970458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.005096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.125, min=9.5367431640625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.0098876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.0089111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.75, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=324.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.78125, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5703125, min=3.337860107421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=3.0279159545898438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=105.5, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7109375, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.859375, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=141.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0859375, min=1.5497207641601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.96484375, min=2.5391578674316406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.765625, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=8.535385131835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.00010251998901367188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7421875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7578125, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.125, min=1.1265277862548828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=5.930662155151367e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=84.0, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.40625, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=1.5497207641601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8984375, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.71875, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=3.218650817871094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.00022220611572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.546875, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.625, min=2.086162567138672e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=8.106231689453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.625, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=2.637505531311035e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=3.147125244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.28125, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.1875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0004291534423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00188446044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=0.0001735687255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.875, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=2.4080276489257812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.00118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.34375, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.3125, min=0.007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=6.5267086029052734e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.0003337860107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.25, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.00018405914306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.014404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=4.076957702636719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=6.0558319091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.009033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.375, min=0.0118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.25, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.25, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.0, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.000957489013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.00185394287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.00830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.25, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=102.5, min=0.054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=1.1563301086425781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=94.0, min=0.002471923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.125, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=119.0, min=0.0859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=167.0, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.0111083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=314.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.68359375, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=2.5987625122070312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.78125, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=2.372264862060547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.875, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.046875, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=127.0, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9609375, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=1.5497207641601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0625, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.046875, min=7.867813110351562e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=1.7881393432617188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.921875, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8984375, min=8.940696716308594e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.796875, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.546875, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=1.4841556549072266e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.0, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0625, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=1.9788742065429688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.125, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=3.719329833984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.6875, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.515625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=0.00010061264038085938
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0625, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=3.170967102050781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.875, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.625, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5625, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=3.337860107421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=3.8623809814453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.75, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.90625, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.00016307830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.125, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=4.00543212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.0047607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.025146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.25, min=0.0001850128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.625, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.625, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.0086669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=2.658367156982422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.75, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.01019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.0, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.5, min=1.2755393981933594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=79.5, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.5, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.0030670166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=314.0, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6484375, min=8.630752563476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.8125, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=109.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7109375, min=4.0531158447265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.96875, min=1.2516975402832031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5390625, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=132.0, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0546875, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8515625, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5234375, min=0.00016498565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.125, min=2.4199485778808594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=5.412101745605469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.890625, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.890625, min=0.00010395050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=2.7894973754882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=6.031990051269531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.640625, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.53125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.3125, min=6.723403930664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.890625, min=4.00543212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.65625, min=0.000720977783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.5, min=0.000720977783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.8125, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=1.9550323486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.515625, min=1.609325408935547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.00732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.671875, min=0.00384521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.21875, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.00089263916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.625, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.890625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.53125, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.03125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.40625, min=0.00250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.78125, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.00130462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.01068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.002838134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=8.726119995117188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.00177764892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=7.581710815429688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.625, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.00579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=0.010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=112.5, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=84.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=129.0, min=0.0034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.875, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.0223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.004638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.034423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.0026092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.018798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.00141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=244.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.109375, min=1.3053417205810547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=1.1920928955078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=0.00011491775512695312
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=98.5, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.59765625, min=9.012222290039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=4.0531158447265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=9.489059448242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.046875, min=0.00022983551025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=7.420778274536133e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=2.956390380859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3984375, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6796875, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.640625, min=3.4809112548828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.703125, min=5.9604644775390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=1.5348196029663086e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=7.2479248046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7109375, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=2.2530555725097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.796875, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.796875, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=1.7404556274414062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=1.1026859283447266e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.453125, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=8.153915405273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=2.1457672119140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.84375, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.90625, min=0.002777099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=4.291534423828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5625, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.9375, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.25, min=0.000339508056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.0, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.375, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.5, min=0.00360107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.00011730194091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.018798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=9.298324584960938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.004180908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=0.0032196044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.0159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.5, min=0.0125732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.017822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.875, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.020263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.0, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.03662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.5, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=306.0, min=0.0052490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6328125, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=2.014636993408203e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=9.202957153320312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71484375, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.88671875, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=150.0, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.046875, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.046875, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.375, min=1.2814998626708984e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.671875, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2265625, min=3.838539123535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=2.47955322265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.34375, min=0.00162506103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=2.110004425048828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.703125, min=3.0100345611572266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.00167083740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=1.6689300537109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=1.1444091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.796875, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4296875, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.859375, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=8.335337042808533e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.203125, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=2.6702880859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.34375, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.25, min=0.00634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.46875, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=0.00011777877807617188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.75, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=3.933906555175781e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.65625, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.1875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.9375, min=3.314018249511719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=5.698204040527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.00116729736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.75, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.34375, min=0.0027313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.00518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.00921630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.6875, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.0031585693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.004608154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.004302978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=0.01080322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=4.291534423828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.25, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.75, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=105.5, min=0.04150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.01031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=5.698204040527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=62.0, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.5, min=0.0028839111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.5, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.00164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.25, min=0.005584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=296.0, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.55859375, min=0.0004673004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=2.944469451904297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0390625, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0234375, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3046875, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=149.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1015625, min=8.165836334228516e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1015625, min=2.0742416381835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=2.384185791015625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.921875, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2734375, min=0.00011301040649414062
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.453125, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6875, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.3125, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.8125, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=6.9141387939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=3.874301910400391e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.84375, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7734375, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.46875, min=1.2516975402832031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.21875, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.002838134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.8125, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=3.9577484130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=0.00015354156494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.0029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.9375, min=0.0033416748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.9375, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.440017700195312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.5625, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.625, min=0.0033416748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.71875, min=3.838539123535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.3125, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.65625, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.00494384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.0004367828369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.0015411376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.0107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.0033111572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.007568359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=2.6702880859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.048095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.375, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.75, min=0.009033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.01373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=0.00634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=1.6689300537109375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.0106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.875, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.006378173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=79.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.0, min=4.32133674621582e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=84.0, min=0.0133056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=193.0, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.02587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=197.0, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.015625, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0234375, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=117.5, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7890625, min=4.9114227294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.71484375, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0625, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=147.0, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.68359375, min=2.592802047729492e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.84765625, min=2.6106834411621094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6015625, min=5.698204040527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.25, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=2.384185791015625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=0.00015354156494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=92.0, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.484375, min=1.9818544387817383e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.921875, min=5.900859832763672e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.765625, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=1.043081283569336e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7890625, min=0.0003795623779296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.28125, min=0.00135040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3125, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=0.000949859619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.03125, min=5.364418029785156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.00970458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.390625, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=2.5331974029541016e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=7.869675755500793e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.6875, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=5.7220458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.25, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=1.8104910850524902e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.00024127960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.00174713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=1.9550323486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.125, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=4.220008850097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=3.695487976074219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.006317138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=2.9802322387695312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.125, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=4.220008850097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.0, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.00011491775512695312
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.00014781951904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.0, min=0.00823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.014892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=1.811981201171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.0, min=0.021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=102.5, min=0.0196533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.01092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.75, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=104.0, min=0.006988525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.5, min=0.04052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=128.0, min=0.0086669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=8.630752563476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=160.0, min=0.007598876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.0, min=0.0135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.75, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=310.0, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=109.0, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.63671875, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.8715858459472656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=151.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9921875, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.85546875, min=1.3232231140136719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=1.3947486877441406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=1.0907649993896484e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=0.00019359588623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.28125, min=0.000957489013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.484375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=3.382563591003418e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7734375, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7265625, min=3.0994415283203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.890625, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.09375, min=0.0003795623779296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=1.895427703857422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.71875, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.6875, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.25, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=7.331371307373047e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.0, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=9.59634780883789e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.59375, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=2.181529998779297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.3125, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=5.078315734863281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.375, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.5, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.25, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0017852783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.5, min=0.007232666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.0218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.5, min=6.496906280517578e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.625, min=0.003570556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.0098876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=0.0031585693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.00927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=2.384185791015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=105.5, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.0028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.0081787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.006683349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.0, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=102.5, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.0029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.05517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=75.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=149.0, min=0.00732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.5, min=0.0004291534423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=276.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=5.8650970458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=4.4405460357666016e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=2.47955322265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.0, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6640625, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9140625, min=4.5299530029296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2109375, min=1.9669532775878906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=126.5, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.66796875, min=0.0002193450927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.875, min=0.0002193450927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=5.984306335449219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9609375, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1328125, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8671875, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=5.555152893066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.109375, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.03125, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.484375, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.671875, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.9375, min=0.00010395050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=6.0498714447021484e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.875, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.359375, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.484375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.00347900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.34375, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=2.205371856689453e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=3.337860107421875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.34375, min=0.0036163330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0625, min=0.00518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.0003376007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.625, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9550323486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.484375, min=0.000949859619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.000339508056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.001861572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5625, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.125, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.75, min=0.0001735687255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=1.6614794731140137e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.00176239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.01318359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=2.8014183044433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.007232666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.0, min=0.0027313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.75, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.017822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=64.0, min=0.000537872314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.00015354156494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.0301513671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=81.0, min=0.0031585693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0625, min=0.000537872314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0036468505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=270.0, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7421875, min=1.811981201171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4765625, min=1.811981201171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.4375, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.48046875, min=6.341934204101562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=5.0067901611328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.89453125, min=9.000301361083984e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.86328125, min=5.2928924560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1875, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9140625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=5.888938903808594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.34375, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=0.00010728836059570312
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.0625, min=8.285045623779297e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=1.1324882507324219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=2.9355287551879883e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.890625, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.21875, min=1.0728836059570312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=2.8371810913085938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5625, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=2.0742416381835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.65625, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.0, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.078125, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.637505531311035e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.9375, min=0.00012159347534179688
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.78125, min=0.000682830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.21875, min=0.0032196044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.515625, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.890625, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.71875, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.5, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.6875, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=2.682209014892578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.125, min=0.0037689208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=4.315376281738281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.5, min=0.00185394287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=6.0558319091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=0.00021839141845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=3.7670135498046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.007720947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.0, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.004974365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.00823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.01129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0147705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.005828857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.0034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.75, min=0.003570556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=55.0, min=0.003753662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.005218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=92.5, min=5.054473876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.625, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.011474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=105.5, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.034423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=225.0, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.0390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=318.0, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.87109375, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.0, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.63671875, min=1.6689300537109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.87109375, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8515625, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.95703125, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.79296875, min=5.435943603515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.328125, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9140625, min=2.9802322387695312e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8125, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4765625, min=2.205371856689453e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.875, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=1.430511474609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=0.0019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.75, min=6.0558319091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.65625, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=5.412101745605469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.921875, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=4.805624485015869e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.625, min=0.0032958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9765625, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.3125, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.005096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.984375, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.6875, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5625, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.625, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.0001926422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.0625, min=0.00023365020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=4.4889748096466064e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00011539459228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.40625, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=9.894371032714844e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0004482269287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.00024127960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0016632080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0002384185791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.0400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=1.4781951904296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.375, min=0.00555419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.00726318359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=2.110004425048828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=62.0, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.75, min=0.00946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.0, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=82.0, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.04931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=69.5, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.000186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=98.5, min=0.005523681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.1015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=288.0, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.0262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.75, min=0.0032958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4765625, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=101.5, min=0.00089263916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.88671875, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0390625, min=4.744529724121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3984375, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=144.0, min=0.00179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8515625, min=0.00014019012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.78515625, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.171875, min=0.00018405914306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=6.742775440216064e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.86328125, min=7.2479248046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=88.0, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.21875, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.78125, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.453125, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.53125, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.65625, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.703125, min=3.4809112548828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.25, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.40625, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.90625, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0004596710205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.359375, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=2.7120113372802734e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.703125, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.09375, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.00011587142944335938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=3.0994415283203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.953125, min=1.9431114196777344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=9.72747802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.15625, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.125, min=4.744529724121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.21875, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0625, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=4.291534423828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.78125, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=9.775161743164062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.3125, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.25, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.00141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.8125, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.01953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=3.4421682357788086e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.0052490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.875, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=1.5139579772949219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.5, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=1.33514404296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.375, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.125, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.25, min=0.0001735687255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.006439208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=1.9550323486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.002838134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=1.8358230590820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.01068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.01080322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.00135040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=93.0, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.0, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.03125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.00537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=161.0, min=0.0089111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=0.0004024505615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.00811767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=0.00543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=306.0, min=0.00518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6328125, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.5, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6640625, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.921875, min=0.0001010894775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6015625, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.046875, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=0.00014019012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.90625, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.078125, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=94.0, min=0.000972747802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.140625, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=0.00012111663818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.703125, min=0.00018405914306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=9.238719940185547e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.53125, min=1.1980533599853516e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.046875, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.59375, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.28125, min=2.4557113647460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.921875, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.625, min=0.0003948211669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.0, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.0003757476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=2.193450927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=6.258487701416016e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.65625, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.125, min=2.658367156982422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.625, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.25, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.6875, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.000186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.25, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.000972747802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=9.632110595703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.125, min=0.01458740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.005706787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.00909423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.25, min=0.0081787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=94.5, min=0.0086669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.007659912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.25, min=0.003021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.0260009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=125.0, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.00689697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.625, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.041259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=165.0, min=0.0054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.75, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=310.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.296875, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5546875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=107.5, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.78125, min=7.599592208862305e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.87109375, min=3.933906555175781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.28125, min=5.304813385009766e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=133.0, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0, min=3.838539123535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7890625, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.75, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.83984375, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.625, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9453125, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.34375, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.625, min=3.1948089599609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.859375, min=1.6570091247558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.421875, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=0.000690460205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=6.8247318267822266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.21875, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=4.553794860839844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=2.637505531311035e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=0.00020313262939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.125, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=2.0742416381835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.00153350830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.919269561767578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=1.919269561767578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.125, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=2.6464462280273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.5625, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.875, min=0.0068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.00194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.005157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.005706787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.000972747802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.00433349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.25, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.625, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0625, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.0018463134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=0.01348876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=0.00020122528076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.00014019012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.000431060791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.125, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.25, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.125, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.004791259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=6.914138793945312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=136.0, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=93.0, min=0.018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=109.5, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=106.0, min=0.0130615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=199.0, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.037353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.125, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=276.0, min=0.000965118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=1.1861324310302734e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=0.00011587142944335938
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.578125, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.78125, min=9.357929229736328e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3984375, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=145.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.66015625, min=1.8715858459472656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8125, min=1.8715858459472656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.859375, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.0032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.296875, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=7.510185241699219e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.109375, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=5.155801773071289e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=3.6716461181640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.003143310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.59375, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=0.0003337860107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.000690460205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.40625, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=7.063150405883789e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.921875, min=4.124641418457031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.00135040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.90625, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.09375, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0, min=0.00017833709716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.71875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=1.5974044799804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.28125, min=3.1948089599609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=8.285045623779297e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.75, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.71875, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.21875, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.609325408935547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.96875, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.00018405914306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.4375, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=1.329183578491211e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.00018978118896484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.00171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.1875, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.125, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=2.777576446533203e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.5, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.00010967254638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.75, min=0.003143310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.25, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.01458740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=2.8252601623535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.5, min=0.007110595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.000972747802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.01153564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.00162506103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.5, min=0.0257568359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.0, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=2.86102294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.0245361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.25, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=137.0, min=0.00628662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=5.245208740234375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=183.0, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.75, min=0.0186767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.003631591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.90625, min=0.00015926361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=109.5, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6796875, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.76171875, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.46875, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=125.0, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.68359375, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.97265625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4140625, min=6.4373016357421875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.609375, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.8671875, min=5.650520324707031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=4.5299530029296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8984375, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.859375, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.34375, min=2.0742416381835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.921875, min=0.00154876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=5.513429641723633e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.59375, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=1.049041748046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.59375, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=3.7550926208496094e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.25, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.984375, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.00021648406982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.796875, min=0.00159454345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=8.630752563476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=9.107589721679688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.6875, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.5, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0625, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=2.6941299438476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.4375, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=4.935264587402344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.40625, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=0.00173187255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.625, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.00017833709716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.00019168853759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.00176239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.01336669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.75, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=2.3603439331054688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.004974365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.0, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.01544189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.03662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=1.049041748046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.0037994384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5625, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.0260009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=116.0, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=6.723403930664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=110.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8125, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.7265625, min=1.0311603546142578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.93359375, min=2.5153160095214844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=137.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.234375, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.46875, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=1.8849968910217285e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.265625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.609375, min=1.138448715209961e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=2.041459083557129e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.078125, min=2.6464462280273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.515625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.671875, min=4.857778549194336e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.015625, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.90625, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.00130462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.140625, min=0.00016117095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=2.2530555725097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.3125, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.78125, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.765625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=5.364418029785156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.84375, min=0.0003261566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.5, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=3.695487976074219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0, min=0.003021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.75, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.875, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.0728836059570312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=3.147125244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.0625, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=3.409385681152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.125, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.8125, min=0.00250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=0.00011014938354492188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.9375, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.0003795623779296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.75, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.75, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.004638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.00634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.5, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.006500244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=1.621246337890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.5, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0130615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.25, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.5, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=114.0, min=0.00775146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.01019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.25, min=0.0029449462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=122.0, min=0.0030364990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.6875, min=0.00408935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.043701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=144.0, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.043212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.75, min=0.003692626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=250.0, min=0.002960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.703125, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=1.0251998901367188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.00168609619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=116.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.94921875, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.94921875, min=1.4841556549072266e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=130.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.86328125, min=1.1682510375976562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9375, min=1.1682510375976562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7578125, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.65625, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.62890625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6171875, min=5.1975250244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=93.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7890625, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=2.944469451904297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.46875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=1.1324882507324219e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.890625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8203125, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8203125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.609375, min=1.3589859008789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.75, min=0.0004253387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9609375, min=4.5299530029296875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.59375, min=5.841255187988281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.8125, min=0.0003757476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.75, min=0.0037689208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.78125, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=3.1948089599609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0625, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.9375, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.375, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=1.7881393432617188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.000957489013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.25, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=8.225440979003906e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.53125, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=0.00011968612670898438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.0004444122314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.4375, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.75, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.75, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=1.3649463653564453e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=8.58306884765625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.0, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.0250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.00762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.006011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=74.0, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.01300048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=1.1742115020751953e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.5, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.000339508056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=97.0, min=0.0186767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=202.0, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.0, min=0.0033111572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=296.0, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.67578125, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=4.5299530029296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=7.510185241699219e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.5, min=0.0021820068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.875, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.94921875, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.40625, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=131.0, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.73828125, min=2.765655517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.953125, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=1.5735626220703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.00011873245239257812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=3.069639205932617e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.28125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=4.696846008300781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.359375, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=1.3113021850585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0001850128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.0003261566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.078125, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=3.8623809814453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=0.00188446044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=2.491474151611328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.953125, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.15625, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.1875, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5625, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.75, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=0.0004673004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=2.7298927307128906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.0625, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0625, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.125, min=0.0029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.001861572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=4.315376281738281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00010347366333007812
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.375, min=0.0035552978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=3.886222839355469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.00010728836059570312
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.0089111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.125, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.00250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.00537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=3.266334533691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.0279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=125.0, min=0.006317138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.000720977783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=76.5, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=94.5, min=0.0133056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.5, min=1.1563301086425781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=82.0, min=0.006195068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=115.5, min=0.036376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=79.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=141.0, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.25, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.375, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=206.0, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.21875, min=3.9577484130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=1.233816146850586e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=95.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.431640625, min=4.6253204345703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.77734375, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6015625, min=1.7404556274414062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.34375, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5234375, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.34375, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=4.798173904418945e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7578125, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=4.857778549194336e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.1875, min=2.4080276489257812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=2.753734588623047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.890625, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=1.800060272216797e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5703125, min=6.723403930664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.359375, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=2.300739288330078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.734375, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=1.537799835205078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.921875, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.921875, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.00016307830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=2.193450927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=2.3126602172851562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.84375, min=0.005767822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=0.005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.875, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=7.390975952148438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.6875, min=0.0004024505615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.00153350830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.00506591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.00010728836059570312
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=4.673004150390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.00946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.0224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.75, min=0.005523681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.00543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=0.0015411376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.125, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.0216064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.0019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=0.00689697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.0220947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.0673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=188.0, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.75, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.75, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=274.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.62890625, min=1.8715858459472656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=1.8715858459472656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.953125, min=9.5367431640625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=110.0, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.75390625, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=3.0159950256347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=151.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.125, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=1.633167266845703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=8.153915405273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.484375, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9765625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2734375, min=5.7697296142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=87.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9453125, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2734375, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0032196044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.125, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=9.98377799987793e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.00176239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8515625, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.390625, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.796875, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=2.562999725341797e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=6.496906280517578e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.828125, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.71875, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.1875, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00012159347534179688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5, min=0.00014591217041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.84375, min=0.0028839111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=1.4007091522216797e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.625, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.875, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.00019168853759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=1.3232231140136719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.375, min=0.011474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=1.4424324035644531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.0, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=2.8014183044433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=3.933906555175781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.625, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.0032958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.75, min=0.0018463134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.0234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.00830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.5, min=0.0177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0072021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.0, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=97.0, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.0186767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=174.0, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.75, min=0.00543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=298.0, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6953125, min=0.00024318695068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.2265625, min=4.5299530029296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=112.5, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9296875, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=4.100799560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.546875, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=166.0, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.87890625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3203125, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.90625, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=4.976987838745117e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1484375, min=1.2636184692382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.953125, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6640625, min=2.5033950805664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6328125, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.1875, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=9.012222290039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.00193023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.09375, min=4.124641418457031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=2.014636993408203e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.625, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.515625, min=0.0036468505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=5.555152893066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.1875, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.5, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.78125, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.265625, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.9375, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.1875, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.921875, min=0.000339508056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0625, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.5, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.0003376007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.875, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.01025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=9.870529174804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.5, min=0.00738525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0076904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=1.919269561767578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.0142822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.0201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.75, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.00012159347534179688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=103.5, min=0.0306396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.25, min=0.00177764892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.5, min=0.01129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=117.5, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=119.5, min=0.05029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=89.5, min=0.004180908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.00183868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=91.5, min=0.05712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=92.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.75, min=0.000720977783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0172119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=72.5, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.0036163330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=296.0, min=3.0279159545898438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.671875, min=2.4318695068359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=2.4318695068359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=115.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7578125, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9296875, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5390625, min=0.00011873245239257812
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=128.0, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9765625, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9765625, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.484375, min=6.109476089477539e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.46875, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3671875, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.40625, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=9.72747802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.59375, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=1.5139579772949219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=0.00023746490478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=4.935264587402344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=1.2278556823730469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.515625, min=9.489059448242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.890625, min=8.058547973632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.90625, min=0.00179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.8125, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.1875, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.0004482269287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=1.990795135498047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.875, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.125, min=0.0027313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=1.0311603546142578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.875, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.90625, min=0.00173187255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=3.600120544433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0015411376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.75, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.8125, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.25, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.75, min=0.0031890869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.00830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.375, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=2.9921531677246094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.00167083740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.25, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.0, min=9.179115295410156e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.002777099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.0034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0002689361572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.00360107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.75, min=0.000675201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=96.0, min=0.013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.75, min=0.0004444122314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0133056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=92.0, min=0.003875732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.25, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=96.0, min=0.039306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=162.0, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.000942230224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=92.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=218.0, min=0.0654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=5.0067901611328125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=324.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.78125, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=2.4318695068359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.890625, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=109.5, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6875, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.921875, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.390625, min=2.7060508728027344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=134.0, min=0.000675201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9296875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9296875, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.59375, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=5.364418029785156e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0859375, min=1.5974044799804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.0625, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.53125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9375, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9765625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.65625, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9296875, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.890625, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.59375, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=6.723403930664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=5.1975250244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.984375, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=2.7418136596679688e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.6875, min=1.5974044799804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.25, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.921875, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.921875, min=8.940696716308594e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.00182342529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.09375, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.625, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.000949859619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.90625, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.4375, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=9.965896606445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.78125, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.625, min=0.00164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.0625, min=0.005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=1.519918441772461e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.25, min=0.00183868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.125, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=0.000339508056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.0035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.00823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.75, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=62.5, min=0.0032958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.009521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.00018787384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.5, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.02734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.0, min=0.002777099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.005157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=69.5, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=93.0, min=0.00010156631469726562
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.03857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=115.5, min=0.004974365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=91.0, min=0.041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=270.0, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.04345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.0, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=102.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.70703125, min=2.2910535335540771e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=3.0159950256347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5625, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=147.0, min=0.0037994384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.69140625, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6953125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.65625, min=7.420778274536133e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.359375, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.078125, min=0.00010251998901367188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=1.245737075805664e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.921875, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7890625, min=3.147125244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.40625, min=1.2934207916259766e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.125, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=7.152557373046875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=3.8623809814453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.078125, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=5.030632019042969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.9375, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=1.5616416931152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.00017833709716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.421875, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=8.440017700195312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5625, min=0.0001773834228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=7.212162017822266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=4.982948303222656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.21875, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.8835067749023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.78125, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=7.271766662597656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.9375, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.875, min=0.0035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=0.000949859619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=4.2438507080078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.005279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=5.054473876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.003265380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=6.031990051269531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=0.004974365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=91.5, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.00141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=98.5, min=0.00738525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.625, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.00616455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.25, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=109.0, min=0.0054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.006805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=120.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.5, min=0.0164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=209.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.90234375, min=1.3709068298339844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=112.5, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.63671875, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.91015625, min=6.723403930664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.15625, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.81640625, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=8.046627044677734e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.65625, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1328125, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.90625, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=3.075599670410156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=0.00010013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6796875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.6875, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.625, min=0.0003757476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.828125, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.00162506103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.46875, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.640625, min=5.698204040527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9296875, min=0.00017833709716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.0, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.90625, min=2.9653310775756836e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=1.4185905456542969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.390625, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=8.195638656616211e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.5625, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.96875, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.875, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=3.7670135498046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.09375, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.1875, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.875, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=0.00011968612670898438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.0, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=1.1444091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=3.147125244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=0.00823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=9.5367431640625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.0189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.5, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.00469970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.75, min=4.673004150390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.0255126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=92.5, min=9.012222290039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=90.0, min=0.01025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.0250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=142.0, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.125, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=68.5, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.0062255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=0.00567626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=306.0, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.046875, min=2.1219253540039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.046875, min=2.1219253540039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=1.7642974853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=113.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.60546875, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.63671875, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.87890625, min=0.0001773834228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=152.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71875, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.82421875, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5546875, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.00131988525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0234375, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=88.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6640625, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6640625, min=1.2934207916259766e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8984375, min=3.4809112548828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.3125, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=1.0907649993896484e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.40625, min=3.886222839355469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9765625, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.90625, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=2.2172927856445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.8125, min=0.0004444122314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.375, min=2.8014183044433594e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.34375, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.984375, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.40625, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.3125, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.000728607177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=4.7206878662109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.00347900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.00347900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=3.3676624298095703e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.625, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.75, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.007659912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.5, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=8.058547973632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.0036468505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.5, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.005706787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.375, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=9.5367431640625e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.00958251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=7.581710815429688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.00022220611572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.005828857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.0003337860107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=87.0, min=0.00933837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.25, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=197.0, min=0.0269775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.00537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=168.0, min=0.0042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.04443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.0036163330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=1.8700957298278809e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=6.705522537231445e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=109.0, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.79296875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6015625, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.7421875, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=139.0, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.703125, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.69921875, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9609375, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6875, min=3.170967102050781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.015625, min=4.649162292480469e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.265625, min=8.726119995117188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.578125, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=2.765655517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.40625, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8984375, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0002193450927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.984375, min=5.91278076171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=2.6941299438476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.828125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=6.914138793945312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=2.7179718017578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.359375, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=2.3692846298217773e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.625, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.546875, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=2.765655517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.53125, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=1.0788440704345703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.84375, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0004825592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.1875, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.34375, min=0.009033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=4.100799560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.875, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=2.3692846298217773e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.625, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.125, min=0.0019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.0, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.03125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.00494384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00011157989501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=0.005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.0, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.0203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=4.7206878662109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.00360107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.00116729736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.01300048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.5, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=74.0, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.024169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=99.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.02490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=131.0, min=0.01348876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.0284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=185.0, min=0.0223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.5, min=0.04931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=0.00494384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=169.0, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.15625, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.40625, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=4.792213439941406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=111.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.67578125, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9140625, min=1.3947486877441406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=160.0, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.79296875, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.89453125, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7265625, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0030364990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.765625, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.421875, min=0.00010395050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6328125, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=1.5497207641601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.515625, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.0, min=0.00193023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.71875, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.78125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.703125, min=0.00011777877807617188
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=2.4199485778808594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.96875, min=0.0031890869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=2.592802047729492e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.25, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=0.0004863739013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.000728607177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.78125, min=6.341934204101562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.4375, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.09375, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=4.5299530029296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.375, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=5.459785461425781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.00177764892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.28125, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=3.8623809814453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.515625, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.4375, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.5, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.125, min=0.01708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.25, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.00347900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.01007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.375, min=0.00182342529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=9.417533874511719e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.00543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=1.0848045349121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.25, min=0.0018463134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.01031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.5, min=0.003265380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.00020122528076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.5, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=120.0, min=0.003875732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.71875, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=151.0, min=0.006500244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.125, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.01318359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=100.0, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=137.0, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.5625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.75, min=0.00518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=107.5, min=0.0032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.01287841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.4483928680419922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=111.5, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5703125, min=3.886222839355469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.67578125, min=2.7865171432495117e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1484375, min=0.00019359588623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=152.0, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8515625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=5.984306335449219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=0.00010442733764648438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.859375, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.90625, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.75, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.265625, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=0.00014591217041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.859375, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.015625, min=6.198883056640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.484375, min=2.467632293701172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.71875, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=2.86102294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5625, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=2.110004425048828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.375, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.75, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.3125, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.28125, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=5.078315734863281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.375, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=3.9577484130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.5, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.65625, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.00182342529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=0.00131988525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.0038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.00640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=6.389617919921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=0.0034942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.25, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.01300048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.0137939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5625, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.005706787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=97.5, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.016845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=118.0, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=90.5, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.00010967254638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=188.0, min=0.004791259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=118.5, min=0.01495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=172.0, min=0.012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=156.0, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.0135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.953125, min=0.0002422332763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.73046875, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6875, min=1.6808509826660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.8828125, min=0.0001926422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=144.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8359375, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8515625, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.109375, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=9.298324584960938e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1796875, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.00180816650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.875, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=7.152557373046875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=8.58306884765625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6640625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=1.800060272216797e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.046875, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.4375, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.003082275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=4.172325134277344e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.9375, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.828125, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.000843048095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.90625, min=0.0031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.671875, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.25, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.0031585693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.0017852783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.1875, min=0.0004291534423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=1.1801719665527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.0032196044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=3.2901763916015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0625, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=1.1205673217773438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.00010538101196289062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.0002384185791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.0, min=0.006500244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.03857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.006622314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.375, min=0.009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.75, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.00665283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.375, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=4.00543212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=2.8014183044433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.25, min=0.00154876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0181884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.5, min=0.005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.052001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.0, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=93.0, min=0.0030670166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.0255126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=139.0, min=0.0279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.4375, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=108.5, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=111.5, min=0.01104736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.01507568359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=228.0, min=0.0322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.0157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=169.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.15625, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=117.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.69921875, min=0.00018405914306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.68359375, min=9.965896606445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.90234375, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.94140625, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.94140625, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=8.404254913330078e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.984375, min=0.00014019012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.21875, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=2.6226043701171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=1.0728836059570312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.328125, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=1.9073486328125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=0.0004825592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=82.5, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.703125, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6171875, min=3.075599670410156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.703125, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.00170135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.8125, min=0.0003795623779296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.625, min=5.221366882324219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.109375, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.40625, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=2.7120113372802734e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.65625, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=9.5367431640625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.59375, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=4.315376281738281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=3.9577484130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.25, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.3125, min=4.9114227294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.6875, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.59375, min=0.00011301040649414062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.53125, min=0.007354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.0037078857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.0625, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.375, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.8125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.006072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.01202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.875, min=0.00469970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.875, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=55.25, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.25, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.006317138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.5, min=2.86102294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=93.5, min=0.006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=8.344650268554688e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=131.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.0, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=130.0, min=0.000682830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.875, min=0.001861572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.01141357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=128.0, min=0.022705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=101.5, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.02001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.00142669677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.64453125, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6953125, min=0.00011157989501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.71875, min=1.2516975402832031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=150.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8515625, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.109375, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6328125, min=1.609325408935547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.0024261474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.34375, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.828125, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.28125, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.046875, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.34375, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=7.581710815429688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.40625, min=0.00021648406982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=4.506111145019531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.46875, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.625, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.75, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.75, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.953125, min=2.276897430419922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=5.91278076171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.6875, min=0.00469970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.34375, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.0073184967041016e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.625, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.00021648406982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.9375, min=0.006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=4.8160552978515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.000682830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.9375, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=4.291534423828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.00537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.125, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.125, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.0181884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.375, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.75, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=86.5, min=0.006072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=96.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.032958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=77.0, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=110.0, min=0.04296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=141.0, min=0.0208740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.5, min=0.02685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.0, min=1.2576580047607422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=121.0, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.05517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.005584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.09326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=310.0, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=5.507469177246094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=6.914138793945312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=103.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.796875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.796875, min=2.86102294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1875, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=152.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.171875, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.171875, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.671875, min=0.000873565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.6875, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.97265625, min=3.189779818058014e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.984375, min=8.440017700195312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.796875, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.40625, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.00016498565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.21875, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5859375, min=1.621246337890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=0.0001850128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.71875, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.859375, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=5.984306335449219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.09375, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=1.5616416931152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=2.1338462829589844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.25, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=4.209578037261963e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.53125, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=0.00014591217041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=5.245208740234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=7.963180541992188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=8.630752563476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.25, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0004711151123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.34375, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=5.5730342864990234e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.0, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.125, min=0.00347900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.8125, min=0.0026092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.75, min=0.00011301040649414062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.0103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.75, min=0.010498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=9.775161743164062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.75, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0002422332763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.125, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.00592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.5, min=0.006317138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=112.5, min=0.01513671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0235595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=102.0, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.3125, min=0.000339508056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.5, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=152.0, min=0.00994873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0216064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=186.0, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.0191650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=7.68899917602539e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=2.0116567611694336e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=105.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.75390625, min=9.894371032714844e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.7265625, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2890625, min=5.3882598876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=136.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7578125, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.70703125, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6875, min=6.258487701416016e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8984375, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5390625, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.0004863739013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.296875, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6328125, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.90625, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.46875, min=2.2530555725097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.90625, min=5.91278076171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.421875, min=0.000843048095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=1.245737075805664e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.5, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.84375, min=0.00020122528076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=4.029273986816406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00016498565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.46875, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.90625, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.375, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=0.0002689361572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.8125, min=0.00022983551025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=0.000431060791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.125, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.53125, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=3.695487976074219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.3125, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=3.719329833984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.375, min=0.005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.875, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=25.375, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.0108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.875, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.00014019012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0081787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.375, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.00665283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.125, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.75, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.01043701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.01708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0004711151123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=105.5, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.009033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.5, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=108.0, min=0.015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.04296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=79.0, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=118.5, min=0.0120849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=156.0, min=0.01104736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=0.0031585693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=0.005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=328.0, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.94921875, min=1.2367963790893555e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=1.2367963790893555e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.71875, min=0.00018024444580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=113.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7734375, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.734375, min=9.107589721679688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9765625, min=9.357929229736328e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=160.0, min=0.0003948211669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.81640625, min=5.817413330078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.86328125, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.609375, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4296875, min=9.164214134216309e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.8515625, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=87.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.609375, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=2.276897430419922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.234375, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8671875, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=4.351139068603516e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8984375, min=5.650520324707031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.703125, min=3.933906555175781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=4.124641418457031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8515625, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.5, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.34375, min=4.029273986816406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7109375, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.5, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.8125, min=1.8596649169921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=1.609325408935547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.40625, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.59375, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.0, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.59375, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.3125, min=0.00023365020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.125, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.1875, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.625, min=0.0004673004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.0030670166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=9.632110595703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.0021820068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.01904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.00022602081298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.125, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.017333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.006011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.0025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.003448486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.026123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=2.9087066650390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=110.5, min=0.0037841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.00021839141845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=55.5, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=3.9577484130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=150.0, min=0.005584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=96.5, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.0, min=0.0002384185791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=99.0, min=0.00408935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.00141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=95.0, min=0.017822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.75, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=81.0, min=0.0322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.01177978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.5, min=0.0017852783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=1.0356307029724121e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8359375, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.7734375, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.203125, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=153.0, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.109375, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4609375, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.15625, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1953125, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.75, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.34375, min=9.417533874511719e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.004608154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=2.4557113647460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.953125, min=2.4080276489257812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.84375, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=4.982948303222656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.75, min=0.003021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=3.361701965332031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.3125, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.4375, min=0.0032958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.75, min=0.001556396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=1.9431114196777344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=5.5730342864990234e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.1875, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=3.0994415283203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.0038299560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.875, min=0.006195068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=55.75, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.00012111663818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=5.841255187988281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.022705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.01226806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.25, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=0.00018787384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.0030975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.01904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.00012159347534179688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=165.0, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.0002193450927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.04833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=79.0, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=134.0, min=0.01275634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=146.0, min=0.00933837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.00469970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.006744384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.00142669677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=6.198883056640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.74609375, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.7734375, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4609375, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=150.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1171875, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.65625, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5703125, min=1.823902130126953e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5390625, min=0.000957489013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.578125, min=1.6570091247558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.34375, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=0.0003871917724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.234375, min=5.030632019042969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=3.123283386230469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9296875, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.21875, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=9.107589721679688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.9375, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=3.039836883544922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.796875, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.3125, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=6.389617919921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.4375, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0004482269287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0004024505615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.5, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.4375, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=1.043081283569336e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=3.0279159545898438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.1875, min=4.291534423828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.25, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.1875, min=0.005157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=8.702278137207031e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.4375, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.0, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.01043701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=7.963180541992188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.125, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=2.6702880859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.009033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=8.535385131835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.0213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.875, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.027587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.0025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.5, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.0, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=2.574920654296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=114.5, min=0.014892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=90.5, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=113.5, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=2.86102294921875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=121.5, min=0.007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=202.0, min=0.024658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.25, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.0135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=310.0, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=1.1682510375976562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=109.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9140625, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.796875, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9609375, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=150.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=6.079673767089844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.84375, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.21875, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.921875, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=4.458427429199219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=0.000965118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=0.0004711151123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.828125, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.1875, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=1.7762184143066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.828125, min=0.0004291534423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=3.600120544433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.375, min=0.0025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.78125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.0001964569091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.002777099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.625, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.0003757476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.0625, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.00015926361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0625, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.71875, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=6.341934204101562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.0002193450927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.1875, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.8125, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=7.3909759521484375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=5.9604644775390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.007537841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.00848388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.01214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=2.384185791015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.875, min=2.384185791015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.5, min=0.0108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.5, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.875, min=7.2479248046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.016845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.00543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=8.058547973632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=77.5, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=79.5, min=0.00023365020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.5, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.03515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=93.0, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0172119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=173.0, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=0.0034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=282.0, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.359375, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.328125, min=3.0994415283203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.5, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.57421875, min=2.60770320892334e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.640625, min=1.0251998901367188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.15625, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=156.0, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.890625, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.625, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8515625, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.46875, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.046875, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.75, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8828125, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.203125, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.65625, min=2.777576446533203e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.5625, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.859375, min=0.0030975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.625, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=8.106231689453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.765625, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.25, min=1.1920928955078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.25, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.21875, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.4375, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.0625, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.9375, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.9375, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=2.682209014892578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.625, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=5.745887756347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=3.337860107421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=4.935264587402344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=5.984306335449219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.00162506103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.375, min=0.0029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.0, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=0.00019359588623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.004302978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.00020122528076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=85.5, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.25, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.25, min=0.00250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.00762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.00592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.0228271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=104.5, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=198.0, min=0.0179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.0201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=102.5, min=5.650520324707031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=111.5, min=0.023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.5, min=0.049560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=92.5, min=1.3709068298339844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=236.0, min=0.005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.0162353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=5.91278076171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=186.0, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.046875, min=5.692243576049805e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.40625, min=3.7066638469696045e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.90625, min=0.00010538101196289062
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.62890625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.62109375, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9453125, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=151.0, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7578125, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8828125, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6015625, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.609375, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6796875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7265625, min=7.033348083496094e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=89.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.875, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=8.881092071533203e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9921875, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6015625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.375, min=2.2649765014648438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.375, min=2.2649765014648438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.890625, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.00164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=2.4199485778808594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.00011682510375976562
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.59375, min=0.00016307830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.828125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.421875, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.0, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.59375, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.6875, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.9375, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.21875, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.03125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=2.47955322265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.00018405914306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.9375, min=0.003875732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.625, min=4.220008850097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.0, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.005218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=2.5033950805664062e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.000690460205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00018405914306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.375, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.00933837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=87.5, min=0.0091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.002960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=111.5, min=0.0400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=7.987022399902344e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.4375, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=105.0, min=0.0113525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.75, min=0.0196533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=198.0, min=0.010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.0028839111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.5, min=0.0103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=209.0, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1640625, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.375, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=2.2172927856445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=102.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6796875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.57421875, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.328125, min=3.933906555175781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=165.0, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.3984375, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9765625, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6875, min=0.00014591217041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.828125, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5546875, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.34375, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=5.459785461425781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.765625, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=2.5391578674316406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.109375, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9296875, min=2.6106834411621094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.984375, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=8.106231689453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.90625, min=2.181529998779297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.90625, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.203125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=4.3213367462158203e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.890625, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.25, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0625, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.00018787384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.1875, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.125, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=1.621246337890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.78125, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=1.2159347534179688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=8.625, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.375, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=4.315376281738281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.25, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.6875, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00010347366333007812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.125, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.375, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.875, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=5.054473876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.875, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=1.71661376953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=0.017333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.125, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.0108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.0103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.0, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.005096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=122.0, min=0.0194091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.005584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=0.00347900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=115.5, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=115.5, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=109.5, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=4.9114227294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.0341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=1.1920928955078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=99.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.75, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=120.5, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.01220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=209.0, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1640625, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.00010013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=107.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7890625, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6796875, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.640625, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=154.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.99609375, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.859375, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.78125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=2.1457672119140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.625, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.828125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.390625, min=8.463859558105469e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.703125, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=5.555152893066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.640625, min=0.0003795623779296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8203125, min=1.621246337890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.53125, min=9.775161743164062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.421875, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=2.1696090698242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=0.0004253387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.671875, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.1875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.84375, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=0.00011491775512695312
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.34375, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=1.0788440704345703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.0034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.3125, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.00506591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.875, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=8.535385131835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.0625, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.71875, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.00250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=5.14984130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.625, min=0.003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.25, min=0.003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.25, min=0.004180908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.0081787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.00011157989501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.875, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.003326416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=1.7881393432617188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.004302978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.75, min=0.0084228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.0191650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=112.0, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=106.5, min=0.00089263916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=180.0, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=108.5, min=0.0224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=9.417533874511719e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=115.0, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0194091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.5, min=0.0146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=2.765655517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=310.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8515625, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=110.5, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8984375, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.89453125, min=5.364418029785156e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.90625, min=8.058547973632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=135.0, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.671875, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.9375, min=3.725290298461914e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.53125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.765625, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.296875, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.171875, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.109375, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5234375, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=3.600120544433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5546875, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.890625, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=2.4199485778808594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.109375, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.296875, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=3.9637088775634766e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.34375, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.984375, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=2.7894973754882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00023365020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.8125, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=7.361173629760742e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.00015354156494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.0, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.78125, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=4.410743713378906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=2.682209014892578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=3.147125244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.00010442733764648438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.6875, min=0.00384521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.0, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.0103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0024261474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.1875, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.00012111663818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.00653076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.875, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.5, min=0.00014019012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.003021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.25, min=0.01171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.0, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.006500244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.75, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=89.0, min=0.006744384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.00012111663818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.009521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.006072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=114.0, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=121.0, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.625, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=94.5, min=0.049072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=90.0, min=0.0076904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=166.0, min=0.00194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.75, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=282.0, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.359375, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=2.3126602172851562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=96.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.546875, min=6.151199340820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6171875, min=0.00012159347534179688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.87890625, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=147.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.58984375, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=3.1948089599609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.46875, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=6.318092346191406e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.859375, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.34375, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=4.673004150390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.003021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9453125, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.828125, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.59375, min=5.6743621826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.65625, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=0.00011539459228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.000675201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.15625, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=5.364418029785156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.00010395050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.46875, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=4.500150680541992e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8515625, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.828125, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=0.0030364990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=3.790855407714844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.34375, min=5.8650970458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=5.1021575927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.25, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.40625, min=0.00186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=7.200241088867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=7.987022399902344e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.003692626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=4.839897155761719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.25, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.25, min=0.00159454345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.000682830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0031585693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.005584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.25, min=0.00011873245239257812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=0.00579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.125, min=0.01092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=2.2530555725097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.625, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.5, min=0.0263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.005218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=111.5, min=3.790855407714844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=101.0, min=0.006317138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=86.5, min=0.0107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=94.5, min=0.0064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=125.0, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.0458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=186.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.046875, min=5.692243576049805e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.046875, min=9.119510650634766e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=115.5, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6171875, min=1.7762184143066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.546875, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.69921875, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=130.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.81640625, min=1.3709068298339844e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9765625, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=4.708766937255859e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.703125, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.625, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=90.5, min=3.1948089599609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.171875, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.34375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=5.364418029785156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.28125, min=9.72747802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.765625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.984375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=1.0669231414794922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.609375, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.46875, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=1.71661376953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.953125, min=3.933906555175781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.5, min=6.198883056640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.46875, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.890625, min=0.0002689361572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.4375, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=1.2159347534179688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.125, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=1.1980533599853516e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.0001850128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.15625, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.0625, min=0.0004444122314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=5.6743621826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.75, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.003631591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.00154876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.125, min=0.0027313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.00579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.0, min=0.0001010894775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=85.5, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.0, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=102.0, min=0.00927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.01025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=89.0, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=116.0, min=0.012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=125.0, min=0.009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.875, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.06396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=84.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=193.0, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.5, min=0.004669189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=209.0, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1640625, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=1.919269561767578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=101.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.68359375, min=0.00014591217041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.66015625, min=4.284083843231201e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0234375, min=3.790855407714844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=166.0, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.03125, min=2.6226043701171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1171875, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.046875, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.828125, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.703125, min=7.003545761108398e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3046875, min=6.389617919921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.90625, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.25, min=1.33514404296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.6875, min=2.8967857360839844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=0.00024318695068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.75, min=7.82012939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=4.351139068603516e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.03125, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.375, min=5.435943603515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=0.00153350830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=5.2928924560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.875, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.25, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.00010967254638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.375, min=0.0034637451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.625, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.5625, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.125, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.00084686279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.625, min=0.0042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=2.574920654296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.8125, min=0.0047607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.8125, min=0.00165557861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.006927490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.00168609619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.5, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.00016117095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.0196533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.0, min=1.3589859008789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.03271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.25, min=0.00567626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.25, min=0.00010251998901367188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.0054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.014404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.75, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.007110595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=87.0, min=0.00872802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=112.0, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=160.0, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=134.0, min=0.005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=1.1563301086425781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.125, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.0145263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.375, min=0.01031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=209.0, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1640625, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=5.7697296142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.76953125, min=4.696846008300781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.67578125, min=2.6345252990722656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.390625, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=160.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.203125, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9765625, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5390625, min=1.519918441772461e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.703125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.98046875, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.953125, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.6875, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.3125, min=3.266334533691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=1.049041748046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.375, min=1.9073486328125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9140625, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=0.000972747802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.859375, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.234375, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.000431060791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=6.884336471557617e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.9375, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.9375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5, min=0.000843048095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=4.649162292480469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.0035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.96875, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.84375, min=0.000537872314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=1.3828277587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.375, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.53125, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.875, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.125, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.00019359588623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.375, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.75, min=0.002960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.3125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.01031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=4.2438507080078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.0, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.0, min=0.008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.25, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.01953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.25, min=1.823902130126953e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.0230712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.01385498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.0, min=0.0001926422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=102.0, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.75, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.004791259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.01611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=115.5, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=215.0, min=0.0147705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=87.5, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=159.0, min=0.022216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.01708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=308.0, min=0.01611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.04052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=55.5, min=0.010498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=310.0, min=0.0023956298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.828125, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=110.5, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8984375, min=3.9577484130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9140625, min=3.528594970703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3828125, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=140.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.390625, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4140625, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.703125, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.84375, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.078125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9609375, min=5.626678466796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5, min=0.00160980224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=9.357929229736328e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.625, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7734375, min=3.0159950256347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.890625, min=2.300739288330078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.375, min=0.0004024505615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0625, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.625, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.421875, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=2.7120113372802734e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.75, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.09375, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.21875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0003261566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.00024318695068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.00640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.003753662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5625, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.0, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.125, min=3.2901763916015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.0118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.75, min=0.00160980224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.0, min=4.5299530029296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=0.0003376007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.0028839111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.125, min=0.0101318359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.00762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.0024261474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.0, min=0.014404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.75, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.25, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=99.0, min=0.02734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=1.1920928955078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=237.0, min=0.014404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.03515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.00084686279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.0002422332763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=282.0, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.359375, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=6.407499313354492e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=95.5, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5859375, min=0.00015354156494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.72265625, min=6.0558319091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.015625, min=0.00016117095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=152.0, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.72265625, min=0.0003337860107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.97265625, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.625, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.0, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.421875, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=4.857778549194336e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.95703125, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7734375, min=5.1021575927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=3.719329833984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.828125, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=2.4884939193725586e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.046875, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.046875, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.90625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.234375, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.484375, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.375, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=1.1920928955078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.921875, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=26.375, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.84375, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.03125, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.34375, min=1.2278556823730469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.9375, min=0.0091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0625, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.00154876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=5.0067901611328125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.00182342529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.00677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.0, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.5, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.00347900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.75, min=0.0001926422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.0, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.5, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=115.0, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.01434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=170.0, min=0.00165557861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.0250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.125, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.00408935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=186.0, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.046875, min=5.692243576049805e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.046875, min=1.049041748046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=1.823902130126953e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=115.5, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.62109375, min=5.507469177246094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.53515625, min=1.609325408935547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.140625, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=124.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9140625, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.390625, min=1.3709068298339844e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.21875, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.71875, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=2.9355287551879883e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6796875, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.25, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=1.2874603271484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=8.535385131835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=2.771615982055664e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=4.1484832763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=84.5, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7578125, min=3.838539123535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7578125, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.828125, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.96875, min=0.00016117095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.953125, min=9.655952453613281e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.375, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.65625, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.671875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.609375, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.484375, min=0.00011873245239257812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.46875, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=8.106231689453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=5.245208740234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.375, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.84375, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=2.86102294921875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5625, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.00130462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.007598876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.5, min=0.0035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.005096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=5.6743621826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.0093994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.125, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.5, min=0.0086669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.00592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.75, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.0, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.25, min=0.00021839141845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.75, min=0.057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.0, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.5, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=89.0, min=0.007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=115.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=109.0, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=91.0, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.5, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.01708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.75, min=4.363059997558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=169.0, min=0.01068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.0086669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=264.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6328125, min=2.60770320892334e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=2.60770320892334e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=1.2516975402832031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.171875, min=1.7642974853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.171875, min=1.811981201171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=128.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.28125, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.28125, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=2.1338462829589844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.875, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7578125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.21875, min=6.151199340820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.078125, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=5.435943603515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.96875, min=0.00024127960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.609375, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=2.9325485229492188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00142669677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7578125, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.828125, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8828125, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=2.181529998779297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.15625, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.46875, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.40625, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=1.430511474609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=4.2438507080078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.40625, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.75, min=0.006805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.75, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.375, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.00011777877807617188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=0.002838134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=2.586841583251953e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.0068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=0.00579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.75, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.00567626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=88.0, min=0.00537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.0, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.004791259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.0004825592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=0.00982666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0174560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.5, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.4375, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.0732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=69.5, min=1.1444091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=112.0, min=0.003692626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=94.5, min=0.00089263916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=162.0, min=0.0029449462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.5, min=0.0086669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.375, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.25, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=209.0, min=9.72747802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1640625, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=1.4066696166992188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=123.0, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.66796875, min=8.058547973632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.171875, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=8.404254913330078e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=120.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.859375, min=0.00016307830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.328125, min=0.00011396408081054688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5859375, min=5.078315734863281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=0.00023746490478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=2.775341272354126e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.83984375, min=2.3245811462402344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=4.363059997558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9375, min=8.106231689453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.390625, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.671875, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.75, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.828125, min=2.181529998779297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.0036468505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.34375, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5625, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=4.553794860839844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.953125, min=0.0004673004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.002777099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.00162506103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=5.459785461425781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=5.14984130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.53125, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.75, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.40625, min=0.003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.8125, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.00010967254638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.0037994384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=4.506111145019531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.00830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.0001773834228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.25, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.0, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.0, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.5, min=0.005279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=94.0, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.00165557861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=0.0072021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.75, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=96.0, min=0.0126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=147.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=154.0, min=0.01904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.6875, min=0.00179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=124.5, min=0.0546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=292.0, min=0.019775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.375, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.25, min=0.03076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=264.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6328125, min=2.60770320892334e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.125, min=2.60770320892334e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.40625, min=4.380941390991211e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.5, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.125, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.171875, min=3.6507844924926758e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0703125, min=0.00018978118896484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=123.5, min=0.00360107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=9.965896606445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.125, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6015625, min=9.298324584960938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3984375, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.453125, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.046875, min=4.363059997558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.796875, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8046875, min=1.8715858459472656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.71875, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=2.5272369384765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.59375, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.75, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.4375, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.125, min=0.0003871917724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=6.854534149169922e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.890625, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.375, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.34375, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.25, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=4.315376281738281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.125, min=9.870529174804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.5, min=0.00154876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=2.956390380859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.25, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.0001735687255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=2.4318695068359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=0.005279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.0128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.0, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.75, min=0.00185394287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0074462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=6.198883056640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.00023746490478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=110.0, min=0.0164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=91.0, min=0.02392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=62.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=102.5, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.01953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.75, min=0.00506591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=264.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6640625, min=0.0004596710205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1015625, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.625, min=1.5497207641601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=113.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.83984375, min=5.1975250244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.125, min=2.9355287551879883e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.171875, min=2.586841583251953e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=133.0, min=0.0030670166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0078125, min=0.0003337860107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=1.3649463653564453e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6875, min=5.364418029785156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.003631591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.875, min=0.00160980224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=6.020069122314453e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=4.673004150390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.40625, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.90625, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.90625, min=3.0994415283203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.9375, min=0.0027008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=1.0952353477478027e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.03125, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.046875, min=0.0001926422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.03125, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.4375, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.84375, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.000949859619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.65625, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.000431060791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.59375, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=4.315376281738281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=8.153915405273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.0, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.6875, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.0032501220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.625, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.0, min=0.0203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.625, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.01068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.25, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=4.124641418457031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.5, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.125, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.00732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=115.5, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=1.049041748046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.0, min=5.698204040527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=126.5, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=215.0, min=0.008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.5, min=0.00171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=282.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.359375, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3515625, min=7.295608520507812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=1.6450881958007812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=93.0, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5625, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.57421875, min=0.00011157989501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.84375, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=152.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65234375, min=0.000537872314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.73828125, min=6.556510925292969e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=0.0004024505615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=5.7697296142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.046875, min=0.00010061264038085938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=9.953975677490234e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=7.867813110351562e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.125, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.015625, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=4.029273986816406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9765625, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.359375, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=3.361701965332031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.09375, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.46875, min=2.592802047729492e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=4.8160552978515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.953125, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=2.193450927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.90625, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.125, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.75, min=0.005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=9.894371032714844e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.25, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.34375, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0001964569091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.125, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=4.100799560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.8125, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.375, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.00173187255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.125, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.0033111572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=0.0032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.25, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.125, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.875, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.125, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.0, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=0.002838134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.01708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.25, min=0.000942230224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.01446533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.625, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.875, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.0034637451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.0, min=0.0145263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=96.0, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.0233154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.25, min=0.01300048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=94.0, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.9375, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.5, min=0.00020313262939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=123.0, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=0.000690460205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=186.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.046875, min=5.692243576049805e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.046875, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=1.5497207641601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.0, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5546875, min=1.4126300811767578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.57421875, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.234375, min=9.489059448242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=144.0, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.79296875, min=3.504753112792969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.84765625, min=3.504753112792969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2265625, min=6.723403930664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.8125, min=0.0003376007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.2265625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=89.5, min=0.0037841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.765625, min=4.673004150390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.703125, min=9.238719940185547e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=9.965896606445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=5.662441253662109e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.015625, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7578125, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.0037078857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=1.8477439880371094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.71875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.40625, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.796875, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.00506591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=0.0022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=1.436471939086914e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.75, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.71875, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.0004596710205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=3.075599670410156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.3125, min=0.0015411376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.34375, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.25, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=1.5124678611755371e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=1.329183578491211e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.4375, min=2.193450927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=9.119510650634766e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0015411376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.00011396408081054688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=3.886222839355469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0026092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=6.914138793945312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.0225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.0081787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.125, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.015380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.0, min=0.03173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.00830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=0.0068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.25, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.017333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=62.75, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=86.5, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=114.5, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=102.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=85.5, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.007568359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=94.5, min=0.01019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=75.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=160.0, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.875, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.0244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=0.0140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=304.0, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71484375, min=4.0531158447265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=4.0531158447265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=100.5, min=0.00469970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.84375, min=3.904104232788086e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.84375, min=1.9818544387817383e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2265625, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=142.0, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.78125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1875, min=2.8848648071289062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8046875, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=5.140900611877441e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1953125, min=2.6345252990722656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.8125, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.15625, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.90625, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.921875, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.515625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.00183868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.046875, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9609375, min=3.838539123535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.671875, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.0001964569091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.484375, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.25, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.625, min=1.9222497940063477e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=6.67572021484375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.00168609619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.625, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.875, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=1.3947486877441406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=0.00024127960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.25, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.0027008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.0, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.75, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=2.0265579223632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.8125, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.0, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.0022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=2.956390380859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.0, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.125, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.5, min=0.003936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.0, min=0.0037841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.00116729736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.01092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.0, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=127.5, min=0.0084228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.01953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.00016498565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=97.0, min=0.00634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.01300048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=114.5, min=0.021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=102.0, min=0.03173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=100.5, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=176.0, min=0.0069580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=102.5, min=0.0125732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=102.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=322.0, min=0.00750732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=1.7404556274414062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.984375, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=102.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5390625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.84375, min=1.4126300811767578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0390625, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=128.0, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.75, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.75, min=2.7418136596679688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=4.6253204345703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.640625, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.40625, min=0.0002422332763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=2.6464462280273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4296875, min=0.0004024505615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.000682830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.859375, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.4375, min=1.8477439880371094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.140625, min=0.00141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=4.041939973831177e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.625, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.046875, min=2.384185791015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=9.72747802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.375, min=0.00019168853759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0001773834228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.34375, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5, min=5.6743621826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.0038299560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0625, min=0.00131988525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.00010347366333007812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=3.993511199951172e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=0.00010442733764648438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.000690460205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.3125, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.00010728836059570312
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.125, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=8.106231689453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.003692626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.00011157989501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=1.0371208190917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.00927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.0, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.01287841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.625, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.01031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.75, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.00738525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=4.1484832763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=62.0, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.051513671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=116.5, min=0.0103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.625, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.0157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=62.25, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=0.02685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.0001010894775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=99.5, min=0.0244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=118.0, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=5.793571472167969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=173.0, min=0.0162353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.01416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=233.0, min=0.00543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.0, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.0150146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=306.0, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.03125, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=2.3245811462402344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=107.5, min=6.020069122314453e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9921875, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.64453125, min=7.331371307373047e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=1.537799835205078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=156.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.88671875, min=5.030632019042969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8515625, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2109375, min=3.361701965332031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.25, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=5.125999450683594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.171875, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=1.6450881958007812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5, min=0.0004367828369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.625, min=3.600120544433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.234375, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.53125, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=2.2351741790771484e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5625, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5625, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.0, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=1.0371208190917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.0, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.125, min=0.00018024444580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.1875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.625, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.625, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=8.726119995117188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.003875732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.01031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.5, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.0033111572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.75, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.25, min=0.00084686279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.75, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=0.0004787445068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.0157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.0, min=0.0152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=4.839897155761719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=102.5, min=0.017333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.75, min=0.00010395050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.875, min=0.007354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=139.0, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=77.0, min=0.00118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.017333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=123.0, min=0.01519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=113.5, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=90.0, min=0.0037078857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.1875, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=103.5, min=0.01141357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=1.1682510375976562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=133.0, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=84.5, min=0.01153564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.0, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.02734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.0140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=298.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.70703125, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4453125, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=0.00015926361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.5, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9921875, min=0.00014591217041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9921875, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=148.0, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.265625, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.265625, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3203125, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.359375, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=4.553794860839844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=91.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.734375, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.1875, min=5.221366882324219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.40625, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.90625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.90625, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.28125, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.03125, min=0.000537872314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=1.33514404296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.703125, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=2.5331974029541016e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.953125, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.1875, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.984375, min=5.626678466796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.625, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0625, min=0.0016632080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=7.82012939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.0, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.4375, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=3.2186508178710938e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.000873565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.75, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.15625, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=5.662441253662109e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.3125, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0001201629638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.5625, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.375, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.875, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.00167083740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5625, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.375, min=0.00191497802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.00141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.25, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.75, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.0027008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.75, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.125, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.04248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=9.107589721679688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.02490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.000675201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.01226806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=97.5, min=0.010498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=123.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=178.0, min=0.0032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=156.0, min=0.0390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=79.0, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=115.0, min=0.028564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=91.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=256.0, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.002960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=306.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.03125, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.03125, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.00011539459228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.79296875, min=2.956390380859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.70703125, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.00020313262939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=167.0, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9453125, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.96484375, min=2.2530555725097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.375, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.125, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=2.905726432800293e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=0.0002422332763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.5, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.00014781951904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9765625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3984375, min=3.62396240234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=7.2479248046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=3.933906555175781e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.203125, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=1.2814998626708984e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=1.6689300537109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.890625, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.546875, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=1.8596649169921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.984375, min=7.963180541992188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.96875, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.25, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.90625, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.3125, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.25, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.5625, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=3.719329833984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0003948211669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.125, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.75, min=0.0026092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.00183868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=5.245208740234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.625, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.000957489013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=7.581710815429688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.00994873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.125, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=0.00014019012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.5, min=0.01007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.75, min=0.023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.017333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.25, min=0.00408935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.0, min=0.0032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.5, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=0.10400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.5, min=4.744529724121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=141.0, min=0.0174560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=89.5, min=0.0673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=147.0, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=83.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=96.0, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.005523681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=0.00848388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=310.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.0, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.80078125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.82421875, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0390625, min=5.923211574554443e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=144.0, min=0.00250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.03125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0390625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.421875, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.00360107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.953125, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=1.0907649993896484e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3125, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.171875, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.28125, min=7.212162017822266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=4.291534423828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.515625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.25, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.671875, min=3.337860107421875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=2.8014183044433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.75, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.25, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.75, min=2.1457672119140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.00011682510375976562
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.00162506103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=1.2934207916259766e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.28125, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=1.537799835205078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5625, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.00018787384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.53125, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.125, min=0.006195068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.875, min=0.01513671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.00011587142944335938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.25, min=0.00738525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.005706787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.0093994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.625, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=0.0001773834228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.25, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=0.00592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.01519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.75, min=0.00775146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.375, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.5, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.0089111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=111.5, min=0.006744384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.25, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=196.0, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.5, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=132.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.0004367828369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=107.5, min=0.0291748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=107.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=286.0, min=0.00170135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.0146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.0074462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=1.8030405044555664e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.71875, min=1.1920928955078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=95.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.72265625, min=5.3882598876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.72265625, min=1.3232231140136719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0703125, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.000843048095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=141.0, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.828125, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.81640625, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5234375, min=8.726119995117188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.0, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8984375, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8515625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9921875, min=7.987022399902344e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.5, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6875, min=9.632110595703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.546875, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=1.6689300537109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8125, min=0.0001964569091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.28125, min=2.3692846298217773e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=3.4570693969726562e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.640625, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6796875, min=1.4007091522216797e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=0.00019359588623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.453125, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.671875, min=0.00010395050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=1.5497207641601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.515625, min=0.0021820068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=2.1457672119140625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8828125, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.3125, min=4.458427429199219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=9.47713851928711e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.90625, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=1.9431114196777344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.25, min=0.00186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=1.895427703857422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.53125, min=0.00384521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=7.987022399902344e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.000415802001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.25, min=0.0004825592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=2.8371810913085938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.00023746490478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.006317138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.0032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.375, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.375, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.75, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.018798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.0, min=4.792213439941406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.5, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.01373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.25, min=0.00014781951904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=0.00616455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.0089111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.75, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.0036163330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.0, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=115.0, min=0.0036468505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=97.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=113.5, min=0.0031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=94.5, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=98.0, min=0.010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.25, min=0.00347900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.25, min=0.035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=230.0, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.037353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=260.0, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.69921875, min=0.00018024444580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=4.696846008300781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.0, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0546875, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3515625, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=147.0, min=0.003326416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.796875, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.84765625, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4296875, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.765625, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.265625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=92.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8984375, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4296875, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=1.1444091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.484375, min=1.0132789611816406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=1.9669532775878906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=0.0002689361572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.3671875, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.484375, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=2.968311309814453e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=2.384185791015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.625, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=2.5987625122070312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=4.744529724121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.53125, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=2.2172927856445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.84375, min=0.003936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=5.626678466796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.0004863739013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.9375, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.0004711151123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.84375, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.728534698486328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.0004673004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.15625, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.5625, min=0.00186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=3.0994415283203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5625, min=0.00592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0004711151123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.5, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.625, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=1.5974044799804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.75, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.00494384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=9.5367431640625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.02294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.0004291534423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.04931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.003570556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.0096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.0, min=0.005828857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.0047607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.75, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.125, min=0.00135040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=117.0, min=0.0152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=1.8715858459472656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=92.0, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=111.0, min=0.05224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=95.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=102.0, min=0.015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.017822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.75, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=98.5, min=0.000690460205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.625, min=8.046627044677734e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=5.698204040527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0703125, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=145.0, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.80078125, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8203125, min=0.0001201629638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.84375, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.0002384185791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.90625, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6328125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4453125, min=0.0001201629638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.4375, min=0.00130462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6328125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=4.410743713378906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8046875, min=2.6464462280273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.28125, min=6.020069122314453e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.640625, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=4.863739013671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.5, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=9.775161743164062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=9.775161743164062e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0625, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=4.678964614868164e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.375, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.8125, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.34375, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.6875, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.34375, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=2.658367156982422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.25, min=9.870529174804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.6875, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.01324462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.0, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.00555419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.003265380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.125, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.375, min=0.00579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.01043701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.25, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.0036468505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.01239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.875, min=2.682209014892578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.25, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.9375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.75, min=0.01043701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.00167083740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.625, min=0.006744384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.375, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.75, min=0.00921630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.25, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=68.5, min=0.00194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=0.003265380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.8125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.007232666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=143.0, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.125, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.0257568359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=288.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.60546875, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3984375, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=9.107589721679688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=125.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6484375, min=2.16066837310791e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=1.564621925354004e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0390625, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=147.0, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.140625, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.140625, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=8.761882781982422e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7109375, min=5.8710575103759766e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.5, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.171875, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.4375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8828125, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.59375, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=1.5079975128173828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6171875, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0, min=0.004669189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=1.0609626770019531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=0.0003871917724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.3125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.40625, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=6.0558319091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=0.000972747802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.75, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=8.726119995117188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.625, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.625, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=7.2479248046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.5625, min=0.005279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.000720977783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.0, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.6875, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.75, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=25.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.75, min=0.00250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.000431060791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.625, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.01409912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.75, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.00154876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=94.5, min=0.0076904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.0167236328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=0.0001735687255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.0, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=101.0, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.0191650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=188.0, min=0.006500244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=87.0, min=0.0223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=75.5, min=1.049041748046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=320.0, min=0.003143310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.875, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.00142669677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=276.0, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=111.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.63671875, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.609375, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.125, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=0.000827789306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.90234375, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.84375, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.609375, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3828125, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.15625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=2.6941299438476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8359375, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9609375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.953125, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4296875, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=3.9637088775634766e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=3.0159950256347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.96875, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=7.539987564086914e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.53125, min=0.003936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=3.1948089599609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.75, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.421875, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.9375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.84375, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.9375, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.34375, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.875, min=0.01513671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.875, min=0.0030975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0625, min=0.004974365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=0.00194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.125, min=3.62396240234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=7.963180541992188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.002471923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.00014591217041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.875, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.5, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.875, min=2.1696090698242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.00167083740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=9.870529174804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.0, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0625, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.0133056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.0, min=0.00160980224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.00016498565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.02001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=109.0, min=0.003692626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.5, min=0.01416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=96.5, min=0.00019168853759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=181.0, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.8125, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=84.5, min=0.060546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=243.0, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.5, min=0.0179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.75, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.0967254638671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=2.3245811462402344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.0, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6484375, min=0.00016117095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.7109375, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0390625, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=115.5, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71875, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8984375, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=4.76837158203125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=84.0, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6015625, min=4.076957702636719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5703125, min=5.078315734863281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.078125, min=2.86102294921875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9140625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=1.4662742614746094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=6.389617919921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5703125, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=1.3560056686401367e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2890625, min=1.8835067749023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.21875, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=1.780688762664795e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=3.719329833984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.375, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.046875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5859375, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.6875, min=0.00185394287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.484375, min=0.00159454345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.375, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.78125, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=3.695487976074219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.65625, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=8.535385131835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.0003337860107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.3125, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=7.115304470062256e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=2.86102294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.1875, min=0.0033111572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=1.5735626220703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.000965118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.001556396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.0025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=7.200241088867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=5.030632019042969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=5.9604644775390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.006622314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.875, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.025146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.375, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.25, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.0242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.25, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.25, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.01544189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.75, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.5, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.75, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.0, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.00433349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.04248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.0, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=75.5, min=0.0001926422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.5, min=0.00982666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.75, min=0.01177978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.75, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.009521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=328.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.62109375, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=0.00015354156494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=9.012222290039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.92578125, min=4.124641418457031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=0.00022983551025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.03125, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=133.0, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4296875, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.671875, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.609375, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=6.151199340820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6953125, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4140625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.00738525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.859375, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7265625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9609375, min=0.00014781951904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.00018024444580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.765625, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7421875, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=6.914138793945312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.9375, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.000713348388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=8.106231689453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.90625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=4.506111145019531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.0, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.59375, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0001964569091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.40625, min=0.0074462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=1.919269561767578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5625, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.1875, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=3.62396240234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.75, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=2.396106719970703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.3125, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.4375, min=0.0152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=1.5079975128173828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.875, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.0172119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.00024127960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.125, min=4.00543212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.875, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.0111083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.00811767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00011682510375976562
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.375, min=0.00011777877807617188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.5, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=92.5, min=0.044189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.5, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=170.0, min=0.00555419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5625, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0206298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=215.0, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.02587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=0.01214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4765625, min=3.361701965332031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=4.6193599700927734e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.00024127960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.72265625, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.71875, min=5.3882598876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.046875, min=8.630752563476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=144.0, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8359375, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8359375, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.265625, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.75, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5390625, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7890625, min=1.996755599975586e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.85546875, min=1.1920928955078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0004673004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.578125, min=2.0742416381835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=9.298324584960938e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7109375, min=6.4373016357421875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.96875, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.03125, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=2.849102020263672e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=7.450580596923828e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.21875, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.3125, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=0.0001010894775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.001861572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.828125, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.001556396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.65625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.875, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.59375, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.8835067749023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.000728607177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.78125, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.1875, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=4.589557647705078e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=3.6716461181640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.5, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.0018463134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.0, min=4.506111145019531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.033447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.5, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.25, min=0.004180908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.00174713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.0, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.5, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=1.33514404296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.009521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.25, min=5.245208740234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.25, min=0.00193023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.75, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.01373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.5, min=0.00567626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.0, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.00970458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.5, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=76.0, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.8125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.0106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.25, min=7.295608520507812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.375, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.75, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=288.0, min=0.006134033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.03662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=0.0047607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=260.0, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.69921875, min=0.00018024444580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4609375, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8828125, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.8125, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=103.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.015625, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9921875, min=5.125999450683594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=136.0, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.296875, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.296875, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0859375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.703125, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1875, min=1.4156103134155273e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.921875, min=3.528594970703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9140625, min=0.00010347366333007812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=5.245208740234375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.000186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.3125, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=5.030632019042969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.984375, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.15625, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=5.364418029785156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.234375, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.9375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.78125, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.515625, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.765625, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.000957489013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.0016632080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5625, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.5625, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.8125, min=0.0003261566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.625, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.828938007354736e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.00750732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.875, min=0.005096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.0026092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.00168609619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=0.003570556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.00250244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.0004825592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.25, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.0106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.0030670166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.25, min=8.440017700195312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.875, min=2.9325485229492188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.875, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.0, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=71.0, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.01116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.875, min=2.7418136596679688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=0.0042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.0177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=149.0, min=0.0272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=106.5, min=0.02197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=97.0, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=4.00543212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.6875, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.5, min=0.0152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=136.0, min=0.004974365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=3.8623809814453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.875, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.0113525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.0, min=0.00860595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=2.5033950805664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.0, min=0.004638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6484375, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=0.00017833709716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9140625, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=158.0, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.72265625, min=1.633167266845703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8515625, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8515625, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5390625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.77734375, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.4375, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.6328125, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.46875, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=4.410743713378906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.859375, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=2.396106719970703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.375, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=4.32133674621582e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.1875, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.75, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.09375, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=4.935264587402344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.125, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0625, min=0.00010013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=7.295608520507812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=1.7881393432617188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.00020313262939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.125, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=6.020069122314453e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0625, min=3.4809112548828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=1.8596649169921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5625, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.75, min=0.0120849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.25, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.0031890869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=62.5, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.25, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=0.00015354156494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.000957489013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.375, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.0198974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.0, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=4.0531158447265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=9.5367431640625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.75, min=0.002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.75, min=2.396106719970703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.5, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.034912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=115.0, min=0.0218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.00775146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=92.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=95.0, min=0.01556396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.0062255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.75, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=146.0, min=0.0098876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=169.0, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.15625, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4296875, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=114.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.69921875, min=5.2928924560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.67578125, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.03125, min=2.4557113647460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=132.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71875, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.25, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.171875, min=0.00011014938354492188
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.875, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6171875, min=3.933906555175781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.01446533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.21875, min=5.626678466796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=7.200241088867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.3125, min=4.231929779052734e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8359375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=6.705522537231445e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.96875, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.46875, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.625, min=0.00170135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=1.5944242477416992e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6953125, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.1875, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=2.276897430419922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.46875, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.46875, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.00019359588623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.00299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.2934207916259766e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.4375, min=0.0035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.125, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.0166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.875, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.00176239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.00010967254638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.25, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.0001773834228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.25, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.0054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=3.0040740966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.0, min=0.02685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.0, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.000949859619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.01129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.5, min=0.0169677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=87.5, min=0.00640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=94.0, min=0.017578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=119.5, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=158.0, min=0.0111083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.024169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.5, min=0.0029449462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=177.0, min=0.01031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.03857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=0.0123291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.03125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.0, min=0.0029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=2.1904706954956055e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6328125, min=8.726119995117188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.69921875, min=4.410743713378906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0390625, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=162.0, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8203125, min=8.881092071533203e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6875, min=1.609325408935547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=4.5299530029296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.003448486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=4.9114227294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1875, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.3125, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=1.4960765838623047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.3125, min=8.106231689453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.0014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.296875, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.25, min=0.00860595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.203125, min=1.895427703857422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.03125, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.40625, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=0.00018024444580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=4.172325134277344e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.671875, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=9.298324584960938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.625, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.6875, min=0.003143310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0001773834228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.00089263916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.0625, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=6.151199340820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.375, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.00024127960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.375, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.0, min=0.0027008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.75, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.01708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.375, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.5, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.0, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=95.0, min=0.01263427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=5.7220458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=79.0, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.02783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.000431060791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.013916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.75, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.5, min=0.020263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=104.0, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=112.5, min=0.0037994384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.8125, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.01275634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=69.0, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=112.5, min=0.00299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.01336669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=104.5, min=0.0196533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.75, min=0.003326416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.0, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.00408935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=310.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=3.8623809814453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.82421875, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.828125, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=155.0, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.40625, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3203125, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.6875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8359375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0234375, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.90625, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.265625, min=1.633167266845703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.0625, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=3.838539123535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.453125, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.953125, min=5.650520324707031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5, min=8.463859558105469e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=2.5779008865356445e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.25, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.21875, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.3125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.375, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=3.2901763916015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.125, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0625, min=1.1444091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=1.6689300537109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.8125, min=0.0032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.25, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.125, min=0.005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.000186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.75, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=7.390975952148438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.00179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.00170135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.75, min=0.00714111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=1.0281801223754883e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.375, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.125, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.25, min=0.0024261474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=4.982948303222656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.625, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.75, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.5, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.00823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.002410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.00897216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.0, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.0, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.0264892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.5, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.0, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=94.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=148.0, min=0.042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.1875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=119.0, min=0.0159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.5, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=160.0, min=0.007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.125, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.01092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=62.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=204.0, min=0.0299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.5, min=0.0272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.0047607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=282.0, min=0.00011157989501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.359375, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=5.602836608886719e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=6.616115570068359e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=93.5, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5625, min=4.202127456665039e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6328125, min=2.765655517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.93359375, min=1.1086463928222656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=149.0, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7890625, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=1.7955899238586426e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.0625, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=4.917383193969727e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0859375, min=4.5299530029296875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=86.5, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.000972747802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.9375, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.4375, min=5.745887756347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.000873565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8125, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8125, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5859375, min=1.5735626220703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.75, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.28125, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=4.863739013671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.0, min=0.00360107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.53125, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.875, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.21875, min=0.002960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.28125, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.59375, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=2.8848648071289062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.46875, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.00011539459228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.40625, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.0019378662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.75, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.006927490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.0003261566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.25, min=0.00020313262939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.00921630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=0.00015926361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.375, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.0030364990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.003692626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.625, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.0, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=2.777576446533203e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.25, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.004852294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=4.291534423828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.5, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.75, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.0125732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=120.0, min=0.007293701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=113.5, min=0.038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=113.5, min=1.1563301086425781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.5, min=0.006805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=100.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=304.0, min=0.020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.053466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=71.0, min=0.0024261474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=169.0, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.15625, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.28125, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=114.0, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6875, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.67578125, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.859375, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9609375, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9609375, min=2.944469451904297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1328125, min=0.00011157989501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5546875, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=90.0, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.9375, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=1.9788742065429688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.9375, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.00022983551025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.90625, min=0.0001010894775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.671875, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.671875, min=5.848705768585205e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=9.012222290039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.578125, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.4375, min=0.001556396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=5.781650543212891e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=0.00019168853759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.25, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.84375, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.59375, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.375, min=0.00141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.875, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=1.3053417205810547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.875, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.00193023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=3.62396240234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.0, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.1875, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.00506591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0037994384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.00130462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.007293701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.375, min=1.7508864402770996e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.0, min=0.005889892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.0098876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.625, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.0126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=77.0, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0, min=0.0018463134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.019775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=104.0, min=0.015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.0123291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.00372314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=110.0, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0002193450927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=144.0, min=0.026611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=120.0, min=0.00830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.045654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=172.0, min=0.003021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.004638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.0238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=62.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.859375, min=0.00010251998901367188
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.58203125, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6875, min=8.726119995117188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4765625, min=1.5974044799804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=164.0, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8828125, min=4.744529724121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.828125, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.921875, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.984375, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.78125, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.953125, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=2.753734588623047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=1.895427703857422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.546875, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.90625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.984375, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.65625, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=0.0004596710205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.375, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.796875, min=0.0038299560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.00018978118896484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.15625, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=1.2814998626708984e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.4375, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.4375, min=0.000720977783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=7.200241088867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.84375, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.0, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.1875, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=5.745887756347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.9375, min=0.005523681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.0186767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.0130615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=4.863739013671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=30.125, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.5, min=0.00191497802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.5, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.375, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.0, min=4.315376281738281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=77.5, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.0106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.75, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=92.5, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.0595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.00982666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.020263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=82.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=123.0, min=0.018798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.6875, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=72.5, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=146.0, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.0, min=0.017333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=224.0, min=0.00494384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.0, min=0.003631591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=294.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.66796875, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.1205673217773438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.046875, min=0.00016498565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=114.0, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.796875, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.796875, min=0.00021839141845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2109375, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=163.0, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.2734375, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.421875, min=4.744529724121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.90625, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.59375, min=4.076957702636719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=4.857778549194336e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.59375, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=82.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9375, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=2.2649765014648438e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=8.726119995117188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8515625, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6015625, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.828125, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.6875, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.921875, min=0.00012159347534179688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.109375, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.640625, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.671875, min=0.0004291534423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.0036163330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.625, min=2.682209014892578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.5, min=0.000667572021484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.375, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=2.193450927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.00011730194091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.96875, min=0.0001850128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.96875, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.0, min=0.0003261566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.0, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=9.298324584960938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.5, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.125, min=0.021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=4.6253204345703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.125, min=0.00177764892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=7.390975952148438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.75, min=0.00640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.25, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=7.295608520507812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=0.005157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.25, min=0.00726318359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.0, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=2.5272369384765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.0191650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.0, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.25, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.00182342529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=84.0, min=0.00014781951904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=141.0, min=0.007537841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.0081787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=176.0, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.08349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=336.0, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=282.0, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.359375, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.421875, min=3.6716461181640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=92.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.53125, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.63671875, min=6.22868537902832e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.84375, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=153.0, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0625, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=8.058547973632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1796875, min=7.82012939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.53125, min=5.513429641723633e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=90.0, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=6.645917892456055e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.109375, min=1.71661376953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9140625, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.765625, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7421875, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.984375, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.21875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.828125, min=7.927417755126953e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.1875, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.90625, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.00177764892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.40625, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=0.00010156631469726562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=1.9431114196777344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=3.170967102050781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.953125, min=5.9604644775390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.0, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.4375, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.000423431396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.005615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.125, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.8125, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.625, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.75, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.375, min=0.00182342529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=0.004791259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.1875, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.00628662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.25, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.0, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.625, min=0.002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.25, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=79.0, min=0.0037841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.022216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=0.00537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.0, min=0.0103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.03466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=104.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.016845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.0, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=95.0, min=0.00689697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=176.0, min=0.005218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.125, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=276.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7578125, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=2.9087066650390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=1.7762184143066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=95.0, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.78515625, min=4.1425228118896484e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.66015625, min=4.458427429199219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=160.0, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8984375, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.86328125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8125, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.96484375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.0, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.03125, min=0.0017852783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=0.00014781951904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.734375, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.734375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.0, min=3.504753112792969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=6.151199340820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7421875, min=7.200241088867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3046875, min=4.363059997558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=0.0001010894775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.140625, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6875, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.375, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.359375, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=2.637505531311035e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.625, min=0.006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.890625, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.828125, min=5.036592483520508e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.265625, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=1.0728836059570312e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.21875, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.0001926422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.625, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.5625, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.78125, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.75, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=5.364418029785156e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=6.389617919921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.75, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.5625, min=0.0067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=2.1457672119140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.375, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=5.7697296142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.0016632080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=4.029273986816406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.0074462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=2.8014183044433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.000949859619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0291748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=2.0623207092285156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.75, min=0.0042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.00738525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.0, min=5.793571472167969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.01611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=7.82012939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.6875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.016357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=90.0, min=0.0034942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.031494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.375, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0098876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=247.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7265625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=2.1696090698242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=2.428889274597168e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.0004444122314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.0, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.58203125, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8984375, min=4.1425228118896484e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1953125, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=144.0, min=0.0032958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71484375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.75, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.328125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4609375, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=90.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.78125, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.71875, min=8.847564458847046e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=5.364418029785156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.0035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.4375, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.703125, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.328125, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1875, min=4.029273986816406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.671875, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=0.0003261566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.15625, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=2.086162567138672e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.578125, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.890625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.25, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0625, min=0.00011587142944335938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=6.771087646484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.0033111572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.03125, min=8.225440979003906e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.75, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.6875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.125, min=0.0024261474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=5.2928924560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=0.00012111663818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.0, min=3.2901763916015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.0031890869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.0, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.5, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=9.059906005859375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.75, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.0284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.02001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.75, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.0021820068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=90.0, min=0.0986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=109.0, min=0.005615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.0004711151123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=159.0, min=0.07666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=103.0, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=102.0, min=0.00689697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=101.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=164.0, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=200.0, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.859375, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=101.5, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.67578125, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.7734375, min=5.435943603515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1953125, min=0.00016307830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=153.0, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.75390625, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.046875, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6640625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=1.2874603271484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=84.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.078125, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5703125, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8203125, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=1.3470649719238281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=5.626678466796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.78125, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.484375, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=2.1696090698242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=0.0003871917724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.703125, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=3.039836883544922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=1.4424324035644531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.65625, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.96875, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=1.609325408935547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.125, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.8125, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=8.153915405273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.90625, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9431114196777344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.75, min=0.0038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.84375, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.9375, min=0.005828857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.625, min=0.004302978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.000431060791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.625, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=9.012222290039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.003875732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=5.91278076171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.625, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.0242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.01287841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.00628662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.01300048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0625, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.0, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.75, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=92.5, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.125, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.02294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=98.0, min=0.004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.00182342529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=84.0, min=0.007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=62.25, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=115.5, min=0.0062255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.0179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=202.0, min=0.006134033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=9.202957153320312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=3.266334533691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=95.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.66015625, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.671875, min=7.59027898311615e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.109375, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=149.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.109375, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=3.695487976074219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.359375, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7265625, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.609375, min=3.504753112792969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.328125, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9140625, min=1.6614794731140137e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.03125, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7421875, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.03125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.734375, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.3125, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=1.633167266845703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=5.745887756347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.40625, min=0.00131988525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.8125, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.34375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=2.4318695068359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.4375, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=5.185604095458984e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=3.218650817871094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.78125, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.25, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=1.138448715209961e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.003631591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.375, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.4375, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.25, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=5.1975250244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.000713348388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.3125, min=8.106231689453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.00193023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.9375, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=2.4318695068359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=5.7220458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.5, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.0001964569091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0031585693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.5, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.0024261474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.25, min=0.002410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=62.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=5.7697296142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=75.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.005462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.0, min=0.03759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.0, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=97.0, min=0.00157928466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.875, min=0.003082275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.0, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=178.0, min=0.0174560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.0, min=0.004852294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=169.0, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.15625, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=7.3015689849853516e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=125.0, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.62890625, min=7.82012939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.78125, min=2.9802322387695312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.15625, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=149.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71875, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5703125, min=5.841255187988281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.71875, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6171875, min=3.9637088775634766e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.1875, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=3.2901763916015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.203125, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=8.828938007354736e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.671875, min=8.535385131835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9375, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.84375, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=1.4185905456542969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6484375, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.1875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.28125, min=0.0037078857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.28125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=8.225440979003906e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.000965118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.8125, min=1.0505318641662598e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.34375, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.0625, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.6298145055770874e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=5.745887756347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.4375, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5625, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0004863739013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.625, min=0.000713348388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.002410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.1875, min=0.00167083740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.004638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.0, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.00616455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=5.745887756347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.0018768310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.375, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.000843048095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.0, min=6.198883056640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.125, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.00958251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.0, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.01287841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.25, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.00177764892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.0, min=0.00170135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=113.5, min=0.005706787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.125, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=94.0, min=0.01220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.5, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=178.0, min=0.00188446044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=85.0, min=0.0272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=123.5, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.005096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.033447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.00177764892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.2278556823730469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=0.00011491775512695312
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=119.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.53515625, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.78515625, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8984375, min=0.0004367828369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.46875, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.640625, min=5.185604095458984e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=4.857778549194336e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.890625, min=0.00011682510375976562
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=5.53131103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.390625, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=2.574920654296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.390625, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=0.00019359588623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.71875, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.40625, min=5.424022674560547e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=0.0001850128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.3125, min=0.00017833709716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=5.2928924560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.71875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.96875, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=9.489059448242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.265625, min=0.000965118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.3125, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=3.4809112548828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.625, min=0.0024871826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.71875, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.001556396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.1875, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=5.1021575927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.40625, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=3.5315752029418945e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.00131988525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.8125, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.25, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.75, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.0034942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.00174713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.625, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.0137939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.0, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.0, min=0.00188446044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.01202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=69.5, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=121.5, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.00909423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=71.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=105.0, min=0.0166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.014404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=178.0, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=93.5, min=0.0269775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=159.0, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=200.0, min=0.01092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=113.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=262.0, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.0040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=231.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.62890625, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.4483928680419922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=8.821487426757812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=111.0, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.03125, min=1.341104507446289e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.03125, min=1.138448715209961e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1484375, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=124.0, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3671875, min=2.7418136596679688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.84375, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.203125, min=2.2649765014648438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.609375, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6171875, min=3.5762786865234375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=5.14984130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.640625, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=1.2099742889404297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7109375, min=0.0004863739013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.671875, min=2.8967857360839844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.75, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.671875, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=4.363059997558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.125, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.90625, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.59375, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=5.2928924560546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.46875, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.8849968910217285e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=5.9604644775390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.75, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.875, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.5, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.78125, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=1.6808509826660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0004787445068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.00433349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=95.0, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.02734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.01129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=109.0, min=0.0218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=4.8160552978515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.75, min=0.0164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5625, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=119.5, min=0.0177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.00159454345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.0172119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=97.5, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.5, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=98.0, min=0.01483154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=95.5, min=0.016845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=95.5, min=6.914138793945312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=136.0, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.0019378662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=235.0, min=0.0194091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.0830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.0, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=276.0, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.0, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5546875, min=0.0003757476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.953125, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1328125, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=129.0, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.86328125, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.09375, min=8.96453857421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.65625, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.890625, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.09375, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0030975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.75, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=1.990795135498047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8828125, min=5.412101745605469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.375, min=1.1146068572998047e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.890625, min=0.0022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.875, min=5.054473876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=0.000690460205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.003570556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.21875, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.005462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.796875, min=2.7060508728027344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.84375, min=0.0003376007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.5, min=1.1444091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.03125, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.59375, min=2.181529998779297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.625, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.00011014938354492188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.00011873245239257812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.01263427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.00191497802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.75, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.125, min=0.0042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.01556396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.125, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.00628662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.75, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=9.012222290039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.00160980224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.0220947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.004852294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.75, min=0.00323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.06787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.00469970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.02099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=95.5, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=135.0, min=0.009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=0.0017852783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.01226806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=227.0, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.60546875, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=4.673004150390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.63671875, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.65234375, min=4.291534423828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.796875, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=127.0, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.9609375, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.078125, min=3.719329833984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5078125, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.359375, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.890625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.34375, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.78125, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=3.5017728805541992e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=5.0067901611328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.15625, min=0.0018463134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.953125, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.953125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.859375, min=2.8371810913085938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.65625, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.21875, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.796875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.71875, min=0.000858306884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.75, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=1.9550323486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=3.56137752532959e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.4375, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0625, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00023365020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=4.220008850097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0, min=0.00167083740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.84375, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=8.64267349243164e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.00176239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.75, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.625, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.0145263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.00958251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=4.6253204345703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.002777099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.03759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=5.936622619628906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.375, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.00689697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=69.5, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=109.5, min=0.01458740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=2.8252601623535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=107.0, min=0.00194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=102.0, min=0.0037841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.019775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=93.0, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0037841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=133.0, min=0.005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.02392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=104.5, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=166.0, min=0.01043701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.002838134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=244.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.0030670166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.046630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.00323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=227.0, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.60546875, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=4.5299530029296875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=105.0, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.66796875, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6328125, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2578125, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=94.0, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8046875, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=1.1205673217773438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.640625, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.71875, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=9.775161743164062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.109375, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=1.71661376953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.21875, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.21875, min=1.519918441772461e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=0.0004367828369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.875, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.375, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=0.00010347366333007812
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.796875, min=1.0609626770019531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=1.1086463928222656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.734375, min=0.00116729736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.875, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.75, min=0.0029449462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.6875, min=8.940696716308594e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.8125, min=0.0004367828369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.15625, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=3.7670135498046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.1875, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.0, min=0.0027008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.01177978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.003143310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.003936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.875, min=0.00011920928955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.00188446044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=64.0, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.0027313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=62.75, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.01422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=94.5, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.75, min=0.03466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.00188446044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.0284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.75, min=0.00011968612670898438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=143.0, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.01409912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.0, min=6.16908073425293e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=155.0, min=0.00860595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=88.5, min=0.0211181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=102.0, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=238.0, min=0.000537872314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.01708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=256.0, min=0.00958251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=227.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.60546875, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=3.6694109439849854e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.1875, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=103.5, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.70703125, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.67578125, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.296875, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=103.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.3125, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.921875, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.03125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.703125, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.09375, min=0.0004787445068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.4375, min=1.71661376953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.578125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=7.338821887969971e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=1.1324882507324219e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.8125, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=6.4373016357421875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=0.00011014938354492188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=5.745887756347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.25, min=0.00160980224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.00131988525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.6875, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.796875, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=5.6743621826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.1875, min=0.0003948211669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.000881195068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.4375, min=0.00433349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00010251998901367188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.75, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.875, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=7.212162017822266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.484375, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.00017833709716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.625, min=0.0031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00433349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.6875, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=5.078315734863281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.0018463134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=3.886222839355469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.00154876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.0, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.25, min=7.200241088867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.25, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.01483154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.625, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.75, min=0.006744384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.015380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.5, min=0.0033416748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.5, min=0.0262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.625, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.01904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=2.872943878173828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0037078857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.0001010894775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.0, min=0.0302734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.75, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=108.0, min=0.0172119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.0206298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.5, min=0.007568359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.75, min=0.025146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=69.0, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=133.0, min=0.014892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.02099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=146.0, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=175.0, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.06982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=75.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=260.0, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=0.01446533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=227.0, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.60546875, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.234375, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=0.00021839141845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=102.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.703125, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.67578125, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2109375, min=2.0742416381835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=102.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1484375, min=0.00010633468627929688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=5.476176738739014e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.515625, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.03125, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.296875, min=0.00012111663818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.007110595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.25, min=7.12275505065918e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=7.12275505065918e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.625, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.296875, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=2.1457672119140625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5625, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.578125, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=4.363059997558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.71875, min=7.009506225585938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.359375, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=1.952052116394043e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.359375, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=4.00543212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.25, min=0.000904083251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.78125, min=0.0027008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00022220611572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.5, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.78125, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.0004673004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.125, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.78125, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.25, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=4.500150680541992e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=7.3015689849853516e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.625, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.125, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.3125, min=0.00958251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=3.62396240234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.00872802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.5, min=0.0189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=0.00021648406982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.0047607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.0, min=0.00018978118896484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.01080322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.0, min=2.6106834411621094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=98.5, min=0.00506591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=95.0, min=0.0028839111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.75, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.01153564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.5, min=0.0001735687255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=121.0, min=0.006927490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.0211181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.5, min=0.0003337860107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=111.5, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=0.00010013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=166.0, min=0.010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.3125, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0296630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.25, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=196.0, min=9.72747802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.006134033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.003265380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0032501220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=276.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=2.5510787963867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.921875, min=6.079673767089844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5859375, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5546875, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.8984375, min=4.744529724121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=124.0, min=0.0022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8046875, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3125, min=4.839897155761719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8125, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.6875, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0390625, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.984375, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=3.844499588012695e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.421875, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=1.9669532775878906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.9375, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000461578369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.828125, min=0.00020313262939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.21875, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.25, min=0.00010728836059570312
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.34375, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.609375, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.40625, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=2.9802322387695312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.1875, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=2.765655517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.84375, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.96875, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=3.0279159545898438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.40625, min=0.00159454345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=4.00543212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.375, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0031890869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.0033416748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=0.020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.25, min=0.00116729736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.00165557861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.75, min=0.004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.625, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.25, min=3.0279159545898438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.5, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.25, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.0, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=64.5, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.125, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.0096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=135.0, min=0.00084686279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.047607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.75, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=82.0, min=0.008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=5.14984130859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.0130615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=64.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=175.0, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.5, min=0.0038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.2278556823730469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.25, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.0, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6796875, min=4.029273986816406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.66015625, min=2.2172927856445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0859375, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6484375, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.265625, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.0038299560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6328125, min=0.00010061264038085938
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=5.435943603515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9609375, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.921875, min=9.238719940185547e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9296875, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.078125, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.03125, min=6.341934204101562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.875, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.71875, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.5625, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=5.364418029785156e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.000537872314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0625, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=2.0265579223632812e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.000965118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=25.0, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.25, min=0.0028839111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.8125, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.75, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=4.553794860839844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=1.6838312149047852e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.25, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.3125, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=4.0531158447265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.75, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.00011491775512695312
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.15625, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.00010347366333007812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.0002307891845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.5, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.0091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=4.9114227294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.25, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.0037689208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=4.410743713378906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=6.556510925292969e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.000827789306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.0, min=0.006072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.375, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.625, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.25, min=0.00634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=180.0, min=0.01092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.006378173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.0002689361572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=137.0, min=0.00909423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.01202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.25, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=2.765655517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.005218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=117.0, min=0.007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.25, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=98.5, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=174.0, min=0.00543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=147.0, min=0.01470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.0184326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.75, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.75, min=0.0054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0004787445068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=7.390975952148438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=4.708766937255859e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.95703125, min=5.507469177246094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.68359375, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.234375, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1328125, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3671875, min=0.00011730194091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.4375, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4921875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.390625, min=2.47955322265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=84.0, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.765625, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5546875, min=3.0279159545898438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=1.8358230590820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.078125, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.03125, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=3.790855407714844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.03125, min=5.435943603515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.375, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.375, min=2.467632293701172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=0.0002841949462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.9375, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.375, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0625, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=1.7881393432617188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.1875, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=2.0503997802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.000186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.625, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.004638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.004974365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.015380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.25, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.01007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.00848388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.75, min=0.0019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.84375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.0, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.0035552978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.01153564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=4.2438507080078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=78.0, min=0.0142822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.25, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.0064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.5, min=0.0185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.6875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.04833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=98.5, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=131.0, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=192.0, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.125, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.003570556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.5, min=0.00015354156494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.25, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=247.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7265625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=3.24249267578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.953125, min=5.793571472167969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=112.0, min=0.005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5546875, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.78515625, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.546875, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=124.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.15625, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.92578125, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=0.00010442733764648438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.25, min=0.00022983551025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.265625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.00543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.28125, min=0.00084686279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7421875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.828125, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.15625, min=0.000827789306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7890625, min=1.4543533325195312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.75, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=5.745887756347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.25, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.5, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.46875, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.4375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.75, min=0.0028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.000827789306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.53125, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=2.8014183044433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.3125, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.375, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0625, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=8.535385131835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.3125, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=7.12275505065918e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.25, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.00021839141845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.625, min=4.76837158203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=0.005279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.625, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.5, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.25, min=0.02392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=82.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.005615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.0281982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=0.006805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.00775146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.25, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.00787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=115.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=164.0, min=0.015380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=89.5, min=0.036865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=112.5, min=1.8835067749023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=135.0, min=0.00994873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=91.5, min=0.019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=132.0, min=0.003936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0019378662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.5, min=0.052490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.75, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.3172626495361328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=9.417533874511719e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=111.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.46484375, min=5.626678466796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.83203125, min=7.724761962890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.375, min=8.881092071533203e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=153.0, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.65625, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.703125, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=3.218650817871094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.828125, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9921875, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.96875, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=1.6689300537109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.75, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.3125, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9921875, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=2.905726432800293e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0001850128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.75, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.296875, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.234375, min=6.389617919921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.000568389892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.0027313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=2.0384788513183594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9296875, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.84375, min=0.000675201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=8.106231689453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.0, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.203125, min=2.968311309814453e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.5625, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=1.0371208190917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.002655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.8358230590820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.003448486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.375, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5625, min=0.00592041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=6.3478946685791016e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=0.001739501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.125, min=0.00677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=4.410743713378906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.0027313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.75, min=0.00173187255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.000911712646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.00018024444580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.00121307373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.71875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.0186767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.0, min=2.8014183044433594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.00016117095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.75, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=69.5, min=0.00118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.0, min=0.00011682510375976562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.0, min=0.0069580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=101.5, min=0.019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=145.0, min=0.0123291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=1.1265277862548828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=194.0, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.25, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.023681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=135.0, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.003753662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=284.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.578125, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=4.863739013671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.625, min=0.00020313262939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=105.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6640625, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.65234375, min=5.626678466796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.83203125, min=3.159046173095703e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=158.0, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.85546875, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.84375, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.8125, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.09375, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9453125, min=9.179115295410156e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.125, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.84375, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.109375, min=2.384185791015625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=9.489059448242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.9375, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=4.76837158203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.1875, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=4.5299530029296875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.000431060791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.002838134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.171875, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.484375, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.125, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.4375, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5625, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.4375, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.0, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.8596649169921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0625, min=0.002227783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.46875, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=2.1457672119140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.4375, min=0.006927490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.0037689208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.00118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.6875, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.25, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=0.00118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.125, min=4.9114227294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.125, min=0.00116729736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.75, min=0.009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.01190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.25, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.75, min=0.0140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.0108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.25, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.01531982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.00830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.875, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=166.0, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=92.5, min=0.1162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=82.0, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=129.0, min=0.007293701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=90.0, min=0.0213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=77.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=177.0, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.025146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.0042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=328.0, min=2.6702880859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.83984375, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.98828125, min=2.872943878173828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9609375, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=116.5, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.60546875, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6640625, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9375, min=1.1175870895385742e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=152.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0234375, min=8.106231689453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=1.1444091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.484375, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.6193599700927734e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=3.266334533691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.03125, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9609375, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.09375, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=0.0003948211669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.59375, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.71875, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.71875, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.03125, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.59375, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=2.592802047729492e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.75, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.15625, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=2.4318695068359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.375, min=0.00107574462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.90625, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=5.888938903808594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.65625, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.5616416931152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.6875, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.75, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.96875, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=3.0994415283203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.00186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.1875, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=7.808208465576172e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.003875732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.5, min=0.00762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.0024261474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=1.8477439880371094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.875, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.4375, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.375, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.25, min=1.5974044799804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.875, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.00011682510375976562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.25, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.00567626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.004791259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.8125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.25, min=0.00118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=76.0, min=0.006317138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.00159454345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.5, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.01226806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.5, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=135.0, min=0.0052490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=111.0, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=148.0, min=0.01153564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.06591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=101.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=324.0, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.0019683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=1.8477439880371094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6875, min=4.00543212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.68359375, min=1.2874603271484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.94921875, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=159.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.74609375, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=4.792213439941406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6953125, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3828125, min=1.0907649993896484e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.203125, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=93.0, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.671875, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6796875, min=0.00010347366333007812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=0.00011396408081054688
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8359375, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=7.200241088867188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.0, min=0.000682830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7265625, min=1.2159347534179688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9140625, min=0.0003948211669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.84375, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=2.6226043701171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=3.147125244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.25, min=0.005462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.953125, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.921875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.75, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=0.0003376007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.625, min=0.003021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.78125, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=5.054473876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.125, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.0967254638671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=2.9921531677246094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.25, min=0.0036163330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.625, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.00174713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.375, min=0.007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.01116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.00299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.75, min=0.02197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.125, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.5, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.01025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=2.0265579223632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=69.0, min=0.00732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.000537872314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.005218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.75, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.5, min=0.015380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.5, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.0169677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=95.0, min=0.00982666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.96875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=0.00138092041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=119.0, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.005889892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=69.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=284.0, min=0.0294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=296.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.55859375, min=0.0004673004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.00323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.703125, min=4.553794860839844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.64453125, min=7.450580596923828e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.8984375, min=7.510185241699219e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=149.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.2109375, min=4.2438507080078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9375, min=4.2438507080078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.125, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.2890625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.140625, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.671875, min=7.331371307373047e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=0.000469207763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.34375, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.59375, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=2.574920654296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.078125, min=0.00015926361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=5.412101745605469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.375, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.28125, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=2.5480985641479492e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.21875, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.1875, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.00018978118896484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.25, min=0.0004711151123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0002613067626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=0.000743865966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=7.867813110351562e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.0, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.71875, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.00168609619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.6875, min=0.00579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=6.341934204101562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=4.2438507080078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.375, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.01129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.00011301040649414062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.00020503997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.25, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.125, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.00084686279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.0002384185791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.375, min=0.0001926422119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.0, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.0, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.0, min=0.01019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=76.0, min=0.002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.0218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.0, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.375, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.5, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.75, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=88.0, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=122.5, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.007415771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.75, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=174.0, min=0.005767822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.005584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.625, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=302.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.4375, min=3.3974647521972656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=1.0058283805847168e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71875, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.71875, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0234375, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.95703125, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3984375, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.046875, min=3.5762786865234375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.28125, min=4.887580871582031e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=8.487701416015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.6875, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.671875, min=1.329183578491211e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9140625, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.890625, min=3.6716461181640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.140625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.0, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.625, min=0.001556396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.1875, min=0.0004482269287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.953125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.90625, min=0.0030364990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.984375, min=5.078315734863281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.84375, min=9.000301361083984e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=0.00012111663818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.390625, min=7.390975952148438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.65625, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=8.702278137207031e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.001861572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.84375, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.03125, min=1.3470649719238281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.6875, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.000873565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.25, min=0.00051116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.0, min=9.202957153320312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=0.00170135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=5.9138983488082886e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.6875, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=5.1021575927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0003757476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.00014781951904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=0.0002765655517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.01214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=0.0047607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=4.1484832763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.125, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.5, min=0.021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=55.75, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.75, min=0.01153564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.5, min=0.000598907470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=81.0, min=0.007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.000812530517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.032470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.0, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.03125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=53.0, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.75, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.625, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=72.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=213.0, min=0.01019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.0, min=0.003082275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.75, min=0.0289306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.25, min=0.0233154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.00023174285888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=276.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7578125, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4375, min=8.344650268554688e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.640625, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.8125, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=103.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.73046875, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6875, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.96484375, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=179.0, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.76171875, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=5.304813385009766e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.953125, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.46875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5546875, min=7.271766662597656e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=90.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.046875, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=2.1576881408691406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.28125, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.25, min=0.00010251998901367188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.006591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.71875, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.796875, min=0.0021820068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.140625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=1.5348196029663086e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.3125, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.765625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.9375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.765625, min=0.00024318695068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.0625, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=2.276897430419922e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.15625, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.625, min=0.0004253387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.84375, min=9.000301361083984e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.53125, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.1875, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.1875, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.0004520416259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=0.00011682510375976562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.28125, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.125, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.625, min=0.00177764892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.005584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.0025787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.0, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.0036468505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.75, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.00018978118896484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.0, min=0.00084686279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=0.0001735687255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.125, min=0.0023956298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.375, min=0.005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.25, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.00142669677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.875, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.004669189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=1.8596649169921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.0118408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=2.110004425048828e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.0137939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=3.147125244140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.01123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=68.5, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=99.5, min=0.000804901123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.0185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.0, min=0.0003871917724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=171.0, min=0.00982666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.21875, min=0.00072479248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=154.0, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=152.0, min=0.0162353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=95.5, min=0.01519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=94.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=186.0, min=0.005828857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.75, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.0, min=0.0147705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=247.0, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.7265625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.984375, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=116.5, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6171875, min=0.00018978118896484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6875, min=0.000171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3125, min=5.9604644775390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=159.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.90625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.84765625, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.65625, min=1.3828277587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.65625, min=0.000659942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.546875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1171875, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=93.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.953125, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=1.7404556274414062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=8.630752563476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.203125, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9765625, min=5.632638931274414e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.21875, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.75, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=5.793571472167969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.53125, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.140625, min=4.410743713378906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=7.390975952148438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.921875, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.25, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.953125, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.828125, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.00078582763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.03125, min=0.00183868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.796875, min=0.000148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.625, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.875, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.00020885467529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.65625, min=0.002410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=6.389617919921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.6875, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.5625, min=0.0027008056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.625, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.0034027099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.6875, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=4.124641418457031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=24.5, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.25, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.0074462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.5, min=0.003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.75, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.0137939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.125, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=5.412101745605469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.0, min=0.006988525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.0, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.25, min=0.00726318359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=109.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=99.5, min=0.005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.1875, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.0030975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=92.0, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.02685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=64.0, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.625, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=94.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=95.5, min=0.00994873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=138.0, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=86.5, min=0.00113677978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=108.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=208.0, min=0.00634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.875, min=0.0003223419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.25, min=0.025146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=200.0, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.859375, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.171875, min=0.00010156631469726562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=100.5, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.72265625, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.73046875, min=0.00018978118896484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3046875, min=1.1175870895385742e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=160.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.734375, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.90625, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5078125, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.75, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.71875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.984375, min=0.00010442733764648438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.421875, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.890625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=0.00017070770263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.09375, min=1.9550323486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=5.841255187988281e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=1.436471939086914e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9765625, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.40625, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.671875, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.625, min=0.0035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.6875, min=0.00023365020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.9375, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.21875, min=0.000736236572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.00074005126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.09375, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=1.7523765563964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.00104522705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5625, min=0.0033721923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.0, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0, min=0.00506591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=3.528594970703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=7.963180541992188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=0.0004291534423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.59375, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.4375, min=0.0016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.0023956298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.125, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.3125, min=0.00011968612670898438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.25, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.0035247802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0004444122314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.75, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.0, min=0.003448486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.375, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.0, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.00171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.0, min=0.0167236328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.0, min=4.38690185546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.25, min=0.0107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.0, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=0.00897216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.125, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.022705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=69.0, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.000885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.00160980224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.0, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=70.5, min=0.0068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.0400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.0, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=145.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.00032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=95.5, min=0.000949859619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=152.0, min=0.01171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.0, min=0.009521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=114.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=241.0, min=0.010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=231.0, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.62890625, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.9453125, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=3.635883331298828e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=110.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.55859375, min=1.8358230590820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.68359375, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=0.0002269744873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.125, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.0032196044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0859375, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8203125, min=1.3887882232666016e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8515625, min=8.630752563476562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.703125, min=2.2649765014648438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.40625, min=2.4437904357910156e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.0004482269287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=0.0008392333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=8.702278137207031e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=2.491474151611328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=91.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8046875, min=3.123283386230469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=1.2576580047607422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3515625, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.0, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.375, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.8125, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.0032806396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.890625, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.1875, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=4.1484832763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.078125, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.5625, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.00012111663818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.0009918212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.4375, min=0.00159454345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0, min=0.0023956298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.46875, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.25, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.75, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.15625, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=8.568167686462402e-08
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.75, min=0.0030364990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.00043487548828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=1.3649463653564453e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.003875732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.1875, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.00958251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.00360107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=5.125999450683594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.0, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=0.00188446044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.00011873245239257812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.25, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.5, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.003692626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.75, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.007720947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=2.6702880859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.25, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=1.8835067749023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.5, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0302734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.75, min=7.3909759521484375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=82.5, min=0.01080322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.0, min=5.7220458984375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.0, min=0.0093994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=75.0, min=2.1219253540039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=127.5, min=0.00555419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.0, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=89.0, min=0.012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.75, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.0052490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=79.5, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=178.0, min=0.006439208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.00714111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=75.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=216.0, min=0.030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.5, min=0.01373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=0.026611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=276.0, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.5, min=4.887580871582031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=3.123283386230469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6015625, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.68359375, min=2.8967857360839844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.234375, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=137.0, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6640625, min=2.6106834411621094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0859375, min=0.00020122528076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3203125, min=0.00018215179443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.0, min=0.008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.8359375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.25, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=100.0, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.09375, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.5, min=0.000102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.875, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.28125, min=0.0008697509765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.53125, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.359375, min=3.457069396972656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.5, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.6875, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.375, min=2.5033950805664062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=0.0003948211669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.75, min=7.915496826171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.8125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.28125, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.59375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=0.00023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.6875, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=9.72747802734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.8125, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.9375, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=1.5735626220703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=4.863739013671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.125, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.8125, min=0.0002193450927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=1.9788742065429688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.25, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.875, min=0.0089111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=6.151199340820312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=5.3882598876953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.0625, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.0, min=0.004913330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=1.621246337890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.75, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.00665283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.0002040863037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.0, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.625, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.5, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=6.961822509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.0194091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.5, min=3.3229589462280273e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.625, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.25, min=0.0067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.25, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.75, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.007293701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.5, min=0.00909423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=0.0002193450927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=86.5, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.00848388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=99.0, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.000484466552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.0264892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.0, min=7.343292236328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=136.0, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.6875, min=0.0030670166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.004302978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.5, min=0.00046539306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=151.0, min=0.011474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.75, min=0.00640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=185.0, min=0.01116943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.00433349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.0062255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.0089111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=1.33514404296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.859375, min=0.0004100799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=113.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.6875, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.68359375, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0546875, min=2.956390380859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=126.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.69140625, min=3.361701965332031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.59375, min=9.775161743164062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.4375, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=1.633167266845703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9609375, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.1875, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=94.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.03125, min=0.0007476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.734375, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5625, min=0.0003108978271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.46875, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.8125, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=7.534027099609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.625, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.875, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.4375, min=0.00018787384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.375, min=6.246566772460938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.125, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=1.633167266845703e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.5, min=0.0022430419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.953125, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.9375, min=1.9788742065429688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.03125, min=0.00024127960205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.4375, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=1.2874603271484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.00019931793212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.1875, min=0.005889892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=4.3392181396484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.125, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=8.270144462585449e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=6.4849853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.375, min=0.00518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.875, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.375, min=0.000377655029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0625, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.625, min=9.679794311523438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.125, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.25, min=0.0038604736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.007232666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=8.940696716308594e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=62.0, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.75, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.375, min=0.00016021728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.625, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.25, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.5, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.00087738037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.25, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.0, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=70.0, min=1.7642974853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=88.5, min=0.03759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.01544189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.25, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.007049560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.0186767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=49.0, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.0123291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.0, min=0.033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.000637054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=164.0, min=0.00604248046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.75, min=0.0177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.0, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=104.5, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.75, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=92.5, min=0.00010013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=205.0, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.00091552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.25, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.00022983551025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=306.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.046875, min=2.1219253540039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=2.1219253540039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=7.390975952148438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.63671875, min=3.695487976074219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.72265625, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1171875, min=6.914138793945312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.8125, min=1.6689300537109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=114.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8359375, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=7.104873657226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0625, min=5.555152893066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.2890625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3828125, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.671875, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=0.0001888275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.00299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.078125, min=2.9921531677246094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=1.71661376953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=92.0, min=0.0019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.0625, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=1.6808509826660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=5.91278076171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.75, min=0.000354766845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.8125, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.921875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.8125, min=4.172325134277344e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.46875, min=0.00010776519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.984375, min=0.00012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.8125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=9.34600830078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=59.25, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.65625, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=0.00011014938354492188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.6875, min=0.0004482269287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.46875, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.5, min=0.0002384185791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.919269561767578e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=9.489059448242188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.3125, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.3125, min=0.0037841796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.000682830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.3125, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.0011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.00069427490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.375, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.00262451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.375, min=0.00010251998901367188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=0.000690460205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.00173187255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.625, min=0.0172119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.875, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=0.000530242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.0211181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.375, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.25, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.75, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.75, min=0.01348876953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.5, min=0.0079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.1875, min=8.153915405273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.0019989013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.625, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=122.5, min=0.0223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.5, min=0.002838134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.5, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=111.5, min=0.016357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.0, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=65.5, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=0.000675201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.00045013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=103.0, min=0.0133056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=75.0, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=119.0, min=0.006011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=80.0, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=88.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=286.0, min=0.01220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.01190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=1.895427703857422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=316.0, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5703125, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.109375, min=9.202957153320312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=100.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.81640625, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.81640625, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1015625, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.82421875, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=0.00011682510375976562
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.6875, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.0625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5625, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=9.918212890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=0.003326416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7421875, min=7.748603820800781e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.671875, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6796875, min=2.2202730178833008e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.859375, min=0.0019378662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=1.043081283569336e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=5.0067901611328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8359375, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7421875, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8671875, min=2.250075340270996e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.75, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=4.124641418457031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.859375, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.25, min=8.58306884765625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.328125, min=0.0021209716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=8.869171142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.0027618408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.15625, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=8.106231689453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.625, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.15625, min=8.678436279296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.34375, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.96875, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.1875, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.90625, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=5.3942203521728516e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.875, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.3125, min=0.000762939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=2.2649765014648438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5625, min=0.00084686279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.59375, min=9.775161743164062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.000957489013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.4375, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.0003566741943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=6.198883056640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.4375, min=0.00119781494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.25, min=0.00994873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.625, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.75, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=0.00640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.25, min=8.296966552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.125, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.75, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=47.0, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.0, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=59.75, min=0.005523681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.8125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.056396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=126.0, min=0.0026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.4375, min=3.0279159545898438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.0279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.5, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=89.0, min=0.00689697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=47.25, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.5, min=0.01177978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.000339508056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=120.5, min=0.02880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=152.0, min=7.539987564086914e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=108.5, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=104.0, min=0.048095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=61.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=76.0, min=0.011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.0074462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.25, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0002422332763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=260.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.76953125, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=5.103647708892822e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.5625, min=0.0001163482666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=112.0, min=0.0035552978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.62890625, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.75, min=2.658367156982422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1640625, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=96.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.0390625, min=6.031990051269531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.171875, min=0.0001583099365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.71875, min=0.00011348724365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.265625, min=5.0067901611328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.734375, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=3.337860107421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.15625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.15625, min=9.499490261077881e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.921875, min=0.000896453857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.015625, min=3.218650817871094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=9.965896606445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.003448486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.03125, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=3.838539123535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.453125, min=0.0022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.03125, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=1.895427703857422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.0, min=1.5616416931152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.5625, min=0.0004596710205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=1.2576580047607422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.0016326904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.000125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.53125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=6.0498714447021484e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.125, min=0.003326416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.03125, min=7.2479248046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.90625, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.2874603271484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.328125, min=0.00019550323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.8125, min=0.005279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.25, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.375, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.25, min=0.000926971435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.00750732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=5.269050598144531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=3.7670135498046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.8125, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.125, min=0.00170135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.00011444091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.0, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.006500244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.75, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.375, min=0.000522613525390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.004669189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.5, min=0.00016307830810546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.25, min=0.00106048583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.0003833770751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.5, min=0.06494140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=2.777576446533203e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=36.75, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.08349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.0, min=0.016357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0087890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.75, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.0, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=96.5, min=0.11083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=110.0, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.01434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=126.5, min=0.006011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.0, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=5.984306335449219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=96.0, min=0.050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.6875, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.75, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.25, min=0.0693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=4.982948303222656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=336.0, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.86328125, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.484375, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=0.00017642974853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=108.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.66796875, min=0.00020694732666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.68359375, min=0.00011587142944335938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2734375, min=3.0159950256347656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=132.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.3046875, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.96484375, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.625, min=7.82012939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0026092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.46875, min=0.00041961669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5078125, min=3.4570693969726562e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.546875, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=84.0, min=0.00885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.5, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.15625, min=6.16908073425293e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.3125, min=0.000446319580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.7734375, min=1.6927719116210938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.6875, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.515625, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6953125, min=1.2069940567016602e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.375, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.109375, min=0.000759124755859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=9.298324584960938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.5, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.46875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.578125, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=3.695487976074219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.0, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.00421142578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.0032196044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=2.193450927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.00038909912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.03125, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=0.00011014938354492188
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.75, min=0.0009002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.6875, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.0, min=0.0023193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.3125, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.90625, min=1.0251998901367188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.71875, min=0.00299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.25, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.125, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.5625, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.375, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.9375, min=0.00323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.75, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=48.0, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.0023956298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.625, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5625, min=0.001861572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.875, min=0.00506591796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=1.7642974853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.25, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.0001354217529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.0, min=0.005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.0, min=1.800060272216797e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.875, min=3.910064697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.0, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=1.9550323486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.0015411376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.75, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=74.5, min=0.0322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.25, min=5.7220458984375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=94.5, min=0.0003910064697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=60.25, min=0.015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=105.5, min=0.00469970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.625, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.01251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=104.5, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.0031890869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.00124359130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.0, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=101.5, min=1.3530254364013672e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=105.0, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.000934600830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.0020904541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=90.5, min=0.017578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=90.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=185.0, min=0.0166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.75, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=6.914138793945312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=318.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.67578125, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.90625, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=94.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8125, min=0.0004253387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8125, min=0.000331878662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1015625, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=149.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.76171875, min=3.552436828613281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.3046875, min=1.728534698486328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.46875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5546875, min=0.00010395050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=91.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.09375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.484375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.59375, min=0.00019741058349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=63.0, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=9.953975677490234e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.625, min=0.00060272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.53125, min=0.00022220611572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.453125, min=0.0003070831298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.125, min=0.00125885009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.3125, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.203125, min=0.0002384185791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.1875, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.21875, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.9375, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.0003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.15625, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=39.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.4375, min=0.000560760498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.71875, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=4.9114227294921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.625, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=2.1904706954956055e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.5625, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.75, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0625, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.96875, min=0.01190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=2.002716064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.625, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.0625, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.25, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.3125, min=0.0031585693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.5625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.875, min=0.0004482269287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.0625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.5625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.000820159912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=5.507469177246094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.375, min=0.005401611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.0, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.0123291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=7.450580596923828e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.0023956298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.000946044921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.375, min=0.0002002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.00164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.004638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00048065185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.052490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=3.719329833984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=0.00112152099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.0026397705078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.00665283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=4.863739013671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.0031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=103.0, min=0.0015411376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=110.0, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=99.5, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=72.0, min=0.05322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=48.25, min=0.00011968612670898438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=103.5, min=0.0026092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=107.5, min=0.0279541015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=212.0, min=0.00982666015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=57.75, min=0.01544189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.75, min=0.000301361083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.5, min=0.0029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=224.0, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.59375, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.59375, min=7.677078247070312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.00019359588623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=109.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.74609375, min=2.0265579223632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.74609375, min=0.00011157989501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.78125, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=142.0, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71875, min=3.3855438232421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=5.698204040527344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.71875, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9765625, min=3.528594970703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9453125, min=0.0001506805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=84.0, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.78125, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.890625, min=1.5616416931152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.8125, min=0.000652313232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=4.267692565917969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=1.3709068298339844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.828125, min=0.00018596649169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.796875, min=0.0001010894775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.8359375, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=0.00021648406982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=1.728534698486328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.21875, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.078125, min=7.12275505065918e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=0.0002536773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.75, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.671875, min=0.0003376007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=1.5616416931152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.00168609619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.0, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.53125, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.4375, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.53125, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.00201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.875, min=0.0004177093505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.6875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.5, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.59375, min=0.0019378662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.00022602081298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.25, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=5.0067901611328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.625, min=0.00122833251953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=0.0002346038818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.0625, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.0, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.78125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.75, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.5, min=0.003814697265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.09375, min=0.00061798095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.6875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.25, min=0.000881195068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.25, min=0.000942230224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=28.125, min=0.0062255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=3.170967102050781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.5, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=0.003021240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=19.75, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=45.25, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.375, min=0.0003414154052734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=37.25, min=0.0028533935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=37.25, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.0025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.00116729736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.25, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=25.0, min=0.00017261505126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.0, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.75, min=0.03466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.0126953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=4.57763671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.25, min=0.02783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=132.0, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.6875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.0, min=0.0030059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=79.0, min=0.0032501220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.0220947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=105.0, min=0.0004024505615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=131.0, min=0.0205078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.75, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=3.814697265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=45.5, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.00162506103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=5.0067901611328125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5, min=0.00017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.0181884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.25, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=167.0, min=0.0025482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.00494384765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=44.0, min=0.00022220611572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=304.0, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.71484375, min=4.0531158447265625e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.59375, min=5.841255187988281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=4.112720489501953e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=104.5, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.53125, min=2.4080276489257812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.6328125, min=2.3126602172851562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.140625, min=3.6716461181640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.0018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=143.0, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.84765625, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0703125, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5625, min=0.00011777877807617188
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.000362396240234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.546875, min=0.00013828277587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.96484375, min=4.076957702636719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=90.5, min=0.004241943359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.65625, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=1.811981201171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.5, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.625, min=1.9073486328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=69.5, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.734375, min=7.05718994140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.421875, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.000217437744140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9765625, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.078125, min=3.981590270996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.84375, min=3.4809112548828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.875, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.640625, min=5.817413330078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.984375, min=0.000232696533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.390625, min=0.000728607177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=2.6226043701171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.578125, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.5, min=1.3053417205810547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.00160980224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.984375, min=0.00010204315185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.375, min=5.4836273193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=0.000827789306640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.001190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.125, min=2.3245811462402344e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.78125, min=0.0006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.65625, min=7.677078247070312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.000713348388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.375, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=8.20159912109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.1875, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.00153350830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.78125, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=1.7762184143066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.625, min=9.632110595703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0014190673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.78125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.75, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.625, min=0.000873565673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.0, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.5, min=0.00860595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.0242919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=7.62939453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.0245361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.5, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.5, min=0.0028076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.5, min=0.000209808349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.0, min=0.00482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.03564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.5, min=0.00095367431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.5, min=0.027587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.000186920166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=58.75, min=0.003082275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.375, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0196533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=84.5, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=106.5, min=0.00775146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.375, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=95.0, min=0.0096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.5, min=1.895427703857422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=121.0, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.25, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.02099609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=76.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=135.0, min=0.017822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.0, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=55.25, min=0.0057373046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.25, min=0.004486083984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.00135040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=6.67572021484375e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=0.00011539459228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=92.0, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.53125, min=1.424551010131836e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.78515625, min=8.106231689453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.875, min=0.0001087188720703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=127.0, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.015625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1484375, min=3.075599670410156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4140625, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.875, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.4375, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0625, min=4.32133674621582e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4453125, min=9.275972843170166e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=6.29425048828125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=89.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.21875, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.671875, min=0.00019168853759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.953125, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6875, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.8125, min=8.7738037109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.00116729736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.0015716552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.0, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=44.0, min=0.000392913818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.875, min=8.058547973632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.09375, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=1.1444091796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.546875, min=0.00080108642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=1.0192394256591797e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=0.0002498626708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.65625, min=0.0002574920654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.828125, min=2.2649765014648438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0015106201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=58.0, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.984375, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.00021648406982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.828125, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.9375, min=0.00179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=0.00162506103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.71875, min=5.0067901611328125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=46.25, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.625, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0625, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.6875, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.00127410888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.00022792816162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.00130462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.25, min=0.00244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.375, min=0.004058837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.4375, min=4.744529724121094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=0.0003604888916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.28125, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=2.3365020751953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.625, min=0.004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.75, min=0.00445556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.1875, min=0.00025177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.5, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.000751495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.5, min=0.006011962890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0014801025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.6875, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.006378173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=0.000583648681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.75, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.625, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.006683349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.5, min=0.00011491775512695312
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.75, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.25, min=0.0050048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=71.0, min=0.00830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.3125, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.875, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.5, min=0.01483154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.75, min=5.793571472167969e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.5, min=0.01318359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=121.0, min=0.0004749298095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.3125, min=9.965896606445312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.004669189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=55.75, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.01519775390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=110.0, min=1.8835067749023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=174.0, min=0.0078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.84375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=95.0, min=0.002044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=227.0, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.15625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.02978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.75, min=0.00012874603271484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=207.0, min=0.0155029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.5, min=0.03173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.25, min=0.0027923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.00135040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.375, min=1.4454126358032227e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=103.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.640625, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.7109375, min=1.043081283569336e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.89453125, min=6.973743438720703e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=156.0, min=0.0009307861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.93359375, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=0.0001316070556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.6640625, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.000347137451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.984375, min=0.00151824951171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.4375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.2421875, min=0.0001430511474609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=72.5, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.0, min=0.00311279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.78125, min=2.1219253540039062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.125, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.375, min=1.33514404296875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.002685546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.203125, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.953125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.453125, min=0.000545501708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.00168609619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.890625, min=0.00011301040649414062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5234375, min=9.760260581970215e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.140625, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.25, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.796875, min=0.00022983551025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=2.4437904357910156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.296875, min=0.00018024444580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.625, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.875, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=1.9073486328125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.359375, min=0.00023746490478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.00115966796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.359375, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.0, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.75, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=1.6242265701293945e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=4.76837158203125e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.3125, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.4375, min=1.0907649993896484e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.75, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.000400543212890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0625, min=0.0015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.9375, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=3.314018249511719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.78125, min=0.00086212158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.000316619873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.96875, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.96875, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=0.00102996826171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.01141357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.004638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.0020294189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.4375, min=0.0103759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.25, min=0.00017833709716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.00066375732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.0, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=38.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=0.0023345947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=21.25, min=0.00010824203491210938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.0021514892578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.0, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=0.00872802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=24.125, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.0, min=0.00176239013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=9.5367431640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=46.75, min=0.01043701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.75, min=6.031990051269531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.75, min=0.00130462646484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.01806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.25, min=0.0001468658447265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.5, min=0.0004558563232421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.28125, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.875, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=93.0, min=0.005615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=57.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=25.25, min=0.0004405975341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.25, min=0.0224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=46.25, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.25, min=0.00872802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.9375, min=0.00055694580078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.75, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=59.75, min=0.000942230224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=71.0, min=0.0045166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=94.0, min=0.044677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=110.0, min=0.0037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=114.0, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.46875, min=0.000728607177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.5, min=0.0084228515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=95.5, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=174.0, min=0.0281982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.34375, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0625, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.0194091796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.00531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.016845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=8.535385131835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.00035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=251.0, min=0.00135040283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=5.602836608886719e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.125, min=0.0002155303955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.3125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=101.0, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.734375, min=9.870529174804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.82421875, min=1.0132789611816406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.90234375, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=153.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8671875, min=2.9802322387695312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.203125, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=6.341934204101562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.0, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.03125, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1328125, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.00023365020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.625, min=4.4345855712890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.4375, min=2.765655517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=65.0, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.78125, min=0.00110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.765625, min=1.7642974853515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.890625, min=0.000156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.390625, min=0.002532958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=3.838539123535156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.4453125, min=2.09808349609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.4375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.125, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.796875, min=2.47955322265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=9.584426879882812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.0005340576171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.59375, min=0.00439453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.015625, min=4.458427429199219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.00142669677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.546875, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.0003719329833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.00092315673828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=47.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.9375, min=0.00099945068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.421875, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.03125, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.4375, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.125, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.75, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.65625, min=0.00408935546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=1.9431114196777344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.6875, min=0.00026702880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.65625, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.625, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.65625, min=0.00012302398681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.5, min=0.00093841552734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.6875, min=0.00170135498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.125, min=0.00171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.25, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.00115203857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.00156402587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.75, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=26.625, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0003452301025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.5, min=0.00171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.0005035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.5, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=43.5, min=0.00225830078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.625, min=0.00933837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=55.25, min=0.00012159347534179688
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.0011749267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.375, min=0.000701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.625, min=0.00323486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.004364013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.9375, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.5, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.0, min=0.0028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=35.0, min=0.000141143798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=47.75, min=0.004302978515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.875, min=0.00070953369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.024658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.0, min=9.052455425262451e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.0, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.09375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=28.0, min=0.000782012939453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.5, min=4.458427429199219e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=83.0, min=0.00628662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.25, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.01806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=103.5, min=0.008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.00022411346435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.875, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=68.5, min=0.007110595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=81.5, min=0.006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.0, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=96.5, min=0.006683349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=97.0, min=0.001922607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=169.0, min=0.0169677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.5625, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.625, min=0.0008087158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=91.5, min=0.0201416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=151.0, min=0.00726318359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.0002899169921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5625, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=64.0, min=0.0012664794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=262.0, min=0.0081787109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=2.2411346435546875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.25, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.068359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=50.0, min=4.76837158203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=104.0, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=318.0, min=0.0038299560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.67578125, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=6.008148193359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.671875, min=3.361701965332031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=106.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.75, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.75, min=5.245208740234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.90625, min=7.152557373046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=131.0, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.109375, min=3.203749656677246e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1796875, min=3.62396240234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.390625, min=4.863739013671875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.984375, min=0.000591278076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.3046875, min=3.0279159545898438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.0, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.15625, min=0.0019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.234375, min=1.5735626220703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.234375, min=4.410743713378906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=1.430511474609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=66.5, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.609375, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=1.996755599975586e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.609375, min=2.0742416381835938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.265625, min=0.000919342041015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=6.586313247680664e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=6.437301635742188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=2.288818359375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=57.25, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.34375, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.734375, min=1.952052116394043e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.25, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.171875, min=0.0002880096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.171875, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.0003376007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.4375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.5, min=0.001312255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.15625, min=0.003753662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=0.00128936767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.5, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.75, min=0.00142669677734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.125, min=5.173683166503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.9375, min=0.00014972686767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.125, min=0.000286102294921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.4375, min=0.004547119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.125, min=0.00013065338134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=0.00020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=45.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.5625, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.5625, min=1.8596649169921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=19.125, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.875, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=2.9921531677246094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.75, min=5.1021575927734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.875, min=0.000293731689453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.75, min=0.003997802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.625, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=27.375, min=0.00152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.0625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.5, min=0.00021839141845703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=22.25, min=0.001434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.1875, min=0.0028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.6875, min=0.00012493133544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.875, min=0.0013885498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.0, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.75, min=0.006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=3.409385681152344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.375, min=0.0009613037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.375, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.0002689361572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=21.875, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.5, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.625, min=1.5735626220703125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=3.719329833984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.0, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.000453948974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.00433349609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=56.0, min=9.059906005859375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.625, min=0.000396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=66.0, min=0.000202178955078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=71.5, min=1.4841556549072266e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=91.0, min=0.0026092529296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.25, min=0.0003337860107421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=96.0, min=0.00811767578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.000499725341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.046630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=72.0, min=0.00189208984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.875, min=0.0001697540283203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=115.0, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=113.0, min=0.000614166259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=118.0, min=0.0022735595703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.5, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=144.0, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=78.5, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=160.0, min=0.0142822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.375, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.00213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=99.0, min=0.00023365020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=298.0, min=0.003326416015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.000888824462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=58.5, min=0.015869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=34.25, min=0.0145263671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=209.0, min=0.00011873245239257812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.1640625, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.4453125, min=0.00023937225341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.875, min=0.00014781951904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=99.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.56640625, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.66796875, min=8.249282836914062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.82421875, min=2.9325485229492188e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.00139617919921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=170.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.8984375, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1796875, min=2.592802047729492e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.7265625, min=0.0002651214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.0, min=0.00054931640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.984375, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.515625, min=1.2695789337158203e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=77.5, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.609375, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.515625, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=7.295608520507812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.5, min=5.340576171875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.005615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.7265625, min=6.198883056640625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.09375, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.921875, min=5.662441253662109e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.46875, min=4.363059997558594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.46875, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5078125, min=3.5762786865234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.3125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.6875, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.6953125, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.984375, min=0.00014400482177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.671875, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.3125, min=2.816319465637207e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.0, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.53125, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.125, min=0.000865936279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.000278472900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.28125, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=2.849102020263672e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.5625, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.3125, min=0.001617431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.8125, min=0.0003681182861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=49.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5, min=0.000789642333984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.000507354736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00012159347534179688
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.1875, min=0.00098419189453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.5625, min=0.00174713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=4.482269287109375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.15625, min=3.4332275390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.125, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.0036773681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.5625, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0001544952392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=15.625, min=0.000370025634765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0001049041748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=55.0, min=0.00034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.9375, min=9.441375732421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=6.735324859619141e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.96875, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=0.003082275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.75, min=0.000179290771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=35.25, min=0.0002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=37.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=17.875, min=3.170967102050781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=37.5, min=0.004150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.75, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=25.125, min=4.792213439941406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=13.6875, min=0.00015544891357421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.5, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.125, min=0.0029144287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=38.25, min=0.006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=32.75, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=32.5, min=0.0029449462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=43.5, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.0, min=0.00013637542724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.25, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.25, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=62.75, min=0.0023040771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=36.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=39.25, min=0.00154876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.4375, min=0.001800537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=65.5, min=0.0308837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.75, min=0.0003299713134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=93.5, min=0.0030670166015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.0023956298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=104.0, min=0.00064849853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=63.25, min=0.01007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=104.0, min=0.01531982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=115.0, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.01165771484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.375, min=0.0048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=77.0, min=0.02587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=79.0, min=3.933906555175781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=120.0, min=0.015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.5625, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.001129150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=89.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=205.0, min=0.02880859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.125, min=0.001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.5, min=0.0238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=52.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=49.25, min=0.005523681640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=260.0, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.76953125, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.453125, min=3.2335519790649414e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.21875, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=102.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.5859375, min=5.9604644775390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.66796875, min=5.9604644775390625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.9609375, min=0.000720977783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=145.0, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.28125, min=9.870529174804688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.1796875, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.515625, min=0.00017452239990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=80.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=1.5854835510253906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.5625, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.609375, min=9.822845458984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.5, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=89.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.8125, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=0.00010156631469726562
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.125, min=4.1961669921875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=1.71661376953125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=78.5, min=0.000621795654296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.328125, min=0.0004138946533203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9453125, min=1.704692840576172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.84375, min=0.0003490447998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.125, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.296875, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.296875, min=3.337860107421875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.84375, min=0.000274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.71875, min=7.772445678710938e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=3.170967102050781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.875, min=0.0003528594970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.000476837158203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.21875, min=0.00058746337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.609375, min=2.47955322265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=6.818771362304688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.375, min=6.628036499023438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.34375, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=5.316734313964844e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=26.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=56.0, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.6875, min=0.000728607177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.28125, min=3.1948089599609375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.5, min=6.67572021484375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.0625, min=0.0004863739013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.4375, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.5, min=0.000823974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.0, min=0.00148773193359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.28125, min=0.00194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=0.00022602081298828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.625, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.15625, min=0.002288818359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.59375, min=4.124641418457031e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.000797271728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5, min=0.00096893310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=30.375, min=0.0002231597900390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.75, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.4375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=11.0625, min=0.0012054443359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=10.0625, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.00145721435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=13.875, min=0.000698089599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=4.172325134277344e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=29.75, min=0.00133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.25, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.0, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.5, min=8.0108642578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=31.75, min=0.0011138916015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.09375, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=19.625, min=0.00469970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=21.875, min=0.0031890869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.375, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.0, min=0.0042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.0, min=0.000133514404296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.75, min=0.000514984130859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=0.00286865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.006134033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=29.625, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.25, min=0.0155029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.000152587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.0, min=0.000118255615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.006866455078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=107.5, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=95.5, min=0.0037078857421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.875, min=0.0004367828369140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0625, min=0.0001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=39.0, min=0.01190185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=81.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=100.0, min=0.0220947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.3125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=56.0, min=0.00787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=63.0, min=0.0003643035888671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=51.5, min=0.0306396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.53125, min=0.0002956390380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.75, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.0, min=0.0074462890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.000629425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=111.0, min=0.0128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.00049591064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.0, min=1.52587890625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.0234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=30.125, min=1.1622905731201172e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=80.5, min=0.00836181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.21875, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=81.5, min=0.004791259765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=73.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=142.0, min=0.0013580322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=34.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.00701904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=42.0, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=336.0, min=0.0006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.86328125, min=0.000225067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.59375, min=8.344650268554688e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.765625, min=0.00011205673217773438
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=109.5, min=0.001251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.70703125, min=0.00023555755615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.7421875, min=8.916854858398438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.15625, min=0.00010919570922851562
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.375, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=147.0, min=7.2479248046875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.765625, min=0.000576019287109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.8984375, min=0.000164031982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.546875, min=0.0001811981201171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=74.0, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.984375, min=0.00090789794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.96875, min=4.9173831939697266e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.95703125, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=95.5, min=0.0010833740234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.28125, min=9.250640869140625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.5390625, min=4.649162292480469e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=6.532669067382812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=75.5, min=0.0002918243408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.953125, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=1.3470649719238281e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.9375, min=0.00021457672119140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.5625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.1875, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.1875, min=2.491474151611328e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.9453125, min=2.3484230041503906e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.0, min=4.9591064453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.5, min=0.0002460479736328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.40625, min=0.00028228759765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0625, min=2.2530555725097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.578125, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.9375, min=0.0008544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.625, min=0.00390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.40625, min=0.000705718994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.09375, min=4.380941390991211e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.6875, min=0.0010528564453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.000385284423828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.6875, min=0.00022029876708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.890625, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=3.2901763916015625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.00274658203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.625, min=0.00171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.859375, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.0625, min=0.000408172607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.3125, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.625, min=0.00341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.53125, min=2.9802322387695312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.90625, min=0.00021266937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=9.25, min=0.000255584716796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=51.5, min=0.00335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.125, min=0.005096435546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.58306884765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=0.000720977783203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.25, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.0, min=0.000438690185546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.875, min=0.00164794921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.1875, min=2.086162567138672e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.9375, min=0.0003757476806640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.875, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.03125, min=0.002716064453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.65625, min=0.00083160400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.1875, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=52.0, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=10.25, min=0.00063323974609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=9.393692016601562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.625, min=0.00173187255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.1875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.875, min=6.103515625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=14.0625, min=0.0002803802490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=19.0, min=0.00037384033203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=35.25, min=0.0012969970703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=16.75, min=0.00079345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=14.375, min=1.296401023864746e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.75, min=0.0001068115234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.00146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=22.75, min=0.00628662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.00016689300537109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=24.125, min=0.00396728515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.875, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=24.375, min=0.01251220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.00019073486328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=52.5, min=0.0018157958984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.5, min=0.00014495849609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=36.25, min=0.0019378662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.375, min=0.00010538101196289062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0003662109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.34375, min=0.000213623046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=25.875, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.006256103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=67.0, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=53.0, min=0.0125732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0004730224609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.875, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=54.75, min=0.003387451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=43.75, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.001007080078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.9375, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.0034332275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=51.75, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.0016937255859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.125, min=0.00144195556640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=50.5, min=0.003173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.5, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.01025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.875, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.0, min=0.00732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=58.0, min=0.0004024505615234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=110.5, min=0.0211181640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.875, min=0.001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.0, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.0076904296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=55.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=140.0, min=0.0167236328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.8125, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=51.75, min=0.029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.75, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=43.5, min=0.013671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.0, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=292.0, min=0.002471923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.296875, min=0.00011110305786132812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.296875, min=7.867813110351562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.8125, min=0.0001392364501953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.00067901611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=102.0, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.63671875, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.70703125, min=5.7697296142578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=0.8984375, min=8.225440979003906e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=149.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.921875, min=2.944469451904297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.0078125, min=0.00011873245239257812
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.125, min=6.580352783203125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.6875, min=0.0003204345703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.0, min=1.2665987014770508e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=7.62939453125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.4375, min=0.000308990478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=85.0, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.125, min=0.0002994537353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.203125, min=4.220008850097656e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.65625, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=21.625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=76.0, min=0.00128173828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.46875, min=0.00010585784912109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.34375, min=0.00011587142944335938
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.1875, min=0.0004062652587890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=11.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=73.5, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.953125, min=0.0007781982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.921875, min=4.601478576660156e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.5, min=0.0002689361572265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=48.5, min=0.0004119873046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9140625, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.40625, min=2.3126602172851562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.484375, min=0.00101470947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.9375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=24.125, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=4.25, min=0.000774383544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=16.75, min=0.00081634521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.5, min=0.002105712890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=32.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.09375, min=0.0003147125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.734375, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.875, min=3.248453140258789e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=53.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.015625, min=0.0012359619140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.6875, min=0.00015163421630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.0003986358642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.90625, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=20.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.15625, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.65625, min=0.0004634857177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=13.25, min=0.00022983551025390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.9375, min=0.000606536865234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=50.25, min=0.0005950927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.765625, min=0.00171661376953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=8.440017700195312e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.765625, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.0020751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.78125, min=0.00189971923828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.000553131103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=30.5, min=0.00075531005859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=15.8125, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=31.625, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.09375, min=0.00052642822265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.00012063980102539062
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=11.8125, min=0.00054168701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.6875, min=0.000728607177734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.3125, min=0.00057220458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.4375, min=8.058547973632812e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=12.875, min=0.0004444122314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=14.25, min=0.000270843505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=15.5625, min=0.0034942626953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=12.0625, min=0.000492095947265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=28.125, min=0.0017242431640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=36.5, min=0.00018310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.125, min=3.62396240234375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=33.0, min=0.002166748046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.0013275146484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=12.625, min=0.000457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.5, min=0.012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=22.75, min=0.00013256072998046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=57.0, min=0.00030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.0, min=0.0004425048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.375, min=3.814697265625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=27.25, min=0.0036163330078125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=26.25, min=0.0010986328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=40.75, min=0.00150299072265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.875, min=0.0001678466796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=23.0, min=0.0013427734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=33.0, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=27.625, min=0.0002079010009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=73.5, min=0.0035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=4.9375, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.03076171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=41.75, min=2.8133392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.75, min=0.00787353515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0006103515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.5625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=82.5, min=0.01055908203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=53.25, min=0.0002727508544921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=97.0, min=0.01373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=0.00014019012451171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.125, min=0.00238037109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=61.25, min=0.01214599609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=60.5, min=0.00015735626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=61.25, min=0.00136566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.90625, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=27.125, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=63.25, min=0.005218505859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=110.0, min=0.0001239776611328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=107.0, min=0.010498046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=70.5, min=0.056640625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=56.75, min=5.5789947509765625e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.25.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=118.0, min=0.01373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=6.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=10.8125, min=0.000732421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.04833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=87.5, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.26.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=192.0, min=0.01336669921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=33.25, min=8.392333984375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=59.0, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=39.75, min=0.00031280517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.27.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=38.75, min=0.0020599365234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=169.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.15625, min=1.2218952178955078e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.15625, min=5.459785461425781e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.0.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.46875, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=13.375, min=0.00040435791015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=113.0, min=0.00067138671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=0.75, min=8.153915405273438e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=0.70703125, min=8.106231689453125e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.1.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.0859375, min=0.0003509521484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=22.0, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=150.0, min=0.00177001953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.09375, min=0.00013446807861328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.109375, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.2.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.1953125, min=2.181529998779297e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.5, min=0.0001125335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=79.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.046875, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.6875, min=4.2282044887542725e-07
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.3.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=1.125, min=6.341934204101562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=67.5, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.078125, min=0.0018463134765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.59375, min=2.5391578674316406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.4.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.3125, min=6.866455078125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=20.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=64.5, min=0.0030517578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.78125, min=7.486343383789062e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=2.328125, min=3.743171691894531e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.5.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=3.96875, min=0.00109100341796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.0017547607421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=71.5, min=0.00077056884765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=1.9609375, min=0.00014209747314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9609375, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.6.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.25, min=0.000194549560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.25, min=0.006805419921875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=3.59375, min=0.000644683837890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=1.9140625, min=2.4199485778808594e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.7.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=2.578125, min=0.00042724609375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=21.125, min=0.0029296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=2.890625, min=0.000579833984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=3.40625, min=2.6226043701171875e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.8.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.53125, min=0.0001659393310546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=12.25, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.5, min=0.00140380859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.65625, min=0.0035858154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=4.0, min=2.2292137145996094e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.9.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.0007171630859375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.000640869140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=42.0, min=0.0001220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.03125, min=0.000972747802734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.03125, min=1.7762184143066406e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.10.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=4.34375, min=4.351139068603516e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=16.25, min=0.000843048095703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=8.25, min=5.692243576049805e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=11.0625, min=0.0004329681396484375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.11.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=14.5625, min=0.000263214111328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=40.75, min=0.001495361328125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=6.46875, min=9.834766387939453e-06
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=1.9311904907226562e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.12.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=5.25, min=0.0005645751953125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.75, min=0.000766754150390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=9.625, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=5.8125, min=0.0024566650390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=6.0625, min=1.609325408935547e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.13.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=6.4375, min=0.000324249267578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.75, min=0.001708984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=29.0, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=7.09375, min=0.0003032684326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=5.5, min=0.0001277923583984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.14.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=10.625, min=0.000850677490234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=60.5, min=0.0002593994140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=9.875, min=0.00147247314453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=9.125, min=0.000240325927734375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.15.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=17.625, min=0.001373291015625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=15.9375, min=0.0010223388671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=12.0, min=0.005157470703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=15.375, min=0.0003261566162109375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.16.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0004215240478515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=14.8125, min=0.00029754638671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=41.75, min=0.00048828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=17.25, min=1.895427703857422e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=16.875, min=0.00021076202392578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.17.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=60.0, min=0.001983642578125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=7.59375, min=0.0002117156982421875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=13.25, min=0.000518798828125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=23.75, min=0.003082275390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=23.25, min=0.000110626220703125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.18.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=41.5, min=0.0030364990234375
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=9.1875, min=7.43865966796875e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=18.0, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=35.5, min=0.00970458984375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=33.5, min=0.0001621246337890625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.19.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=46.0, min=0.004608154296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=17.375, min=0.000244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=22.875, min=0.002197265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=32.5, min=0.00799560546875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=42.25, min=0.00016880035400390625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.20.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=66.5, min=0.002349853515625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.40625, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=26.125, min=0.000335693359375
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=44.25, min=0.0023651123046875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=62.0, min=0.00012683868408203125
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.21.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=90.0, min=0.0059814453125
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=18.125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=11.5, min=9.1552734375e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=83.0, min=0.000835418701171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=74.5, min=0.0003185272216796875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.22.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=131.0, min=0.000247955322265625
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=8.8125, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=17.75, min=0.0009765625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=52.5, min=0.00022125244140625
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=54.5, min=0.0006561279296875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.23.attn.moba_output: dtype=torch.bfloat16, shape=(1, 2048), max=86.0, min=0.00457763671875
[0;36m(EngineCore_DP0 pid=193965)[0;0m Running SseMobaFlashAttentionImpl forward with is_moba=False, type of attn_metadata: <class 'vllm.v1.attention.backends.flash_attn.FlashAttentionMetadata'>
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_q: dtype=torch.bfloat16, shape=(1, 2048), max=5.6875, min=0.0
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_k: dtype=torch.bfloat16, shape=(1, 1024), max=19.25, min=3.0517578125e-05
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_v: dtype=torch.bfloat16, shape=(1, 1024), max=86.0, min=0.01434326171875
[0;36m(EngineCore_DP0 pid=193965)[0;0m [GOOD] model.layers.24.attn.moba_o: dtype=torch.bfloat16, shape=(1, 2048), max=85.5, min=0.00075531005859375[0;36m(APIServer pid=193648)[0;0m INFO:     127.0.0.1:43642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:45:30 [loggers.py:248] Engine 000: Avg prompt throughput: 1.5 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=193648)[0;0m INFO 02-27 07:45:40 [loggers.py:248] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
