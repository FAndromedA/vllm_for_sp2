import torch
import os
from transformers import AutoModel, AutoModelForCausalLM, AutoConfig

from sse_swa_moba_hf.configuration_sse_swa_moba_hf import SSESWAMoBAConfig
from sse_swa_moba_hf.modeling_sse_swa_moba_hf import SSESWAMoBAForCausalLM, SSESWAMoBAModel

AutoConfig.register(SSESWAMoBAConfig.model_type, SSESWAMoBAConfig, exist_ok=True)
AutoModel.register(SSESWAMoBAConfig, SSESWAMoBAModel, exist_ok=True)
AutoModelForCausalLM.register(SSESWAMoBAConfig, SSESWAMoBAForCausalLM, exist_ok=True)

def save_model_param_names(model_name_or_path, output_file="weights_pureswa1.txt"):
    """
    ä½¿ç”¨AutoModelåŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒ.binæ ¼å¼æƒé‡ï¼‰ï¼Œæå–æ‰€æœ‰å‚æ•°åç§°å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶
    
    Args:
        model_name_or_path (str): æ¨¡å‹åç§°ï¼ˆå¦‚bert-base-chineseï¼‰æˆ–åŒ…å«.binæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
        output_file (str): è¾“å‡ºå‚æ•°åç§°çš„æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤æ˜¯å½“å‰ç›®å½•çš„ weights.txt
    """
    try:
        # åŠ è½½æ¨¡å‹é…ç½®ï¼ˆé¿å…ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼‰
        config = AutoConfig.from_pretrained(model_name_or_path)
        
        # ä»æŒ‡å®šè·¯å¾„åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨è¯†åˆ«.binæƒé‡æ–‡ä»¶ï¼‰
        # map_location='cpu' ç¡®ä¿æ— GPUä¹Ÿèƒ½è¿è¡Œ
        model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            config=config,
        )
        
        # æå–æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„åç§°
        # named_parameters() åªè¿”å›å¯è®­ç»ƒå‚æ•°ï¼Œnamed_parameters(recurse=True) é€’å½’è·å–æ‰€æœ‰å±‚
        param_names = [name for name, _ in model.named_parameters(recurse=True)]
        
        # æå–æ¨¡å‹æ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬ä¸å¯è®­ç»ƒçš„ï¼Œå¦‚LayerNormçš„running_meanç­‰ï¼‰
        # å¦‚éœ€åŒ…å«æ‰€æœ‰å‚æ•°ï¼Œå¯æ›¿æ¢ä¸ºï¼šparam_names = [name for name, _ in model.named_parameters()] + [name for name, _ in model.named_buffers()]
        
        # å°†å‚æ•°åç§°å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            for idx, name in enumerate(param_names, 1):
                f.write(f"{name}\n")
        
        print(f"âœ… æˆåŠŸï¼å…±æå–åˆ° {len(param_names)} ä¸ªå‚æ•°åç§°")
        print(f"ğŸ“„ å‚æ•°åç§°å·²ä¿å­˜åˆ°ï¼š{os.path.abspath(output_file)}")

        model.save_pretrained('/mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_moba_gdn_u1to3_pureSwa_1.7b_dense_lr3en5_min0p1_bsz64_ep1_aux1en3_pt_data_800k/modeling2')
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{e}")
        print("\nè¯·æ£€æŸ¥ï¼š")
        print("1. è¾“å…¥çš„æ¨¡å‹åç§°/è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼ˆå¦‚bert-base-chineseï¼‰")
        print("2. æ–‡ä»¶å¤¹ä¸­æ˜¯å¦åŒ…å«pytorch_model.binå’Œconfig.jsonæ–‡ä»¶")
        print("3. å·²å®‰è£…æœ€æ–°ç‰ˆtransformersï¼špip install --upgrade transformers")


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # è¾“å…¥æ¨¡å‹åç§°ï¼ˆå¦‚bert-base-chineseï¼‰æˆ–åŒ…å«.binæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
    model_path = input("è¯·è¾“å…¥æ¨¡å‹åç§°/åŒ…å«.binæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼š").strip()
    
    # è°ƒç”¨å‡½æ•°æ‰§è¡Œä¿å­˜æ“ä½œ
    save_model_param_names(model_path)

# /mnt/jfzn/pyq/ColossalAI-dev/checkpoints/sse_swa128_drop0p5_moba4k_top12_4b_lr5en6_bsz32_pt69p86_ct512k5btk_sft500k_rsft500k_24k_aux1en4/modeling
