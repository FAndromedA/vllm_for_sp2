#!/usr/bin/env python3
"""
MOBA Attention ç­‰ä»·æ€§æµ‹è¯•è„šæœ¬
è¯¥è„šæœ¬ç”¨äºéªŒè¯HuggingFaceé£æ ¼çš„MOBAå®ç°ä¸vLLMé£æ ¼çš„MOBAå®ç°
åœ¨è®­ç»ƒæ¨¡å¼ã€æ¨ç†prefillé˜¶æ®µå’Œæ¨ç†decodeé˜¶æ®µçš„è¾“å‡ºç­‰ä»·æ€§ã€‚
"""

import os
import sys
import torch
import numpy as np
import torch.nn as nn
from typing import Optional, Tuple, Dict, Any

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)

# å°è¯•å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from moba_attn_hf import MoBAAttention as HFMobaAttention
    from moba_attn import VLLMMoBAAttention
    from fla.models.utils import Cache
    from transformers.utils import logging
    logger = logging.get_logger(__name__)
    hf_available = True
except ImportError as e:
    logger.error(f"Failed to import HuggingFace MOBA Attention: {e}")
    hf_available = False

try:
    from vllm.model_executor.layers.attention import AttentionMetadata
    from vllm.model_executor.layers.attention import set_forward_context, get_forward_context
    from vllm.model_executor.layers.attention import CUDAGraphMode
    from vllm.distributed import get_tensor_model_parallel_world_size, get_tensor_model_parallel_rank
    from vllm.distributed import initialize_model_parallel, destroy_model_parallel
    from vllm.config import CacheConfig, QuantizationConfig
    vllm_available = True
except ImportError as e:
    logger.error(f"Failed to import vLLM modules: {e}")
    vllm_available = False

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

def create_attention_metadata(
    batch_size: int,
    seq_len: int,
    use_cu_seqlens: bool = False,
    seq_lens: Optional[list] = None
) -> Dict[str, Any]:
    """åˆ›å»ºæ³¨æ„åŠ›å…ƒæ•°æ®"""
    if use_cu_seqlens and seq_lens is not None:
        num_actual_tokens = sum(seq_lens)
        cu_seqlens = torch.zeros(batch_size + 1, dtype=torch.int32)
        for i in range(batch_size):
            cu_seqlens[i+1] = cu_seqlens[i] + seq_lens[i]
        max_seqlen = max(seq_lens)
    else:
        num_actual_tokens = batch_size * seq_len
        cu_seqlens = torch.arange(0, num_actual_tokens + 1, seq_len, dtype=torch.int32)
        max_seqlen = seq_len
    
    return {
        "num_actual_tokens": num_actual_tokens,
        "cu_seqlens": cu_seqlens,
        "max_seqlen": max_seqlen,
        "num_prefill_tokens": num_actual_tokens,
        "num_decode_tokens": 0,
        "num_prefills": batch_size if use_cu_seqlens else 0,
        "num_decodes": 0
    }

def test_weight_loading():
    """æµ‹è¯•1ï¼šéªŒè¯vLLMæ¨¡å‹èƒ½å¦åŠ è½½HuggingFaceä¿å­˜çš„æƒé‡"""
    print("\n=== æµ‹è¯•1ï¼šæƒé‡åŠ è½½æµ‹è¯• ===")
    
    if not hf_available:
        print("âš ï¸ HuggingFace MOBA Attentionä¸å¯ç”¨ï¼Œè·³è¿‡æƒé‡åŠ è½½æµ‹è¯•")
        return None, None
    
    if not vllm_available:
        print("âš ï¸ vLLM MOBA Attentionä¸å¯ç”¨ï¼Œè·³è¿‡æƒé‡åŠ è½½æµ‹è¯•")
        return None, None
    
    # 1. åˆ›å»ºHuggingFaceæ¨¡å‹ - ä½¿ç”¨bfloat16
    hf_moba = HFMobaAttention(
        hidden_size=512,
        num_heads=8,
        num_kv_heads=8,
        head_dim=64,
        qkv_bias=False,
        qk_norm=False,
        window_size=128,
        rope_theta=10000.0,
        moba_chunk_size=1024,
        moba_topk=4,
        max_position_embeddings=4096,
        layer_idx=0,
        norm_eps=1e-5
    ).to(device, dtype=torch.bfloat16)
    
    print("âœ“ HuggingFace MoBAAttentionæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # 2. ä¿å­˜HuggingFaceæƒé‡
    hf_weights = hf_moba.state_dict()
    torch.save(hf_weights, "hf_moba_weights.pth")
    
    # 3. åˆ›å»ºvLLMæ¨¡å‹
    vllm_config = CacheConfig()
    quant_config = QuantizationConfig()
    
    vllm_moba = VLLMMoBAAttention(
        hidden_size=512,
        num_heads=8,
        num_kv_heads=8,
        head_dim=64,
        qkv_bias=False,
        qk_norm=False,
        window_size=128,
        rope_theta=10000.0,
        moba_chunk_size=1024,
        moba_topk=4,
        max_position_embeddings=4096,
        layer_idx=0,
        norm_eps=1e-5,
        cache_config=vllm_config,
        quant_config=quant_config,
        prefix="model.layers.0.attention"
    ).to(device, dtype=torch.bfloat16)
    
    print("âœ“ vLLM VLLMMoBAAttentionæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # 4. åŠ è½½HuggingFaceæƒé‡åˆ°vLLMæ¨¡å‹
    # éœ€è¦æ‰‹åŠ¨æ˜ å°„æƒé‡åç§°
    hf_to_vllm = {
        "q_proj.weight": "qkv_proj.weight_q",
        "k_proj.weight": "qkv_proj.weight_k",
        "v_proj.weight": "qkv_proj.weight_v",
        "q_proj.bias": "qkv_proj.bias_q",
        "k_proj.bias": "qkv_proj.bias_k",
        "v_proj.bias": "qkv_proj.bias_v",
        "o_proj.weight": "o_proj.weight",
        "o_proj.bias": "o_proj.bias",
        "q_norm.weight": "q_norm.weight",
        "k_norm.weight": "k_norm.weight"
    }
    
    vllm_weights = {}
    for hf_key, vllm_key in hf_to_vllm.items():
        if hf_key in hf_weights:
            vllm_weights[vllm_key] = hf_weights[hf_key]
    
    # åŠ è½½æƒé‡
    vllm_moba.load_state_dict(vllm_weights, strict=False)
    print("âœ“ HuggingFaceæƒé‡æˆåŠŸåŠ è½½åˆ°vLLMæ¨¡å‹")
    
    return hf_moba, vllm_moba

def test_forward_equivalence(hf_moba, vllm_moba):
    """æµ‹è¯•2ï¼šéªŒè¯å‰å‘ä¼ æ’­è¾“å‡ºç­‰ä»·æ€§"""
    print("\n=== æµ‹è¯•2ï¼šå‰å‘ä¼ æ’­ç­‰ä»·æ€§æµ‹è¯• ===")
    
    if not hf_moba or not vllm_moba:
        print("âš ï¸ æ¨¡å‹ä¸å¯ç”¨ï¼Œè·³å‰å‘ä¼ æ’­æµ‹è¯•")
        return
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)
    np.random.seed(42)
    
    # æµ‹è¯•ä¸åŒçš„è¾“å…¥é…ç½®
    test_cases = [
        # (batch_size, seq_len, use_cu_seqlens, description)
        (1, 32, False, "å•æ ·æœ¬ï¼Œå›ºå®šé•¿åº¦"),
        (2, 64, False, "å¤šæ ·æœ¬ï¼Œå›ºå®šé•¿åº¦"),
        (2, 64, True, "å¤šæ ·æœ¬ï¼Œå˜é•¿åºåˆ—(cu_seqlens)"),
    ]
    
    for batch_size, seq_len, use_cu_seqlens, description in test_cases:
        print(f"\næµ‹è¯•ç”¨ä¾‹: {description}")
        
        # åˆ›å»ºè¾“å…¥æ•°æ® - ä½¿ç”¨bfloat16
        hidden_states = torch.randn(batch_size, seq_len, hf_moba.hidden_size, dtype=torch.bfloat16).to(device)
        
        # åˆ›å»ºæ³¨æ„åŠ›æ©ç ï¼ˆç”¨äºå˜é•¿åºåˆ—ï¼‰
        attention_mask = None
        cu_seqlens = None
        actual_seq_lens = None
        
        if use_cu_seqlens:
            # åˆ›å»ºå˜é•¿åºåˆ—é•¿åº¦ï¼ˆæ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„åºåˆ—ï¼‰
            actual_seq_lens = [seq_len - i*5 for i in range(batch_size)]
            max_len = max(actual_seq_lens)
            
            # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
            attention_mask = torch.zeros(batch_size, max_len).to(device)
            for i, length in enumerate(actual_seq_lens):
                attention_mask[i, :length] = 1
            
            # åˆ›å»ºcu_seqlens
            cu_seqlens = torch.zeros(batch_size + 1, dtype=torch.int32).to(device)
            for i in range(batch_size):
                cu_seqlens[i+1] = cu_seqlens[i] + actual_seq_lens[i]
            
            # è°ƒæ•´è¾“å…¥åºåˆ—é•¿åº¦
            hidden_states = hidden_states[:, :max_len, :]
        else:
            # å›ºå®šé•¿åº¦åºåˆ—
            attention_mask = torch.ones(batch_size, seq_len).to(device)
        
        # 1. HuggingFaceå‰å‘ä¼ æ’­
        hf_output, _, _ = hf_moba(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            cu_seqlens=cu_seqlens if use_cu_seqlens else None,
            output_attentions=False,
            use_cache=False
        )
        
        # 2. vLLMå‰å‘ä¼ æ’­
        # vLLMéœ€è¦positionså‚æ•°
        positions = torch.arange(0, hidden_states.shape[1], device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)
        
        # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
        if vllm_available:
            vllm_config = CacheConfig()
            attn_metadata = create_attention_metadata(
                batch_size=batch_size,
                seq_len=hidden_states.shape[1],
                use_cu_seqlens=use_cu_seqlens,
                seq_lens=actual_seq_lens
            )
            
            with set_forward_context(
                attn_metadata=attn_metadata,
                vllm_config=vllm_config,
                num_tokens=attn_metadata["num_actual_tokens"],
                cudagraph_runtime_mode=CUDAGraphMode.NONE
            ):
                vllm_output, _, _ = vllm_moba(
                    hidden_states=hidden_states,
                    positions=positions,
                    attention_mask=attention_mask,
                    output_attentions=False,
                    use_cache=False
                )
        else:
            # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
            vllm_output, _, _ = vllm_moba(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                output_attentions=False,
                use_cache=False
            )
        
        # 3. æ£€æŸ¥è¾“å‡ºç­‰ä»·æ€§
        assert hf_output.shape == vllm_output.shape, \
            f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {hf_output.shape} vs {vllm_output.shape}"
        
        max_diff = torch.max(torch.abs(hf_output - vllm_output))
        mean_diff = torch.mean(torch.abs(hf_output - vllm_output))
        
        print(f"  è¾“å‡ºå½¢çŠ¶: {hf_output.shape}")
        print(f"  æœ€å¤§å·®å¼‚: {max_diff.item():.6f}")
        print(f"  å¹³å‡å·®å¼‚: {mean_diff.item():.6f}")
        
        if max_diff < 1e-5:
            print(f"  âœ“ è¾“å‡ºç­‰ä»·æ€§éªŒè¯é€šè¿‡")
        else:
            print(f"  âœ— è¾“å‡ºå·®å¼‚è¿‡å¤§: {max_diff.item()}")
            raise AssertionError(f"è¾“å‡ºä¸ç­‰ä»·ï¼Œæœ€å¤§å·®å¼‚: {max_diff.item()}")

def test_inference_prefill(hf_moba, vllm_moba):
    """æµ‹è¯•3ï¼šéªŒè¯æ¨ç†prefillé˜¶æ®µ"""
    print("\n=== æµ‹è¯•3ï¼šæ¨ç†Prefillé˜¶æ®µæµ‹è¯• ===")
    
    if not hf_moba or not vllm_moba:
        print("âš ï¸ æ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡prefillæµ‹è¯•")
        return None, None
    
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    hf_moba.eval()
    vllm_moba.eval()
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ - ä½¿ç”¨bfloat16
    batch_size = 2
    seq_len = 64
    hidden_states = torch.randn(batch_size, seq_len, hf_moba.hidden_size, dtype=torch.bfloat16).to(device)
    attention_mask = torch.ones(batch_size, seq_len).to(device)
    
    # 1. HuggingFace prefill
    hf_output, _, hf_past = hf_moba(
        hidden_states=hidden_states,
        attention_mask=attention_mask,
        output_attentions=False,
        use_cache=True
    )
    
    # 2. vLLM prefill
    positions = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)
    
    # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
    if vllm_available:
        vllm_config = CacheConfig()
        attn_metadata = create_attention_metadata(
            batch_size=batch_size,
            seq_len=seq_len,
            use_cu_seqlens=False
        )
        
        with set_forward_context(
            attn_metadata=attn_metadata,
            vllm_config=vllm_config,
            num_tokens=attn_metadata["num_actual_tokens"],
            cudagraph_runtime_mode=CUDAGraphMode.NONE
        ):
            vllm_output, _, vllm_past = vllm_moba(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                output_attentions=False,
                use_cache=True
            )
    else:
        # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
        vllm_output, _, vllm_past = vllm_moba(
            hidden_states=hidden_states,
            positions=positions,
            attention_mask=attention_mask,
            output_attentions=False,
            use_cache=True
        )
    
    # 3. æ£€æŸ¥è¾“å‡ºç­‰ä»·æ€§
    assert hf_output.shape == vllm_output.shape, f"Prefillè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
    
    max_diff = torch.max(torch.abs(hf_output - vllm_output))
    print(f"Prefillè¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")
    
    if max_diff < 1e-5:
        print("âœ“ Prefillé˜¶æ®µéªŒè¯é€šè¿‡")
    else:
        print(f"âœ— Prefillé˜¶æ®µè¾“å‡ºä¸ç­‰ä»·")
        raise AssertionError(f"Prefillè¾“å‡ºä¸ç­‰ä»·")
    
    return hf_past, vllm_past

def test_inference_decode(hf_moba, vllm_moba, hf_past, vllm_past):
    """æµ‹è¯•4ï¼šéªŒè¯æ¨ç†decodeé˜¶æ®µ"""
    print("\n=== æµ‹è¯•4ï¼šæ¨ç†Decodeé˜¶æ®µæµ‹è¯• ===")
    
    if not hf_moba or not vllm_moba:
        print("âš ï¸ æ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡decodeæµ‹è¯•")
        return
    
    if hf_past is None or vllm_past is None:
        print("âš ï¸ PastçŠ¶æ€ä¸ºNoneï¼Œè·³è¿‡decodeæµ‹è¯•")
        return
    
    # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    hf_moba.eval()
    vllm_moba.eval()
    
    batch_size = 2
    decode_steps = 5
    
    for step in range(decode_steps):
        print(f"\nDecode step {step+1}:")
        
        # åˆ›å»ºdecodeè¾“å…¥ï¼ˆå•tokenï¼‰- ä½¿ç”¨bfloat16
        hidden_states = torch.randn(batch_size, 1, hf_moba.hidden_size, dtype=torch.bfloat16).to(device)
        
        # 1. HuggingFace decode
        hf_output, _, hf_past = hf_moba(
            hidden_states=hidden_states,
            past_key_values=hf_past,
            output_attentions=False,
            use_cache=True
        )
        
        # 2. vLLM decode
        # vLLMéœ€è¦positionså‚æ•°ï¼Œè¿™é‡Œä½¿ç”¨ä¹‹å‰çš„ä½ç½®+1
        positions = torch.full((batch_size, 1), 64 + step, device=hidden_states.device)
        
        # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
        if vllm_available:
            vllm_config = CacheConfig()
            # åˆ›å»ºdecodeé˜¶æ®µçš„attention metadata
            attn_metadata = create_attention_metadata(
                batch_size=batch_size,
                seq_len=1,  # decodeé˜¶æ®µæ¯æ¬¡å¤„ç†1ä¸ªtoken
                use_cu_seqlens=False
            )
            
            # æ›´æ–°decodeç›¸å…³çš„å…ƒæ•°æ®
            attn_metadata.update({
                "num_actual_tokens": batch_size * 1,  # decodeé˜¶æ®µæ¯æ¬¡å¤„ç†1ä¸ªtoken
                "num_prefill_tokens": 0,
                "num_decode_tokens": batch_size * 1,
                "num_prefills": 0,
                "num_decodes": batch_size
            })
            
            with set_forward_context(
                attn_metadata=attn_metadata,
                vllm_config=vllm_config,
                num_tokens=attn_metadata["num_actual_tokens"],
                cudagraph_runtime_mode=CUDAGraphMode.NONE
            ):
                vllm_output, _, vllm_past = vllm_moba(
                    hidden_states=hidden_states,
                    positions=positions,
                    past_key_values=vllm_past,
                    output_attentions=False,
                    use_cache=True
                )
        else:
            # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
            vllm_output, _, vllm_past = vllm_moba(
                hidden_states=hidden_states,
                positions=positions,
                past_key_values=vllm_past,
                output_attentions=False,
                use_cache=True
            )
        
        # 3. æ£€æŸ¥è¾“å‡ºç­‰ä»·æ€§
        assert hf_output.shape == vllm_output.shape, f"Decodeè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
        
        max_diff = torch.max(torch.abs(hf_output - vllm_output))
        print(f"  è¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")
        
        if max_diff < 1e-5:
            print(f"  âœ“ Decode step {step+1} éªŒè¯é€šè¿‡")
        else:
            print(f"  âœ— Decode step {step+1} è¾“å‡ºä¸ç­‰ä»·")
            raise AssertionError(f"Decodeè¾“å‡ºä¸ç­‰ä»·")

def test_training_mode(hf_moba, vllm_moba):
    """æµ‹è¯•5ï¼šéªŒè¯è®­ç»ƒæ¨¡å¼"""
    print("\n=== æµ‹è¯•5ï¼šè®­ç»ƒæ¨¡å¼æµ‹è¯• ===")
    
    if not hf_moba or not vllm_moba:
        print("âš ï¸ æ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒæ¨¡å¼æµ‹è¯•")
        return
    
    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    hf_moba.train()
    vllm_moba.train()
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)
    
    # åˆ›å»ºè¾“å…¥æ•°æ® - ä½¿ç”¨bfloat16
    batch_size = 2
    seq_len = 64
    hidden_states = torch.randn(batch_size, seq_len, hf_moba.hidden_size, dtype=torch.bfloat16).to(device)
    attention_mask = torch.ones(batch_size, seq_len).to(device)
    
    # æ·»åŠ æ¢¯åº¦è®¡ç®—
    hidden_states.requires_grad_(True)
    
    # 1. HuggingFaceè®­ç»ƒå‰å‘
    hf_output, _, _ = hf_moba(
        hidden_states=hidden_states,
        attention_mask=attention_mask,
        output_attentions=False,
        use_cache=False
    )
    
    # 2. vLLMè®­ç»ƒå‰å‘
    positions = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)
    
    # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
    if vllm_available:
        vllm_config = CacheConfig()
        attn_metadata = create_attention_metadata(
            batch_size=batch_size,
            seq_len=seq_len,
            use_cu_seqlens=False
        )
        
        with set_forward_context(
            attn_metadata=attn_metadata,
            vllm_config=vllm_config,
            num_tokens=attn_metadata["num_actual_tokens"],
            cudagraph_runtime_mode=CUDAGraphMode.NONE
        ):
            vllm_output, _, _ = vllm_moba(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                output_attentions=False,
                use_cache=False
            )
    else:
        # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
        vllm_output, _, _ = vllm_moba(
            hidden_states=hidden_states,
            positions=positions,
            attention_mask=attention_mask,
            output_attentions=False,
            use_cache=False
        )
    
    # 3. æ£€æŸ¥è¾“å‡ºç­‰ä»·æ€§
    assert hf_output.shape == vllm_output.shape, f"è®­ç»ƒæ¨¡å¼è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
    
    max_diff = torch.max(torch.abs(hf_output - vllm_output))
    print(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")
    
    # 4. æµ‹è¯•åå‘ä¼ æ’­
    hf_loss = hf_output.sum()
    hf_loss.backward(retain_graph=True)
    
    vllm_loss = vllm_output.sum()
    vllm_loss.backward()
    
    # 5. æ£€æŸ¥æ¢¯åº¦ï¼ˆç®€å•æ£€æŸ¥ï¼ŒvLLMå¹¶è¡Œå±‚å¯èƒ½æœ‰ç‰¹æ®Šå¤„ç†ï¼‰
    try:
        # æ¯”è¾ƒq_projæƒé‡çš„æ¢¯åº¦
        if hasattr(hf_moba, 'q_proj') and hasattr(vllm_moba, 'qkv_proj'):
            hf_q_grad = hf_moba.q_proj.weight.grad
            if hf_q_grad is not None and hasattr(vllm_moba.qkv_proj, 'weight'):
                # vLLMçš„qkv_projå¯èƒ½æ˜¯å¹¶è¡Œçº¿æ€§å±‚ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                if hasattr(vllm_moba.qkv_proj, 'weight_q'):
                    vllm_q_grad = vllm_moba.qkv_proj.weight_q.grad
                elif vllm_moba.qkv_proj.weight is not None:
                    vllm_q_grad = vllm_moba.qkv_proj.weight.grad[:hf_q_grad.shape[0]]
                else:
                    vllm_q_grad = None
                
                if vllm_q_grad is not None:
                    grad_max_diff = torch.max(torch.abs(hf_q_grad - vllm_q_grad))
                    print(f"æ¢¯åº¦æœ€å¤§å·®å¼‚: {grad_max_diff.item():.6f}")
                    
                    if max_diff < 1e-5 and grad_max_diff < 1e-5:
                        print("âœ“ è®­ç»ƒæ¨¡å¼éªŒè¯é€šè¿‡")
                    else:
                        print(f"âœ— è®­ç»ƒæ¨¡å¼éªŒè¯å¤±è´¥")
                        raise AssertionError(f"è®­ç»ƒæ¨¡å¼ä¸ç­‰ä»·")
                else:
                    print("âš ï¸ vLLM qkv_projæƒé‡æ¢¯åº¦ä¸å¯ç”¨ï¼Œè·³è¿‡æ¢¯åº¦æ£€æŸ¥")
            else:
                print("âš ï¸ HuggingFace q_projæƒé‡æ¢¯åº¦ä¸å¯ç”¨ï¼Œè·³è¿‡æ¢¯åº¦æ£€æŸ¥")
        else:
            print("âš ï¸ æ¨¡å‹ç»“æ„ä¸åŒ¹é…ï¼Œè·³è¿‡æ¢¯åº¦æ£€æŸ¥")
    except Exception as e:
        print(f"âš ï¸ æ¢¯åº¦æ£€æŸ¥å‡ºé”™: {e}")
        print("  è¿™æ˜¯å› ä¸ºvLLMçš„å¹¶è¡Œçº¿æ€§å±‚å¯èƒ½æœ‰ç‰¹æ®Šçš„æ¢¯åº¦å¤„ç†æœºåˆ¶")
        print("  å¸¸è§åŸå› ï¼š")
        print("  1. å¹¶è¡Œçº¿æ€§å±‚é»˜è®¤requires_grad=False")
        print("  2. åˆ†å¸ƒå¼ç¯å¢ƒä¸­æ¢¯åº¦éœ€è¦ç‰¹æ®Šæ”¶é›†")
        print("  3. å¹¶è¡Œçº¿æ€§å±‚æœ‰è‡ªå·±çš„æ¢¯åº¦ç®¡ç†æœºåˆ¶")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*70)
    print("MOBA Attention ç­‰ä»·æ€§æµ‹è¯•")
    print("æ”¯æŒ HuggingFace å’Œ vLLM ä¸¤ç§å®ç°çš„ç­‰ä»·æ€§éªŒè¯")
    print("="*70)
    
    try:
        # åˆå§‹åŒ–vLLMåˆ†å¸ƒå¼ç¯å¢ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if vllm_available:
            if initialize_model_parallel(1):
                print("âœ“ vLLMæ¨¡å‹å¹¶è¡Œç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
            else:
                print("âš ï¸ vLLMæ¨¡å‹å¹¶è¡Œç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")
        
        # æµ‹è¯•1ï¼šæƒé‡åŠ è½½
        hf_moba, vllm_moba = test_weight_loading()
        
        if not torch.cuda.is_available():
            print(f"\nâš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œè·³è¿‡éœ€è¦GPUçš„æµ‹è¯•")
            return
        
        if not hf_moba or not vllm_moba:
            print(f"\nâš ï¸ è­¦å‘Šï¼šæ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­æµ‹è¯•")
            return
        
        # æµ‹è¯•2ï¼šå‰å‘ä¼ æ’­ç­‰ä»·æ€§
        test_forward_equivalence(hf_moba, vllm_moba)
        
        # æµ‹è¯•3ï¼šæ¨ç†prefillé˜¶æ®µ
        hf_past, vllm_past = test_inference_prefill(hf_moba, vllm_moba)
        
        # æµ‹è¯•4ï¼šæ¨ç†decodeé˜¶æ®µ
        test_inference_decode(hf_moba, vllm_moba, hf_past, vllm_past)
        
        # æµ‹è¯•5ï¼šè®­ç»ƒæ¨¡å¼
        test_training_mode(hf_moba, vllm_moba)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ‰€æœ‰MOBA Attentionç­‰ä»·æ€§æµ‹è¯•é€šè¿‡ï¼")
        print("âœ“ HuggingFaceå’ŒvLLMå®ç°å®Œå…¨ç­‰ä»·")
        print("="*70)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists("hf_moba_weights.pth"):
            os.remove("hf_moba_weights.pth")
            print("\nâœ“ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
        
        # æ¸…ç†vLLMåˆ†å¸ƒå¼ç¯å¢ƒ
        if vllm_available:
            destroy_model_parallel()
            print("âœ“ vLLMæ¨¡å‹å¹¶è¡Œç¯å¢ƒå·²æ¸…ç†")

if __name__ == "__main__":
    main() 