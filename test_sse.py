"""
SSE (Sparse State Expansion) å±‚ç­‰ä»·æ€§æµ‹è¯•è„šæœ¬
è¯¥è„šæœ¬ç”¨äºéªŒè¯HuggingFaceé£æ ¼çš„SSEå®ç°ä¸vLLMé£æ ¼çš„SSEå®ç°
åœ¨è®­ç»ƒæ¨¡å¼ã€æ¨ç†prefillé˜¶æ®µå’Œæ¨ç†decodeé˜¶æ®µçš„è¾“å‡ºç­‰ä»·æ€§ã€‚
"""
import os
import math
import torch
import torch.nn as nn
import numpy as np
from functools import partial
from typing import Optional, Tuple, Dict, List

from fla.models.utils import Cache

# è¿‡æ»¤æ‰å¯èƒ½çš„å¼ƒç”¨è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å¯¼å…¥vLLMå¹¶è¡Œåˆå§‹åŒ–æ¨¡å—ï¼ˆä¸å¯¼å…¥destroy_world_groupï¼‰
try:
    from vllm.distributed.parallel_state import (
        initialize_model_parallel,
        destroy_model_parallel,
        get_tensor_model_parallel_world_size,
        get_tensor_model_parallel_rank,
        get_world_group
    )
    from vllm.distributed import init_distributed_environment
    # æ·»åŠ vLLM forward_contextæ”¯æŒ
    from vllm.config import get_current_vllm_config, CUDAGraphMode
    from vllm.forward_context import set_forward_context
    vllm_available = True
except ImportError as e:
    print(f"âš ï¸ vLLMå¹¶è¡Œæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    vllm_available = False


def manual_weight_initialization(module):
    """æ‰‹åŠ¨æƒé‡éšæœºåˆå§‹åŒ–å‡½æ•°"""
    if isinstance(module, nn.Linear):
        nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            nn.init.ones_(module.bias)
    elif isinstance(module, nn.LayerNorm):
        nn.init.normal_(module.weight)
        nn.init.ones_(module.bias)
    elif hasattr(module, 'weight') and module.weight is not None:
        if len(module.weight.shape) > 1:
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
        else:
            nn.init.ones_(module.weight)


def create_attention_metadata(batch_size, seq_len, use_cu_seqlens=False, seq_lens=None):
    """
    åˆ›å»ºæ­£ç¡®çš„AttentionMetadata
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_len: åºåˆ—é•¿åº¦
        use_cu_seqlens: æ˜¯å¦ä½¿ç”¨å˜é•¿åºåˆ—
        seq_lens: å˜é•¿åºåˆ—çš„å®é™…é•¿åº¦åˆ—è¡¨
        
    Returns:
        AttentionMetadataå­—å…¸
    """
    if use_cu_seqlens and seq_lens is not None:
        # å˜é•¿åºåˆ—å¤„ç†
        max_len = max(seq_lens)
        num_actual_tokens = sum(seq_lens)
        
        # åˆ›å»ºquery_start_loc (cumulative sum of sequence lengths)
        query_start_loc = torch.zeros(batch_size + 1, dtype=torch.int32, device=device)
        for i in range(batch_size):
            query_start_loc[i+1] = query_start_loc[i] + seq_lens[i]
        
        seq_lens_tensor = torch.tensor(seq_lens, dtype=torch.int32, device=device)
        
        # åˆ›å»ºè™šæ‹Ÿçš„block_tableå’Œslot_mappingï¼ˆæµ‹è¯•ç”¨ï¼‰
        block_size = 16  # å‡è®¾çš„å—å¤§å°
        max_blocks_per_seq = (max_len + block_size - 1) // block_size
        block_table = torch.zeros((batch_size, max_blocks_per_seq), dtype=torch.int32, device=device)
        slot_mapping = torch.zeros(num_actual_tokens, dtype=torch.int32, device=device)
        
        attn_metadata = {
            "num_actual_tokens": num_actual_tokens,
            "max_query_len": max_len,
            "query_start_loc": query_start_loc,
            "max_seq_len": max_len,
            "seq_lens": seq_lens_tensor,
            "block_table": block_table,
            "slot_mapping": slot_mapping,
            "num_prefill_tokens": num_actual_tokens,
            "num_decode_tokens": 0,
            "num_prefills": batch_size,
            "num_decodes": 0
        }
    else:
        # å®šé•¿åºåˆ—å¤„ç†
        num_actual_tokens = batch_size * seq_len
        
        # åˆ›å»ºquery_start_loc (æ¯ä¸ªåºåˆ—ä»0å¼€å§‹ï¼Œé•¿åº¦ä¸ºseq_len)
        query_start_loc = torch.arange(0, num_actual_tokens + 1, seq_len,
                                     dtype=torch.int32, device=device)
        
        seq_lens_tensor = torch.full((batch_size,), seq_len, dtype=torch.int32, device=device)
        
        # åˆ›å»ºè™šæ‹Ÿçš„block_tableå’Œslot_mappingï¼ˆæµ‹è¯•ç”¨ï¼‰
        block_size = 16  # å‡è®¾çš„å—å¤§å°
        max_blocks_per_seq = (seq_len + block_size - 1) // block_size
        block_table = torch.zeros((batch_size, max_blocks_per_seq), dtype=torch.int32, device=device)
        slot_mapping = torch.zeros(num_actual_tokens, dtype=torch.int32, device=device)
        
        attn_metadata = {
            "num_actual_tokens": num_actual_tokens,
            "max_query_len": seq_len,
            "query_start_loc": query_start_loc,
            "max_seq_len": seq_len,
            "seq_lens": seq_lens_tensor,
            "block_table": block_table,
            "slot_mapping": slot_mapping,
            "num_prefill_tokens": num_actual_tokens,
            "num_decode_tokens": 0,
            "num_prefills": batch_size,
            "num_decodes": 0
        }
    
    return attn_metadata


def initialize_vllm_distributed():
    """ä½¿ç”¨vLLMçš„init_distributed_environmentåˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if not vllm_available:
        return False
    
    try:
        # å•GPUæ¨¡å¼ä¸‹çš„åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ.setdefault('MASTER_ADDR', 'localhost')
        os.environ.setdefault('MASTER_PORT', '12355')
        os.environ.setdefault('RANK', '0')
        os.environ.setdefault('WORLD_SIZE', '1')
        os.environ.setdefault('LOCAL_RANK', '0')
        
        print("æ­£åœ¨åˆå§‹åŒ–vLLMåˆ†å¸ƒå¼ç¯å¢ƒ...")
        print(f"  MASTER_ADDR: {os.environ['MASTER_ADDR']}")
        print(f"  MASTER_PORT: {os.environ['MASTER_PORT']}")
        print(f"  RANK: {os.environ['RANK']}")
        print(f"  WORLD_SIZE: {os.environ['WORLD_SIZE']}")
        print(f"  LOCAL_RANK: {os.environ.get('LOCAL_RANK', 'æœªè®¾ç½®')}")
        
        init_distributed_environment()
        print("âœ“ vLLMåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— vLLMåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def initialize_vllm_model_parallel():
    """åˆå§‹åŒ–vLLMæ¨¡å‹å¹¶è¡Œ"""
    if not vllm_available:
        return False
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹å¹¶è¡Œ
        initialize_model_parallel(tensor_model_parallel_size=1)
        print(f"âœ“ vLLMæ¨¡å‹å¹¶è¡Œåˆå§‹åŒ–æˆåŠŸ")
        print(f"  å¼ é‡æ¨¡å‹å¹¶è¡Œå¤§å°: {get_tensor_model_parallel_world_size()}")
        print(f"  å¼ é‡æ¨¡å‹å¹¶è¡Œæ’å: {get_tensor_model_parallel_rank()}")
        return True
    except Exception as e:
        print(f"âœ— vLLMæ¨¡å‹å¹¶è¡Œåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_weight_loading(sse_type='gla'):
    """æµ‹è¯•1ï¼šéªŒè¯vLLMæ¨¡å‹èƒ½å¦åŠ è½½HuggingFaceä¿å­˜çš„æƒé‡"""
    print(f"\n=== æµ‹è¯•1ï¼šæƒé‡åŠ è½½æµ‹è¯• ({sse_type.upper()}) ===")
    
    # å¯¼å…¥SSEæ¨¡å‹
    from sse_swa_moba_hf.sse_hf import SSEGLA as HFSSEGLA, SSEGDN as HFSSEGDN
    from sse_swa_moba_vllm.sse import VLLMSSEGLA, VLLMSSEGDN
    
    # æ ¹æ®ç±»å‹é€‰æ‹©æ¨¡å‹ç±»
    if sse_type.lower() == 'gla':
        HFModelClass = HFSSEGLA
        VLLMModelClass = VLLMSSEGLA
        model_name = "SSEGLA"
    elif sse_type.lower() == 'gdn':
        HFModelClass = HFSSEGDN
        VLLMModelClass = VLLMSSEGDN
        model_name = "SSEGDN"
    else:
        raise ValueError(f"æœªçŸ¥çš„SSEç±»å‹: {sse_type}")
    
    # 1. åˆ›å»ºHuggingFaceæ¨¡å‹ - ä½¿ç”¨bfloat16
    hf_sse = HFModelClass(
        hidden_size=512,
        expand_v=1.0,
        head_dim=64,
        num_heads=8,
        num_v_heads=8,
        mode='chunk',
        use_output_gate=True,
        use_short_conv=False,
        num_sparse_partition=4,
        num_writer=1,
        num_reader=1,
        sse_implementation="varlen",
        use_q_softmax=False,
        use_k_softmax=True,
        emulq=True,
        emulk=True,
        gate_logit_normalizer=16,
        gate_low_rank_dim=16,
        layer_idx=0,
        norm_eps=1e-5
    ).to(device, dtype=torch.bfloat16)
    
    # æ‰‹åŠ¨åˆå§‹åŒ–æƒé‡
    hf_sse.apply(manual_weight_initialization)
    print(f"âœ“ HuggingFace {model_name}æ¨¡å‹åˆ›å»ºå¹¶åˆå§‹åŒ–å®Œæˆ")
    
    # 2. ä¿å­˜HuggingFaceæƒé‡
    hf_weights = hf_sse.state_dict()
    torch.save(hf_weights, f"hf_{sse_type}_weights.pth")
    print("âœ“ HuggingFaceæƒé‡ä¿å­˜å®Œæˆ")
    
    # 3. åˆ›å»ºvLLMæ¨¡å‹ - ä½¿ç”¨bfloat16
    if vllm_available:
        vllm_config = get_current_vllm_config()
        
        # ä¿®å¤ï¼šæ ¹æ®sse_typeå†³å®šæ˜¯å¦ä¼ é€’gateå‚æ•°
        if sse_type.lower() == 'gla':
            # GLAæ¨¡å‹éœ€è¦gateå‚æ•°
            vllm_sse = VLLMModelClass(
                vllm_config=vllm_config,
                prefix=f"model.layers.0.sse_{sse_type}",
                hidden_size=512,
                expand_v=1.0,
                head_dim=64,
                num_heads=8,
                num_v_heads=8,
                mode='chunk',
                use_output_gate=True,
                use_short_conv=False,
                num_sparse_partition=4,
                num_writer=1,
                num_reader=1,
                sse_implementation="varlen",
                use_q_softmax=False,
                use_k_softmax=True,
                emulq=True,
                emulk=True,
                gate_logit_normalizer=16,
                gate_low_rank_dim=16,
                layer_idx=0,
                norm_eps=1e-5
            ).to(device, dtype=torch.bfloat16)
        else:
            # GDNæ¨¡å‹ä¸éœ€è¦gateå‚æ•°ï¼ˆVLLMSSEGDNçš„__init__å·²ç»å¤„ç†ï¼‰
            vllm_sse = VLLMModelClass(
                vllm_config=vllm_config,
                prefix=f"model.layers.0.sse_{sse_type}",
                hidden_size=512,
                expand_v=1.0,
                head_dim=64,
                num_heads=8,
                num_v_heads=8,
                mode='chunk',
                use_output_gate=True,
                use_short_conv=False,
                num_sparse_partition=4,
                num_writer=1,
                num_reader=1,
                sse_implementation="varlen",
                use_q_softmax=False,
                use_k_softmax=True,
                emulq=True,
                emulk=True,
                layer_idx=0,
                norm_eps=1e-5
            ).to(device, dtype=torch.bfloat16)
            
        print(f"âœ“ vLLM {model_name}æ¨¡å‹åˆ›å»ºå®Œæˆ")
    else:
        print("âš ï¸ vLLMä¸å¯ç”¨ï¼Œè·³è¿‡vLLMæ¨¡å‹åˆ›å»º")
        return hf_sse, None
    
    # 4. ä»HuggingFaceæƒé‡åŠ è½½åˆ°vLLMæ¨¡å‹
    try:
        # ä½¿ç”¨vLLMæ¨¡å‹çš„load_hf_weightsæ–¹æ³•åŠ è½½æƒé‡
        # è¿™æ˜¯æ›´ç®€æ´å’Œå¯é çš„æ–¹å¼ï¼Œä¸test_attn.pyä¿æŒä¸€è‡´
        vllm_sse.load_hf_weights(hf_weights)
        print(f"âœ“ vLLM {model_name}æ¨¡å‹æˆåŠŸåŠ è½½HuggingFaceæƒé‡")
    except Exception as e:
        print(f"âœ— æƒé‡åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    # 5. è¯¦ç»†éªŒè¯æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½
    print("\n=== æƒé‡éªŒè¯è¯¦æƒ… ===")
    
    # éªŒè¯å…³é”®æŠ•å½±å±‚æƒé‡
    if vllm_available:
        # éªŒè¯q_projæƒé‡
        if hasattr(hf_sse, 'q_proj') and hasattr(vllm_sse, 'q_proj'):
            if hasattr(hf_sse.q_proj, 'weight') and hasattr(vllm_sse.q_proj, 'weight'):
                hf_q_weight = hf_sse.q_proj.weight.data
                vllm_q_weight = vllm_sse.q_proj.weight.data
                q_diff = torch.max(torch.abs(vllm_q_weight - hf_q_weight))
                print(f"q_projæƒé‡æœ€å¤§å·®å¼‚: {q_diff.item():.6f}")
                assert q_diff < 1e-6, "q_projæƒé‡ä¸åŒ¹é…"
        
        # éªŒè¯k_projæƒé‡
        if hasattr(hf_sse, 'k_proj') and hasattr(vllm_sse, 'k_proj'):
            if hasattr(hf_sse.k_proj, 'weight') and hasattr(vllm_sse.k_proj, 'weight'):
                hf_k_weight = hf_sse.k_proj.weight.data
                vllm_k_weight = vllm_sse.k_proj.weight.data
                k_diff = torch.max(torch.abs(vllm_k_weight - hf_k_weight))
                print(f"k_projæƒé‡æœ€å¤§å·®å¼‚: {k_diff.item():.6f}")
                assert k_diff < 1e-6, "k_projæƒé‡ä¸åŒ¹é…"
        
        # éªŒè¯v_projæƒé‡
        if hasattr(hf_sse, 'v_proj') and hasattr(vllm_sse, 'v_proj'):
            if hasattr(hf_sse.v_proj, 'weight') and hasattr(vllm_sse.v_proj, 'weight'):
                hf_v_weight = hf_sse.v_proj.weight.data
                vllm_v_weight = vllm_sse.v_proj.weight.data
                v_diff = torch.max(torch.abs(vllm_v_weight - hf_v_weight))
                print(f"v_projæƒé‡æœ€å¤§å·®å¼‚: {v_diff.item():.6f}")
                assert v_diff < 1e-6, "v_projæƒé‡ä¸åŒ¹é…"
        
        # éªŒè¯o_projæƒé‡
        if hasattr(hf_sse, 'o_proj') and hasattr(vllm_sse, 'o_proj'):
            if hasattr(hf_sse.o_proj, 'weight') and hasattr(vllm_sse.o_proj, 'weight'):
                hf_o_weight = hf_sse.o_proj.weight.data
                vllm_o_weight = vllm_sse.o_proj.weight.data
                o_diff = torch.max(torch.abs(vllm_o_weight - hf_o_weight))
                print(f"o_projæƒé‡æœ€å¤§å·®å¼‚: {o_diff.item():.6f}")
                assert o_diff < 1e-6, "o_projæƒé‡ä¸åŒ¹é…"
        
        # éªŒè¯g_projæƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(hf_sse, 'g_proj') and hasattr(vllm_sse, 'g_proj_0'):
            # vLLMçš„g_projæ˜¯åˆ†é˜¶æ®µçš„
            if hasattr(hf_sse.g_proj, 'weight') and hasattr(vllm_sse.g_proj_0, 'weight'):
                # å¯¹äºåˆ†é˜¶æ®µçš„æŠ•å½±ï¼Œæˆ‘ä»¬åªéªŒè¯ç¬¬ä¸€é˜¶æ®µ
                hf_g_weight = hf_sse.g_proj.weight.data
                vllm_g_weight = vllm_sse.g_proj_0.weight.data
                g_diff = torch.max(torch.abs(vllm_g_weight - hf_g_weight[:vllm_g_weight.shape[0]]))
                print(f"g_projæƒé‡æœ€å¤§å·®å¼‚: {g_diff.item():.6f}")
                # è¿™é‡Œä¸åšä¸¥æ ¼æ–­è¨€ï¼Œå› ä¸ºvLLMçš„å®ç°å¯èƒ½ä¸åŒ
        
        print("âœ“ æ‰€æœ‰å…³é”®æƒé‡éªŒè¯é€šè¿‡")
    
    return hf_sse, vllm_sse


def test_forward_equivalence(hf_sse, vllm_sse, sse_type='gla'):
    """æµ‹è¯•2ï¼šéªŒè¯å‰å‘ä¼ æ’­è¾“å‡ºç­‰ä»·æ€§"""
    print(f"\n=== æµ‹è¯•2ï¼šå‰å‘ä¼ æ’­ç­‰ä»·æ€§æµ‹è¯• ({sse_type.upper()}) ===")
    
    if not vllm_sse:
        print("âš ï¸ vLLMæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³å‰å‘ä¼ æ’­æµ‹è¯•")
        return
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)
    np.random.seed(42)
    
    # æµ‹è¯•ä¸åŒçš„è¾“å…¥é…ç½®
    test_cases = [
        # (batch_size, seq_len, use_cu_seqlens, description)
        (1, 320, False, "å•æ ·æœ¬ï¼Œå›ºå®šé•¿åº¦"),
        (2, 320, False, "å¤šæ ·æœ¬ï¼Œå›ºå®šé•¿åº¦"),
        (2, 320, True, "å¤šæ ·æœ¬ï¼Œå˜é•¿åºåˆ—(cu_seqlens)"),  # æ–°å¢çš„å˜é•¿æµ‹è¯•ç”¨ä¾‹
    ]
    
    for batch_size, seq_len, use_cu_seqlens, description in test_cases:
        print(f"\næµ‹è¯•ç”¨ä¾‹: {description}")
        
        # åˆ›å»ºè¾“å…¥æ•°æ® - ä½¿ç”¨bfloat16
        hidden_states = torch.randn(batch_size, seq_len, hf_sse.hidden_size, dtype=torch.bfloat16).to(device)
        
        # åˆ›å»ºæ³¨æ„åŠ›æ©ç ï¼ˆç”¨äºå˜é•¿åºåˆ—ï¼‰
        attention_mask = None
        cu_seqlens = None
        actual_seq_lens = None
        
        if use_cu_seqlens:
            # åˆ›å»ºå˜é•¿åºåˆ—é•¿åº¦ï¼ˆæ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„åºåˆ—ï¼‰
            actual_seq_lens = [seq_len - i for i in range(batch_size)]
            max_len = max(actual_seq_lens)
            
            # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
            attention_mask = torch.zeros(batch_size, max_len).to(device)
            for i, length in enumerate(actual_seq_lens):
                attention_mask[i, :length] = 1
            
            # åˆ›å»ºcu_seqlens
            cu_seqlens = torch.zeros(batch_size + 1, dtype=torch.int32).to(device)
            for i in range(batch_size):
                cu_seqlens[i+1] = cu_seqlens[i] + actual_seq_lens[i]
            
            # è°ƒæ•´è¾“å…¥åºåˆ—é•¿åº¦
            hidden_states = hidden_states[:, :max_len, :]
        else:
            # å›ºå®šé•¿åº¦åºåˆ—
            attention_mask = torch.ones(batch_size, seq_len).to(device)
        
        # HuggingFaceå‰å‘ä¼ æ’­
        hf_output, hf_aux_loss, _ = hf_sse(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            cu_seqlens=cu_seqlens if use_cu_seqlens else None,
            output_attentions=False,
            use_cache=False
        )
        
        # vLLMå‰å‘ä¼ æ’­
        # vLLMéœ€è¦positionså‚æ•°
        positions = torch.arange(0, hidden_states.shape[1], device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)
        
        # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
        if vllm_available:
            vllm_config = get_current_vllm_config()
            attn_metadata = create_attention_metadata(
                batch_size=batch_size,
                seq_len=hidden_states.shape[1],
                use_cu_seqlens=use_cu_seqlens,
                seq_lens=actual_seq_lens
            )
            
            with set_forward_context(
                attn_metadata=attn_metadata,
                vllm_config=vllm_config,
                num_tokens=attn_metadata["num_actual_tokens"],
                cudagraph_runtime_mode=CUDAGraphMode.NONE
            ):
                vllm_output, vllm_aux_loss, _ = vllm_sse(
                    hidden_states=hidden_states,
                    positions=positions,
                    attention_mask=attention_mask,
                    output_attentions=False,
                    use_cache=False
                )
        else:
            # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
            vllm_output, vllm_aux_loss, _ = vllm_sse(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                output_attentions=False,
                use_cache=False
            )
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        assert hf_output.shape == vllm_output.shape, f"å‰å‘è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {hf_output.shape} vs {vllm_output.shape}"
        
        # æ£€æŸ¥è¾“å‡ºå€¼
        max_diff = torch.max(torch.abs(hf_output - vllm_output))
        print(f"å‰å‘è¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")
        
        if max_diff < 1e-5:
            print(f"âœ“ {description} å‰å‘ä¼ æ’­éªŒè¯é€šè¿‡")
        else:
            print(f"âœ— {description} å‰å‘ä¼ æ’­è¾“å‡ºä¸ç­‰ä»·")
            raise AssertionError(f"å‰å‘è¾“å‡ºä¸ç­‰ä»·")

# æµ‹è¯•æ¨ç†prefillé˜¶æ®µ
def test_inference_prefill(hf_sse: nn.Module, vllm_sse: nn.Module, sse_type='gla'):
    """æµ‹è¯•3ï¼šéªŒè¯æ¨ç†prefillé˜¶æ®µ"""
    print(f"\n=== æµ‹è¯•3ï¼šæ¨ç†Prefillé˜¶æ®µæµ‹è¯• ({sse_type.upper()}) ===")
    hf_sse.eval()
    if vllm_sse is not None:
        vllm_sse.eval()
    
    if not vllm_sse:
        print("âš ï¸ vLLMæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡prefillæµ‹è¯•")
        return None, None
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ - ä½¿ç”¨bfloat16
    batch_size = 2
    seq_len = 320
    hidden_states = torch.randn(batch_size, seq_len, hf_sse.hidden_size, dtype=torch.bfloat16).to(device)
    attention_mask = torch.ones(batch_size, seq_len).to(device)
    
    # ====================
    # æ­£ç¡®åˆå§‹åŒ–past_key_values
    # ====================
    
    # 1. HuggingFace prefill - æ­£ç¡®åˆå§‹åŒ–past_key_values
    # æ ¹æ®fla.models.utils.Cacheçš„å®šä¹‰ï¼Œåº”è¯¥ä½¿ç”¨Cacheç±»åˆå§‹åŒ–ï¼Œè€Œä¸æ˜¯None
    # Cacheç±»éœ€è¦seen_tokenså‚æ•°ï¼ˆåˆå§‹ä¸º0ï¼‰
    hf_past_key_values = Cache(seen_tokens=0)  # æ­£ç¡®åˆå§‹åŒ–Cacheå¯¹è±¡
    print(f"âœ“ HuggingFace past_key_valuesåˆå§‹åŒ–ä¸ºCacheå¯¹è±¡ï¼Œç±»å‹: {type(hf_past_key_values)}")
    
    hf_output, hf_aux_loss, hf_past = hf_sse(
        hidden_states=hidden_states,
        attention_mask=attention_mask,
        past_key_values=hf_past_key_values,  # ä¼ é€’æ­£ç¡®åˆå§‹åŒ–çš„Cacheå¯¹è±¡
        output_attentions=False,
        use_cache=True
    )
    
    # 2. vLLM prefill - åˆå§‹åŒ–past_key_values
    # vLLMä½¿ç”¨å­—å…¸æ ¼å¼
    positions = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)
    vllm_past_key_values = {}  # vLLMä½¿ç”¨å­—å…¸æ ¼å¼
    print(f"âœ“ vLLM past_key_valuesåˆå§‹åŒ–ä¸ºå­—å…¸ï¼Œç±»å‹: {type(vllm_past_key_values)}")
    
    # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
    if vllm_available:
        vllm_config = get_current_vllm_config()
        attn_metadata = create_attention_metadata(
            batch_size=batch_size,
            seq_len=seq_len,
            use_cu_seqlens=False
        )
        
        with set_forward_context(
            attn_metadata=attn_metadata,
            vllm_config=vllm_config,
            num_tokens=attn_metadata["num_actual_tokens"],
            cudagraph_runtime_mode=CUDAGraphMode.NONE
        ):
            vllm_output, vllm_aux_loss, vllm_past = vllm_sse(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                past_key_values=vllm_past_key_values,  # ä¼ é€’åˆå§‹çš„å­—å…¸
                output_attentions=False,
                use_cache=True
            )
    else:
        # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
        vllm_output, vllm_aux_loss, vllm_past = vllm_sse(
            hidden_states=hidden_states,
            positions=positions,
            attention_mask=attention_mask,
            past_key_values=vllm_past_key_values,  # ä¼ é€’åˆå§‹çš„å­—å…¸
            output_attentions=False,
            use_cache=True
        )
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert hf_output.shape == vllm_output.shape, f"Prefillè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {hf_output.shape} vs {vllm_output.shape}"
    print(f"âœ“ Prefillè¾“å‡ºå½¢çŠ¶åŒ¹é…: {hf_output.shape}")
    
    # æ£€æŸ¥è¾“å‡ºå€¼
    max_diff = torch.max(torch.abs(hf_output - vllm_output))
    print(f"Prefillè¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")
    
    if max_diff < 1e-5:
        print(f"âœ“ {sse_type.upper()} Prefillé˜¶æ®µéªŒè¯é€šè¿‡")
    else:
        print(f"âœ— {sse_type.upper()} Prefillé˜¶æ®µè¾“å‡ºä¸ç­‰ä»·")
        raise AssertionError(f"Prefillè¾“å‡ºä¸ç­‰ä»·ï¼Œæœ€å¤§å·®å¼‚: {max_diff.item()}")
    
    # éªŒè¯pastçŠ¶æ€ä¸ä¸ºNone
    assert hf_past is not None, "HuggingFace pastçŠ¶æ€ä¸ºNone"
    assert vllm_past is not None, "vLLM pastçŠ¶æ€ä¸ºNone"
    print(f"âœ“ Prefillé˜¶æ®µpastçŠ¶æ€éªŒè¯é€šè¿‡")
    print(f"  HuggingFace pastç±»å‹: {type(hf_past)}, é•¿åº¦: {len(hf_past) if hasattr(hf_past, '__len__') else 'N/A'}")
    print(f"  vLLM pastç±»å‹: {type(vllm_past)}, é”®æ•°é‡: {len(vllm_past) if isinstance(vllm_past, dict) else 'N/A'}")
    
    return hf_past, vllm_past


def test_inference_decode(hf_sse, vllm_sse, hf_past, vllm_past, sse_type='gla'):
    """æµ‹è¯•4ï¼šéªŒè¯æ¨ç†decodeé˜¶æ®µ"""
    print(f"\n=== æµ‹è¯•4ï¼šæ¨ç†Decodeé˜¶æ®µæµ‹è¯• ({sse_type.upper()}) ===")
    
    # æ”¹è¿›çš„æ£€æŸ¥é€»è¾‘ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    if not vllm_sse:
        print("âš ï¸ vLLMæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡decodeæµ‹è¯•")
        return
    if hf_past is None:
        print("âš ï¸ HuggingFace pastçŠ¶æ€ä¸ºNoneï¼Œè·³è¿‡decodeæµ‹è¯•")
        return
    if vllm_past is None:
        print("âš ï¸ vLLM pastçŠ¶æ€ä¸ºNoneï¼Œè·³è¿‡decodeæµ‹è¯•")
        return
    
    # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    hf_sse.eval()
    vllm_sse.eval()
    
    batch_size = 2
    decode_steps = 5
    
    for step in range(decode_steps):
        print(f"\nDecode step {step+1}:")
        
        # åˆ›å»ºdecodeè¾“å…¥ï¼ˆå•tokenï¼‰- ä½¿ç”¨bfloat16
        hidden_states = torch.randn(batch_size, 1, hf_sse.hidden_size, dtype=torch.bfloat16).to(device)
        
        # HuggingFace decode
        hf_output, hf_aux_loss, hf_past = hf_sse(
            hidden_states=hidden_states,
            past_key_values=hf_past,
            output_attentions=False,
            use_cache=True
        )
        
        # vLLM decode
        # vLLMéœ€è¦positionså‚æ•°ï¼Œè¿™é‡Œä½¿ç”¨ä¹‹å‰çš„ä½ç½®+1
        positions = torch.full((batch_size, 1), 320 + step, device=hidden_states.device)
        
        # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
        if vllm_available:
            vllm_config = get_current_vllm_config()
            # åˆ›å»ºdecodeé˜¶æ®µçš„attention metadata
            attn_metadata = create_attention_metadata(
                batch_size=batch_size,
                seq_len=1,  # decodeé˜¶æ®µæ¯æ¬¡å¤„ç†1ä¸ªtoken
                use_cu_seqlens=False
            )
            
            # æ›´æ–°decodeç›¸å…³çš„å…ƒæ•°æ®
            attn_metadata.update({
                "num_actual_tokens": batch_size * 1,  # decodeé˜¶æ®µæ¯æ¬¡å¤„ç†1ä¸ªtoken
                "num_prefill_tokens": 0,
                "num_decode_tokens": batch_size * 1,
                "num_prefills": 0,
                "num_decodes": batch_size
            })
            
            with set_forward_context(
                attn_metadata=attn_metadata,
                vllm_config=vllm_config,
                num_tokens=attn_metadata["num_actual_tokens"],
                cudagraph_runtime_mode=CUDAGraphMode.NONE
            ):
                vllm_output, vllm_aux_loss, vllm_past = vllm_sse(
                    hidden_states=hidden_states,
                    positions=positions,
                    past_key_values=vllm_past,
                    output_attentions=False,
                    use_cache=True
                )
        else:
            # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
            vllm_output, vllm_aux_loss, vllm_past = vllm_sse(
                hidden_states=hidden_states,
                positions=positions,
                past_key_values=vllm_past,
                output_attentions=False,
                use_cache=True
            )
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        assert hf_output.shape == vllm_output.shape, f"Decodeè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
        
        # æ£€æŸ¥è¾“å‡ºå€¼
        max_diff = torch.max(torch.abs(hf_output - vllm_output))
        print(f"  è¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")
        
        if max_diff < 1e-5:
            print(f"  âœ“ {sse_type.upper()} Decode step {step+1} éªŒè¯é€šè¿‡")
        else:
            print(f"  âœ— {sse_type.upper()} Decode step {step+1} è¾“å‡ºä¸ç­‰ä»·")
            raise AssertionError(f"Decodeè¾“å‡ºä¸ç­‰ä»·")


def test_training_mode(hf_sse, vllm_sse, sse_type='gla'):
    """æµ‹è¯•5ï¼šéªŒè¯è®­ç»ƒæ¨¡å¼"""
    print(f"\n=== æµ‹è¯•5ï¼šè®­ç»ƒæ¨¡å¼æµ‹è¯• ({sse_type.upper()}) ===")
    
    if not vllm_sse:
        print("âš ï¸ vLLMæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒæ¨¡å¼æµ‹è¯•")
        return
    
    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    hf_sse.train()
    vllm_sse.train()
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)
    
    # åˆ›å»ºè¾“å…¥æ•°æ® - ä½¿ç”¨bfloat16
    batch_size = 2
    seq_len = 320
    hidden_states = torch.randn(batch_size, seq_len, hf_sse.hidden_size, dtype=torch.bfloat16).to(device)
    attention_mask = torch.ones(batch_size, seq_len).to(device)
    
    # æ·»åŠ æ¢¯åº¦è®¡ç®—
    hidden_states.requires_grad_(True)
    
    # HuggingFaceè®­ç»ƒå‰å‘
    hf_output, hf_aux_loss, _ = hf_sse(
        hidden_states=hidden_states,
        attention_mask=attention_mask,
        output_attentions=False,
        use_cache=False
    )
    
    # vLLMè®­ç»ƒå‰å‘
    positions = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)
    
    # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
    if vllm_available:
        vllm_config = get_current_vllm_config()
        attn_metadata = create_attention_metadata(
            batch_size=batch_size,
            seq_len=seq_len,
            use_cu_seqlens=False
        )
        
        with set_forward_context(
            attn_metadata=attn_metadata,
            vllm_config=vllm_config,
            num_tokens=attn_metadata["num_actual_tokens"],
            cudagraph_runtime_mode=CUDAGraphMode.NONE
        ):
            vllm_output, vllm_aux_loss, _ = vllm_sse(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                output_attentions=False,
                use_cache=False
            )
    else:
        # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
        vllm_output, vllm_aux_loss, _ = vllm_sse(
            hidden_states=hidden_states,
            positions=positions,
            attention_mask=attention_mask,
            output_attentions=False,
            use_cache=False
        )
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert hf_output.shape == vllm_output.shape, f"è®­ç»ƒæ¨¡å¼è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
    
    # æ£€æŸ¥è¾“å‡ºå€¼
    max_diff = torch.max(torch.abs(hf_output - vllm_output))
    print(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")
    
    if max_diff < 1e-5:
        print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼å‰å‘éªŒè¯é€šè¿‡")
    else:
        print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼è¾“å‡ºä¸ç­‰ä»·")
        raise AssertionError(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºä¸ç­‰ä»·")
    
    # æµ‹è¯•åå‘ä¼ æ’­
    print("\næµ‹è¯•åå‘ä¼ æ’­...")
    
    # HuggingFaceåå‘ä¼ æ’­
    hf_loss = hf_output.sum()
    if hf_aux_loss and len(hf_aux_loss) > 1 and hf_aux_loss[1] is not None:
        hf_loss += hf_aux_loss[1]
    hf_loss.backward(retain_graph=True)
    
    # vLLMåå‘ä¼ æ’­
    vllm_loss = vllm_output.sum()
    if vllm_aux_loss is not None:
        vllm_loss += vllm_aux_loss
    vllm_loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦ï¼ˆä»…æ£€æŸ¥éƒ¨åˆ†å…³é”®å‚æ•°ï¼‰
    print("\næ¢¯åº¦æ£€æŸ¥:")
    
    # æ¯”è¾ƒq_projæƒé‡çš„æ¢¯åº¦
    if hasattr(hf_sse, 'q_proj') and hasattr(hf_sse.q_proj, 'weight') and \
       hasattr(vllm_sse, 'q_proj') and hasattr(vllm_sse.q_proj, 'weight'):
        
        hf_q_grad = hf_sse.q_proj.weight.grad
        vllm_q_grad = vllm_sse.q_proj.weight.grad
        
        if hf_q_grad is not None and vllm_q_grad is not None:
            grad_max_diff = torch.max(torch.abs(hf_q_grad - vllm_q_grad))
            print(f"q_projæ¢¯åº¦æœ€å¤§å·®å¼‚: {grad_max_diff.item():.6f}")
            
            if grad_max_diff < 1e-5:
                print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­éªŒè¯é€šè¿‡")
            else:
                print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­ä¸ç­‰ä»·")
                raise AssertionError(f"è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­ä¸ç­‰ä»·")
        else:
            print("âš ï¸ æ¢¯åº¦ä¸ºNoneï¼Œæ— æ³•æ¯”è¾ƒ")
            
            # å¦‚æœvLLMæ¨¡å‹å‚æ•°æ²¡æœ‰æ¢¯åº¦ï¼Œæ‰‹åŠ¨è®¾ç½®å…è®¸æ¢¯åº¦å¹¶é‡æ–°æµ‹è¯•
            if vllm_q_grad is None and hasattr(vllm_sse.q_proj, 'weight'):
                print("\nå°è¯•æ‰‹åŠ¨å¯ç”¨æ¢¯åº¦å¹¶é‡æ–°æµ‹è¯•...")
                
                # æ‰‹åŠ¨è®¾ç½®å…è®¸æ¢¯åº¦
                vllm_sse.q_proj.weight.requires_grad_(True)
                print("  âœ“ å·²è®¾ç½®q_proj.weight.requires_grad=True")
                
                # æ£€æŸ¥å…¶ä»–é‡è¦å‚æ•°å¹¶è®¾ç½®æ¢¯åº¦
                if hasattr(vllm_sse, 'k_proj') and hasattr(vllm_sse.k_proj, 'weight'):
                    vllm_sse.k_proj.weight.requires_grad_(True)
                    print("  âœ“ å·²è®¾ç½®k_proj.weight.requires_grad=True")
                
                if hasattr(vllm_sse, 'v_proj') and hasattr(vllm_sse.v_proj, 'weight'):
                    vllm_sse.v_proj.weight.requires_grad_(True)
                    print("  âœ“ å·²è®¾ç½®v_proj.weight.requires_grad=True")
                
                # é‡æ–°è¿è¡Œå‰å‘å’Œåå‘ä¼ æ’­
                vllm_sse.zero_grad()
                
                # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
                if vllm_available:
                    vllm_config = get_current_vllm_config()
                    attn_metadata = create_attention_metadata(
                        batch_size=batch_size,
                        seq_len=seq_len,
                        use_cu_seqlens=False
                    )
                    
                    with set_forward_context(
                        attn_metadata=attn_metadata,
                        vllm_config=vllm_config,
                        num_tokens=attn_metadata["num_actual_tokens"],
                        cudagraph_runtime_mode=CUDAGraphMode.NONE
                    ):
                        vllm_output_retry, vllm_aux_loss_retry, _ = vllm_sse(
                            hidden_states=hidden_states,
                            positions=positions,
                            attention_mask=attention_mask,
                            output_attentions=False,
                            use_cache=False
                        )
                else:
                    # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
                    vllm_output_retry, vllm_aux_loss_retry, _ = vllm_sse(
                        hidden_states=hidden_states,
                        positions=positions,
                        attention_mask=attention_mask,
                        output_attentions=False,
                        use_cache=False
                    )
                
                # é‡æ–°è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
                vllm_loss_retry = vllm_output_retry.sum()
                if vllm_aux_loss_retry is not None:
                    vllm_loss_retry += vllm_aux_loss_retry
                vllm_loss_retry.backward()
                
                # å†æ¬¡æ£€æŸ¥æ¢¯åº¦
                vllm_q_grad_retry = vllm_sse.q_proj.weight.grad
                if vllm_q_grad_retry is not None:
                    grad_max_diff_retry = torch.max(torch.abs(hf_q_grad - vllm_q_grad_retry))
                    print(f"é‡æ–°æµ‹è¯•åq_projæ¢¯åº¦æœ€å¤§å·®å¼‚: {grad_max_diff_retry.item():.6f}")
                    
                    if grad_max_diff_retry < 1e-5:
                        print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­éªŒè¯é€šè¿‡ï¼ˆæ‰‹åŠ¨å¯ç”¨æ¢¯åº¦åï¼‰")
                    else:
                        print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­ä¸ç­‰ä»·ï¼ˆæ‰‹åŠ¨å¯ç”¨æ¢¯åº¦åï¼‰")
                        raise AssertionError(f"è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­ä¸ç­‰ä»·")
                else:
                    print("  âš ï¸ ä»ç„¶æ²¡æœ‰æ¢¯åº¦ï¼Œå¹¶è¡Œçº¿æ€§å±‚å¯èƒ½ä¸æ”¯æŒæ ‡å‡†æ¢¯åº¦è®¡ç®—")
                    print("  ä»…éªŒè¯è¾“å‡ºç­‰ä»·æ€§")
                    if max_diff < 1e-5:
                        print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼éªŒè¯é€šè¿‡ï¼ˆä»…éªŒè¯è¾“å‡ºç­‰ä»·æ€§ï¼‰")
                    else:
                        print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼éªŒè¯å¤±è´¥")
                        raise AssertionError(f"è®­ç»ƒæ¨¡å¼ä¸ç­‰ä»·")
    else:
        print("âš ï¸ q_projä¸å­˜åœ¨ï¼Œè·³è¿‡æ¢¯åº¦æ£€æŸ¥")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æŠ•å½±å±‚
        proj_layers = ['qkv_proj', 'q_proj', 'k_proj', 'v_proj']
        found = False
        
        for layer_name in proj_layers:
            if hasattr(hf_sse, layer_name) and hasattr(getattr(hf_sse, layer_name), 'weight') and \
               hasattr(vllm_sse, layer_name) and hasattr(getattr(vllm_sse, layer_name), 'weight'):
                
                found = True
                hf_grad = getattr(hf_sse, layer_name).weight.grad
                vllm_grad = getattr(vllm_sse, layer_name).weight.grad
                
                if hf_grad is not None and vllm_grad is not None:
                    grad_max_diff = torch.max(torch.abs(hf_grad - vllm_grad))
                    print(f"{layer_name}æ¢¯åº¦æœ€å¤§å·®å¼‚: {grad_max_diff.item():.6f}")
                    
                    if grad_max_diff < 1e-5:
                        print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­éªŒè¯é€šè¿‡")
                    else:
                        print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­ä¸ç­‰ä»·")
                        raise AssertionError(f"è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­ä¸ç­‰ä»·")
                else:
                    print(f"âš ï¸ {layer_name}æ¢¯åº¦ä¸ºNoneï¼Œæ— æ³•æ¯”è¾ƒ")
                    break
        
        if not found:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯æ¯”è¾ƒçš„æŠ•å½±å±‚ï¼Œä»…éªŒè¯è¾“å‡ºç­‰ä»·æ€§")
            if max_diff < 1e-5:
                print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼éªŒè¯é€šè¿‡ï¼ˆä»…éªŒè¯è¾“å‡ºç­‰ä»·æ€§ï¼‰")
            else:
                print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼éªŒè¯å¤±è´¥")
                raise AssertionError(f"è®­ç»ƒæ¨¡å¼ä¸ç­‰ä»·")


def test_sse_type(sse_type):
    """æµ‹è¯•ç‰¹å®šç±»å‹çš„SSE"""
    print(f"\n" + "="*70)
    print(f"å¼€å§‹æµ‹è¯• {sse_type.upper()} ç±»å‹çš„SSEå±‚")
    print("="*70)
    
    # æµ‹è¯•1ï¼šæƒé‡åŠ è½½
    hf_sse, vllm_sse = test_weight_loading(sse_type)
    
    if not torch.cuda.is_available():
        print(f"\nâš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œè·³è¿‡{sse_type.upper()}çš„å…¶ä»–æµ‹è¯•")
        return
    
    # æµ‹è¯•2ï¼šå‰å‘ä¼ æ’­ç­‰ä»·æ€§
    test_forward_equivalence(hf_sse, vllm_sse, sse_type)
    
    # æµ‹è¯•3ï¼šæ¨ç†prefillé˜¶æ®µ
    hf_past, vllm_past = test_inference_prefill(hf_sse, vllm_sse, sse_type)
    
    # æµ‹è¯•4ï¼šæ¨ç†decodeé˜¶æ®µ
    test_inference_decode(hf_sse, vllm_sse, hf_past, vllm_past, sse_type)
    
    # æµ‹è¯•5ï¼šè®­ç»ƒæ¨¡å¼
    test_training_mode(hf_sse, vllm_sse, sse_type)
    
    print(f"\n" + "="*70)
    print(f"ğŸ‰ {sse_type.upper()} ç±»å‹çš„SSEå±‚æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("="*70)


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*70)
    print("SSE (Sparse State Expansion) å±‚ç­‰ä»·æ€§æµ‹è¯•")
    print("æ”¯æŒ SSEGLA å’Œ SSEGDN ä¸¤ç§ç±»å‹")
    print("="*70)
    
    try:
        # åˆå§‹åŒ–vLLMåˆ†å¸ƒå¼ç¯å¢ƒ
        if vllm_available:
            if initialize_vllm_distributed():
                # åˆå§‹åŒ–model parallel
                initialize_vllm_model_parallel()
        
        # æµ‹è¯•SSEGLA
        test_sse_type('gla')
        
        # æµ‹è¯•SSEGDN
        test_sse_type('gdn')
        
        print("\n" + "="*70)
        print("ğŸ‰ æ‰€æœ‰SSEç±»å‹çš„æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        print("âœ“ SSEGLA å’Œ SSEGDN å®ç°å®Œå…¨ç­‰ä»·")
        print("="*70)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for sse_type in ['gla', 'gdn']:
            weight_file = f"hf_{sse_type}_weights.pth"
            if os.path.exists(weight_file):
                os.remove(weight_file)
        print("\nâœ“ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
        
        # æ¸…ç†vLLMåˆ†å¸ƒå¼ç¯å¢ƒ
        if vllm_available:
            try:
                destroy_model_parallel()
                print("âœ“ vLLMæ¨¡å‹å¹¶è¡Œç¯å¢ƒå·²æ¸…ç†")
            except:
                pass


if __name__ == "__main__":
    main()