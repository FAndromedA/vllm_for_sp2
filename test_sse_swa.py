#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SSE-SWA (Sparse State Expansion with Sliding Window Attention) å±‚ç­‰ä»·æ€§æµ‹è¯•è„šæœ¬
è¯¥è„šæœ¬ç”¨äºéªŒè¯HuggingFaceé£æ ¼çš„SSE-SWAå®ç°ä¸vLLMé£æ ¼çš„SSE-SWAå®ç°
åœ¨è®­ç»ƒæ¨¡å¼ã€æ¨ç†prefillé˜¶æ®µå’Œæ¨ç†decodeé˜¶æ®µçš„è¾“å‡ºç­‰ä»·æ€§ã€‚
"""
import os
import math
import torch
import torch.nn as nn
import numpy as np
from functools import partial
from typing import Optional, Tuple, Dict, List
from fla.models.utils import Cache

# è¿‡æ»¤æ‰å¯èƒ½çš„å¼ƒç”¨è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)
# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
# å¯¼å…¥vLLMå¹¶è¡Œåˆå§‹åŒ–æ¨¡å—ï¼ˆä¸å¯¼å…¥destroy_world_groupï¼‰
try:
    from vllm.distributed.parallel_state import (
        initialize_model_parallel,
        destroy_model_parallel,
        get_tensor_model_parallel_world_size,
        get_tensor_model_parallel_rank,
        get_world_group
    )
    from vllm.distributed import init_distributed_environment
    # æ·»åŠ vLLM forward_contextæ”¯æŒ
    from vllm.config import get_current_vllm_config, CUDAGraphMode
    from vllm.forward_context import set_forward_context
    vllm_available = True
except ImportError as e:
    print(f"âš ï¸ vLLMå¹¶è¡Œæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    vllm_available = False

def manual_weight_initialization(module):
    """æ‰‹åŠ¨æƒé‡éšæœºåˆå§‹åŒ–å‡½æ•°"""
    if isinstance(module, nn.Linear):
        nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            nn.init.ones_(module.bias)
    elif isinstance(module, nn.LayerNorm):
        nn.init.normal_(module.weight)
        nn.init.ones_(module.bias)
    elif hasattr(module, 'weight') and module.weight is not None:
        if len(module.weight.shape) > 1:
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
        else:
            nn.init.ones_(module.weight)

def create_attention_metadata(batch_size, seq_len, use_cu_seqlens=False, seq_lens=None):
    """
    åˆ›å»ºæ­£ç¡®çš„AttentionMetadata

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_len: åºåˆ—é•¿åº¦
        use_cu_seqlens: æ˜¯å¦ä½¿ç”¨å˜é•¿åºåˆ—
        seq_lens: å˜é•¿åºåˆ—çš„å®é™…é•¿åº¦åˆ—è¡¨

    Returns:
        AttentionMetadataå­—å…¸
    """
    if use_cu_seqlens and seq_lens is not None:
        # å˜é•¿åºåˆ—å¤„ç†
        max_len = max(seq_lens)
        num_actual_tokens = sum(seq_lens)

        # åˆ›å»ºquery_start_loc (cumulative sum of sequence lengths)
        query_start_loc = torch.zeros(batch_size + 1, dtype=torch.int32, device=device)
        for i in range(batch_size):
            query_start_loc[i+1] = query_start_loc[i] + seq_lens[i]

        seq_lens_tensor = torch.tensor(seq_lens, dtype=torch.int32, device=device)

        # åˆ›å»ºè™šæ‹Ÿçš„block_tableå’Œslot_mappingï¼ˆæµ‹è¯•ç”¨ï¼‰
        block_size = 16  # å‡è®¾çš„å—å¤§å°
        max_blocks_per_seq = (max_len + block_size - 1) // block_size
        block_table = torch.zeros((batch_size, max_blocks_per_seq), dtype=torch.int32, device=device)
        slot_mapping = torch.zeros(num_actual_tokens, dtype=torch.int32, device=device)

        attn_metadata = {
            "num_actual_tokens": num_actual_tokens,
            "max_query_len": max_len,
            "query_start_loc": query_start_loc,
            "max_seq_len": max_len,
            "seq_lens": seq_lens_tensor,
            "block_table": block_table,
            "slot_mapping": slot_mapping,
            "num_prefill_tokens": num_actual_tokens,
            "num_decode_tokens": 0,
            "num_prefills": batch_size,
            "num_decodes": 0
        }
    else:
        # å®šé•¿åºåˆ—å¤„ç†
        num_actual_tokens = batch_size * seq_len

        # åˆ›å»ºquery_start_loc (æ¯ä¸ªåºåˆ—ä»0å¼€å§‹ï¼Œé•¿åº¦ä¸ºseq_len)
        query_start_loc = torch.arange(0, num_actual_tokens + 1, seq_len,
                                      dtype=torch.int32, device=device)

        seq_lens_tensor = torch.full((batch_size,), seq_len, dtype=torch.int32, device=device)

        # åˆ›å»ºè™šæ‹Ÿçš„block_tableå’Œslot_mappingï¼ˆæµ‹è¯•ç”¨ï¼‰
        block_size = 16  # å‡è®¾çš„å—å¤§å°
        max_blocks_per_seq = (seq_len + block_size - 1) // block_size
        block_table = torch.zeros((batch_size, max_blocks_per_seq), dtype=torch.int32, device=device)
        slot_mapping = torch.zeros(num_actual_tokens, dtype=torch.int32, device=device)

        attn_metadata = {
            "num_actual_tokens": num_actual_tokens,
            "max_query_len": seq_len,
            "query_start_loc": query_start_loc,
            "max_seq_len": seq_len,
            "seq_lens": seq_lens_tensor,
            "block_table": block_table,
            "slot_mapping": slot_mapping,
            "num_prefill_tokens": num_actual_tokens,
            "num_decode_tokens": 0,
            "num_prefills": batch_size,
            "num_decodes": 0
        }

    return attn_metadata

def initialize_vllm_distributed():
    """ä½¿ç”¨vLLMçš„init_distributed_environmentåˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if not vllm_available:
        return False

    try:
        # å•GPUæ¨¡å¼ä¸‹çš„åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ.setdefault('MASTER_ADDR', 'localhost')
        os.environ.setdefault('MASTER_PORT', '12355')
        os.environ.setdefault('RANK', '0')
        os.environ.setdefault('WORLD_SIZE', '1')
        os.environ.setdefault('LOCAL_RANK', '0')

        print("æ­£åœ¨åˆå§‹åŒ–vLLMåˆ†å¸ƒå¼ç¯å¢ƒ...")
        print(f"  MASTER_ADDR: {os.environ['MASTER_ADDR']}")
        print(f"  MASTER_PORT: {os.environ['MASTER_PORT']}")
        print(f"  RANK: {os.environ['RANK']}")
        print(f"  WORLD_SIZE: {os.environ['WORLD_SIZE']}")
        print(f"  LOCAL_RANK: {os.environ.get('LOCAL_RANK', 'æœªè®¾ç½®')}")

        init_distributed_environment()
        print("âœ“ vLLMåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— vLLMåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
def initialize_vllm_model_parallel():
    """åˆå§‹åŒ–vLLMæ¨¡å‹å¹¶è¡Œ"""
    if not vllm_available:
        return False

    try:
        # åˆå§‹åŒ–æ¨¡å‹å¹¶è¡Œ
        initialize_model_parallel(tensor_model_parallel_size=1)
        print(f"âœ“ vLLMæ¨¡å‹å¹¶è¡Œåˆå§‹åŒ–æˆåŠŸ")
        print(f"  å¼ é‡æ¨¡å‹å¹¶è¡Œå¤§å°: {get_tensor_model_parallel_world_size()}")
        print(f"  å¼ é‡æ¨¡å‹å¹¶è¡Œæ’å: {get_tensor_model_parallel_rank()}")
        return True
    except Exception as e:
        print(f"âœ— vLLMæ¨¡å‹å¹¶è¡Œåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
def test_weight_loading(sse_type='glah'):
    """æµ‹è¯•1ï¼šéªŒè¯vLLMæ¨¡å‹èƒ½å¦åŠ è½½HuggingFaceä¿å­˜çš„æƒé‡"""
    print(f"\n=== æµ‹è¯•1ï¼šæƒé‡åŠ è½½æµ‹è¯• ({sse_type.upper()}) ===")

    # å¯¼å…¥SSE-SWAæ¨¡å‹
    from sse_swa_moba_hf.sse_swa_hf import SSEGLAH as HFSSEGLAH, SSEGDNH as HFSSEGDNH
    from sse_swa_moba_vllm.sse_swa import VLLMSSEGLAH, VLLMSSEGDNH

    # æ ¹æ®ç±»å‹é€‰æ‹©æ¨¡å‹ç±»
    if sse_type.lower() == 'glah':
        HFModelClass = HFSSEGLAH
        VLLMModelClass = VLLMSSEGLAH
        model_name = "SSEGLAH"
    elif sse_type.lower() == 'gdnh':
        HFModelClass = HFSSEGDNH
        VLLMModelClass = VLLMSSEGDNH
        model_name = "SSEGDNH"
    else:
        raise ValueError(f"æœªçŸ¥çš„SSE-SWAç±»å‹: {sse_type}")

    # 1. åˆ›å»ºHuggingFaceæ¨¡å‹ - ä½¿ç”¨bfloat16
    hf_sse_swa = HFModelClass(
        hidden_size=512,
        expand_v=1.0,
        head_dim=64,
        num_heads=8,
        mode='chunk',
        use_output_gate=True,
        use_short_conv=False,
        num_sparse_partition=4,
        num_writer=1,
        num_reader=1,
        sse_implementation="varlen",
        use_q_softmax=False,
        use_k_softmax=True,
        emulq=True,
        emulk=True,
        qkv_bias=False,
        # SWA configs
        swa_num_kv_heads=8,
        swa_qk_norm=False,
        swa_dropout=0.5,
        window_size=64,
        rope_theta=10000.,
        max_position_embeddings=2048,
        layer_idx=0,
        norm_eps=1e-5
    ).to(device, dtype=torch.bfloat16)

    # æ‰‹åŠ¨åˆå§‹åŒ–æƒé‡
    hf_sse_swa.apply(manual_weight_initialization)
    print(f"âœ“ HuggingFace {model_name}æ¨¡å‹åˆ›å»ºå¹¶åˆå§‹åŒ–å®Œæˆ")

    # 2. ä¿å­˜HuggingFaceæƒé‡
    hf_weights = hf_sse_swa.state_dict()
    torch.save(hf_weights, f"hf_{sse_type}_weights.pth")
    print("âœ“ HuggingFaceæƒé‡ä¿å­˜å®Œæˆ")

    # 3. åˆ›å»ºvLLMæ¨¡å‹ - ä½¿ç”¨bfloat16
    if vllm_available:
        vllm_config = get_current_vllm_config()

        vllm_sse_swa = VLLMModelClass(
            vllm_config=vllm_config,
            prefix=f"model.layers.0.sse_swa_{sse_type}",
            hidden_size=512,
            expand_v=1.0,
            head_dim=64,
            num_heads=8,
            mode='chunk',
            use_output_gate=True,
            use_short_conv=False,
            num_sparse_partition=4,
            num_writer=1,
            num_reader=1,
            sse_implementation="varlen",
            use_q_softmax=False,
            use_k_softmax=True,
            emulq=True,
            emulk=True,
            qkv_bias=False,
            # SWA configs
            swa_num_kv_heads=8,
            swa_qk_norm=False,
            swa_dropout=0.5,
            window_size=64,
            rope_theta=10000.,
            max_position_embeddings=2048,
            layer_idx=0,
            norm_eps=1e-5
        ).to(device, dtype=torch.bfloat16)

        print(f"âœ“ vLLM {model_name}æ¨¡å‹åˆ›å»ºå®Œæˆ")
    else:
        print("âš ï¸ vLLMä¸å¯ç”¨ï¼Œè·³è¿‡vLLMæ¨¡å‹åˆ›å»º")
        return hf_sse_swa, None

    # 4. ä»HuggingFaceæƒé‡åŠ è½½åˆ°vLLMæ¨¡å‹
    try:
        # ä½¿ç”¨vLLMæ¨¡å‹çš„load_hf_weightsæ–¹æ³•åŠ è½½æƒé‡
        vllm_sse_swa.load_hf_weights(hf_weights)
        print(f"âœ“ vLLM {model_name}æ¨¡å‹æˆåŠŸåŠ è½½HuggingFaceæƒé‡")
    except Exception as e:
        print(f"âœ— æƒé‡åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

    # 5. è¯¦ç»†éªŒè¯æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½
    print("\n=== æƒé‡éªŒè¯è¯¦æƒ… ===")

    # éªŒè¯å…³é”®æŠ•å½±å±‚æƒé‡
    if vllm_available:
        # éªŒè¯SSEæŠ•å½±å±‚æƒé‡
        sse_proj_layers = ['sse_q_proj', 'sse_k_proj', 'sse_v_proj', 'sse_e_proj']
        for layer_name in sse_proj_layers:
            if hasattr(hf_sse_swa, layer_name) and hasattr(vllm_sse_swa, layer_name):
                if hasattr(getattr(hf_sse_swa, layer_name), 'weight') and hasattr(getattr(vllm_sse_swa, layer_name), 'weight'):
                    hf_weight = getattr(hf_sse_swa, layer_name).weight.data
                    vllm_weight = getattr(vllm_sse_swa, layer_name).weight.data
                    weight_diff = torch.max(torch.abs(vllm_weight - hf_weight))
                    print(f"{layer_name}æƒé‡æœ€å¤§å·®å¼‚: {weight_diff.item():.6f}")
                    assert weight_diff < 1e-6, f"{layer_name}æƒé‡ä¸åŒ¹é…"

        # éªŒè¯SWAæŠ•å½±å±‚æƒé‡
        swa_proj_layers = ['swa_q_proj', 'swa_k_proj', 'swa_v_proj', 'swa_o_proj']
        for layer_name in swa_proj_layers:
            if hasattr(hf_sse_swa, layer_name) and hasattr(vllm_sse_swa, layer_name):
                if hasattr(getattr(hf_sse_swa, layer_name), 'weight') and hasattr(getattr(vllm_sse_swa, layer_name), 'weight'):
                    hf_weight = getattr(hf_sse_swa, layer_name).weight.data
                    vllm_weight = getattr(vllm_sse_swa, layer_name).weight.data
                    weight_diff = torch.max(torch.abs(vllm_weight - hf_weight))
                    print(f"{layer_name}æƒé‡æœ€å¤§å·®å¼‚: {weight_diff.item():.6f}")
                    assert weight_diff < 1e-6, f"{layer_name}æƒé‡ä¸åŒ¹é…"

        # éªŒè¯LoRAæŠ•å½±å±‚æƒé‡
        lora_layers = ['lora_q_proj', 'lora_k_proj']
        for lora_name in lora_layers:
            if hasattr(hf_sse_swa, lora_name) and hasattr(vllm_sse_swa, lora_name):
                hf_lora = getattr(hf_sse_swa, lora_name)
                vllm_lora = getattr(vllm_sse_swa, lora_name)
                for i, (hf_sub, vllm_sub) in enumerate(zip(hf_lora, vllm_lora)):
                    if hasattr(hf_sub, 'weight') and hasattr(vllm_sub, 'weight'):
                        hf_weight = hf_sub.weight.data
                        vllm_weight = vllm_sub.weight.data
                        weight_diff = torch.max(torch.abs(vllm_weight - hf_weight))
                        print(f"{lora_name}.{i}æƒé‡æœ€å¤§å·®å¼‚: {weight_diff.item():.6f}")
                        assert weight_diff < 1e-6, f"{lora_name}.{i}æƒé‡ä¸åŒ¹é…"

        # éªŒè¯SSE-GDNHç‰¹å®šçš„æŠ•å½±å±‚ï¼ˆå¦‚æœæ˜¯GDNHç±»å‹ï¼‰
        if sse_type.lower() == 'gdnh':
            gdn_proj_layers = ['sse_a_proj', 'sse_b_proj']
            for layer_name in gdn_proj_layers:
                if hasattr(hf_sse_swa, layer_name) and hasattr(vllm_sse_swa, layer_name):
                    if hasattr(getattr(hf_sse_swa, layer_name), 'weight') and hasattr(getattr(vllm_sse_swa, layer_name), 'weight'):
                        hf_weight = getattr(hf_sse_swa, layer_name).weight.data
                        vllm_weight = getattr(vllm_sse_swa, layer_name).weight.data
                        weight_diff = torch.max(torch.abs(vllm_weight - hf_weight))
                        print(f"{layer_name}æƒé‡æœ€å¤§å·®å¼‚: {weight_diff.item():.6f}")
                        assert weight_diff < 1e-6, f"{layer_name}æƒé‡ä¸åŒ¹é…"

        print("âœ“ æ‰€æœ‰å…³é”®æƒé‡éªŒè¯é€šè¿‡")

    return hf_sse_swa, vllm_sse_swa
def test_forward_equivalence(hf_sse_swa, vllm_sse_swa, sse_type='glah'):
    """æµ‹è¯•2ï¼šéªŒè¯å‰å‘ä¼ æ’­è¾“å‡ºç­‰ä»·æ€§"""
    print(f"\n=== æµ‹è¯•2ï¼šå‰å‘ä¼ æ’­ç­‰ä»·æ€§æµ‹è¯• ({sse_type.upper()}) ===")

    if not vllm_sse_swa:
        print("âš ï¸ vLLMæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³å‰å‘ä¼ æ’­æµ‹è¯•")
        return

    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)
    np.random.seed(42)

    # æµ‹è¯•ä¸åŒçš„è¾“å…¥é…ç½®
    test_cases = [
        # (batch_size, seq_len, use_cu_seqlens, description)
        (1, 320, False, "å•æ ·æœ¬ï¼Œå›ºå®šé•¿åº¦"),
        (2, 320, False, "å¤šæ ·æœ¬ï¼Œå›ºå®šé•¿åº¦"),
        (2, 320, True, "å¤šæ ·æœ¬ï¼Œå˜é•¿åºåˆ—(cu_seqlens)"),
    ]

    for batch_size, seq_len, use_cu_seqlens, description in test_cases:
        print(f"\næµ‹è¯•ç”¨ä¾‹: {description}")

        # åˆ›å»ºè¾“å…¥æ•°æ® - ä½¿ç”¨bfloat16
        hidden_states = torch.randn(batch_size, seq_len, hf_sse_swa.hidden_size, dtype=torch.bfloat16).to(device)

        # åˆ›å»ºæ³¨æ„åŠ›æ©ç ï¼ˆç”¨äºå˜é•¿åºåˆ—ï¼‰
        attention_mask = None
        cu_seqlens = None
        actual_seq_lens = None

        if use_cu_seqlens:
            # åˆ›å»ºå˜é•¿åºåˆ—é•¿åº¦ï¼ˆæ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„åºåˆ—ï¼‰
            actual_seq_lens = [seq_len - i for i in range(batch_size)]
            max_len = max(actual_seq_lens)

            # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
            attention_mask = torch.zeros(batch_size, max_len).to(device)
            for i, length in enumerate(actual_seq_lens):
                attention_mask[i, :length] = 1

            # åˆ›å»ºcu_seqlens
            cu_seqlens = torch.zeros(batch_size + 1, dtype=torch.int32).to(device)
            for i in range(batch_size):
                cu_seqlens[i+1] = cu_seqlens[i] + actual_seq_lens[i]

            # è°ƒæ•´è¾“å…¥åºåˆ—é•¿åº¦
            hidden_states = hidden_states[:, :max_len, :]
        else:
            # å›ºå®šé•¿åº¦åºåˆ—
            attention_mask = torch.ones(batch_size, seq_len).to(device)

        # HuggingFaceå‰å‘ä¼ æ’­
        hf_output, hf_aux_loss, _ = hf_sse_swa(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            cu_seqlens=cu_seqlens if use_cu_seqlens else None,
            output_attentions=False,
            use_cache=False
        )

        # vLLMå‰å‘ä¼ æ’­
        # vLLMéœ€è¦positionså‚æ•°
        positions = torch.arange(0, hidden_states.shape[1], device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)

        # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
        if vllm_available:
            vllm_config = get_current_vllm_config()
            attn_metadata = create_attention_metadata(
                batch_size=batch_size,
                seq_len=hidden_states.shape[1],
                use_cu_seqlens=use_cu_seqlens,
                seq_lens=actual_seq_lens
            )

            with set_forward_context(
                attn_metadata=attn_metadata,
                vllm_config=vllm_config,
                num_tokens=attn_metadata["num_actual_tokens"],
                cudagraph_runtime_mode=CUDAGraphMode.NONE
            ):
                vllm_output, vllm_aux_loss, _ = vllm_sse_swa(
                    hidden_states=hidden_states,
                    positions=positions,
                    attention_mask=attention_mask,
                    output_attentions=False,
                    use_cache=False
                )
        else:
            # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
            vllm_output, vllm_aux_loss, _ = vllm_sse_swa(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                output_attentions=False,
                use_cache=False
            )

        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        assert hf_output.shape == vllm_output.shape, f"å‰å‘è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {hf_output.shape} vs {vllm_output.shape}"

        # æ£€æŸ¥è¾“å‡ºå€¼
        max_diff = torch.max(torch.abs(hf_output - vllm_output))
        print(f"å‰å‘è¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")

        if max_diff < 1e-5:
            print(f"âœ“ {description} å‰å‘ä¼ æ’­éªŒè¯é€šè¿‡")
        else:
            print(f"âœ— {description} å‰å‘ä¼ æ’­è¾“å‡ºä¸ç­‰ä»·")
            raise AssertionError(f"å‰å‘è¾“å‡ºä¸ç­‰ä»·")
def test_inference_prefill(hf_sse_swa: nn.Module, vllm_sse_swa: nn.Module, sse_type='glah'):
    """æµ‹è¯•3ï¼šéªŒè¯æ¨ç†prefillé˜¶æ®µ"""
    print(f"\n=== æµ‹è¯•3ï¼šæ¨ç†Prefillé˜¶æ®µæµ‹è¯• ({sse_type.upper()}) ===")
    hf_sse_swa.eval()
    if vllm_sse_swa is not None:
        vllm_sse_swa.eval()

    if not vllm_sse_swa:
        print("âš ï¸ vLLMæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡prefillæµ‹è¯•")
        return None, None

    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)

    # åˆ›å»ºæµ‹è¯•è¾“å…¥ - ä½¿ç”¨bfloat16
    batch_size = 2
    seq_len = 320
    hidden_states = torch.randn(batch_size, seq_len, hf_sse_swa.hidden_size, dtype=torch.bfloat16).to(device)
    attention_mask = torch.ones(batch_size, seq_len).to(device)

    # ====================
    # æ­£ç¡®åˆå§‹åŒ–past_key_values
    # ====================

    # 1. HuggingFace prefill - æ­£ç¡®åˆå§‹åŒ–past_key_values
    # æ ¹æ®fla.models.utils.Cacheçš„å®šä¹‰ï¼Œåº”è¯¥ä½¿ç”¨Cacheç±»åˆå§‹åŒ–ï¼Œè€Œä¸æ˜¯None
    # Cacheç±»éœ€è¦seen_tokenså‚æ•°ï¼ˆåˆå§‹ä¸º0ï¼‰
    hf_past_key_values = Cache(seen_tokens=0)  # æ­£ç¡®åˆå§‹åŒ–Cacheå¯¹è±¡
    print(f"âœ“ HuggingFace past_key_valuesåˆå§‹åŒ–ä¸ºCacheå¯¹è±¡ï¼Œç±»å‹: {type(hf_past_key_values)}")

    hf_output, hf_aux_loss, hf_past = hf_sse_swa(
        hidden_states=hidden_states,
        attention_mask=attention_mask,
        past_key_values=hf_past_key_values,  # ä¼ é€’æ­£ç¡®åˆå§‹åŒ–çš„Cacheå¯¹è±¡
        output_attentions=False,
        use_cache=True
    )

    # 2. vLLM prefill - åˆå§‹åŒ–past_key_values
    # vLLMä½¿ç”¨å­—å…¸æ ¼å¼
    positions = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)
    vllm_past_key_values = {}  # vLLMä½¿ç”¨å­—å…¸æ ¼å¼
    print(f"âœ“ vLLM past_key_valuesåˆå§‹åŒ–ä¸ºå­—å…¸ï¼Œç±»å‹: {type(vllm_past_key_values)}")

    # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
    if vllm_available:
        vllm_config = get_current_vllm_config()
        attn_metadata = create_attention_metadata(
            batch_size=batch_size,
            seq_len=seq_len,
            use_cu_seqlens=False
        )

        with set_forward_context(
            attn_metadata=attn_metadata,
            vllm_config=vllm_config,
            num_tokens=attn_metadata["num_actual_tokens"],
            cudagraph_runtime_mode=CUDAGraphMode.NONE
        ):
            vllm_output, vllm_aux_loss, vllm_past = vllm_sse_swa(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                past_key_values=vllm_past_key_values,  # ä¼ é€’åˆå§‹çš„å­—å…¸
                output_attentions=False,
                use_cache=True
            )
    else:
        # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
        vllm_output, vllm_aux_loss, vllm_past = vllm_sse_swa(
            hidden_states=hidden_states,
            positions=positions,
            attention_mask=attention_mask,
            past_key_values=vllm_past_key_values,  # ä¼ é€’åˆå§‹çš„å­—å…¸
            output_attentions=False,
            use_cache=True
        )

    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert hf_output.shape == vllm_output.shape, f"Prefillè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {hf_output.shape} vs {vllm_output.shape}"
    print(f"âœ“ Prefillè¾“å‡ºå½¢çŠ¶åŒ¹é…: {hf_output.shape}")

    # æ£€æŸ¥è¾“å‡ºå€¼
    max_diff = torch.max(torch.abs(hf_output - vllm_output))
    print(f"Prefillè¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")

    if max_diff < 1e-5:
        print(f"âœ“ {sse_type.upper()} Prefillé˜¶æ®µéªŒè¯é€šè¿‡")
    else:
        print(f"âœ— {sse_type.upper()} Prefillé˜¶æ®µè¾“å‡ºä¸ç­‰ä»·")
        raise AssertionError(f"Prefillè¾“å‡ºä¸ç­‰ä»·ï¼Œæœ€å¤§å·®å¼‚: {max_diff.item()}")

    # éªŒè¯pastçŠ¶æ€ä¸ä¸ºNone
    assert hf_past is not None, "HuggingFace pastçŠ¶æ€ä¸ºNone"
    assert vllm_past is not None, "vLLM pastçŠ¶æ€ä¸ºNone"
    print(f"âœ“ Prefillé˜¶æ®µpastçŠ¶æ€éªŒè¯é€šè¿‡")
    print(f"  HuggingFace pastç±»å‹: {type(hf_past)}, é•¿åº¦: {len(hf_past) if hasattr(hf_past, '__len__') else 'N/A'}")
    print(f"  vLLM pastç±»å‹: {type(vllm_past)}, é”®æ•°é‡: {len(vllm_past) if isinstance(vllm_past, dict) else 'N/A'}")

    return hf_past, vllm_past
def test_inference_decode(hf_sse_swa, vllm_sse_swa, hf_past, vllm_past, sse_type='glah'):
    """æµ‹è¯•4ï¼šéªŒè¯æ¨ç†decodeé˜¶æ®µ"""
    print(f"\n=== æµ‹è¯•4ï¼šæ¨ç†Decodeé˜¶æ®µæµ‹è¯• ({sse_type.upper()}) ===")

    # æ”¹è¿›çš„æ£€æŸ¥é€»è¾‘ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    if not vllm_sse_swa:
        print("âš ï¸ vLLMæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡decodeæµ‹è¯•")
        return
    if hf_past is None:
        print("âš ï¸ HuggingFace pastçŠ¶æ€ä¸ºNoneï¼Œè·³è¿‡decodeæµ‹è¯•")
        return
    if vllm_past is None:
        print("âš ï¸ vLLM pastçŠ¶æ€ä¸ºNoneï¼Œè·³è¿‡decodeæµ‹è¯•")
        return

    # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    hf_sse_swa.eval()
    vllm_sse_swa.eval()

    batch_size = 2
    decode_steps = 5

    for step in range(decode_steps):
        print(f"\nDecode step {step+1}:")

        # åˆ›å»ºdecodeè¾“å…¥ï¼ˆå•tokenï¼‰- ä½¿ç”¨bfloat16
        hidden_states = torch.randn(batch_size, 1, hf_sse_swa.hidden_size, dtype=torch.bfloat16).to(device)

        # HuggingFace decode
        hf_output, hf_aux_loss, hf_past = hf_sse_swa(
            hidden_states=hidden_states,
            past_key_values=hf_past,
            output_attentions=False,
            use_cache=True
        )

        # vLLM decode
        # vLLMéœ€è¦positionså‚æ•°ï¼Œè¿™é‡Œä½¿ç”¨ä¹‹å‰çš„ä½ç½®+1
        positions = torch.full((batch_size, 1), 320 + step, device=hidden_states.device)

        # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
        if vllm_available:
            vllm_config = get_current_vllm_config()
            # åˆ›å»ºdecodeé˜¶æ®µçš„attention metadata
            attn_metadata = create_attention_metadata(
                batch_size=batch_size,
                seq_len=1,  # decodeé˜¶æ®µæ¯æ¬¡å¤„ç†1ä¸ªtoken
                use_cu_seqlens=False
            )

            # æ›´æ–°decodeç›¸å…³çš„å…ƒæ•°æ®
            attn_metadata.update({
                "num_actual_tokens": batch_size * 1,  # decodeé˜¶æ®µæ¯æ¬¡å¤„ç†1ä¸ªtoken
                "num_prefill_tokens": 0,
                "num_decode_tokens": batch_size * 1,
                "num_prefills": 0,
                "num_decodes": batch_size
            })

            with set_forward_context(
                attn_metadata=attn_metadata,
                vllm_config=vllm_config,
                num_tokens=attn_metadata["num_actual_tokens"],
                cudagraph_runtime_mode=CUDAGraphMode.NONE
            ):
                vllm_output, vllm_aux_loss, vllm_past = vllm_sse_swa(
                    hidden_states=hidden_states,
                    positions=positions,
                    past_key_values=vllm_past,
                    output_attentions=False,
                    use_cache=True
                )
        else:
            # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
            vllm_output, vllm_aux_loss, vllm_past = vllm_sse_swa(
                hidden_states=hidden_states,
                positions=positions,
                past_key_values=vllm_past,
                output_attentions=False,
                use_cache=True
            )

        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        assert hf_output.shape == vllm_output.shape, f"Decodeè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"

        # æ£€æŸ¥è¾“å‡ºå€¼
        max_diff = torch.max(torch.abs(hf_output - vllm_output))
        print(f"  è¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")

        if max_diff < 1e-5:
            print(f"  âœ“ {sse_type.upper()} Decode step {step+1} éªŒè¯é€šè¿‡")
        else:
            print(f"  âœ— {sse_type.upper()} Decode step {step+1} è¾“å‡ºä¸ç­‰ä»·")
            raise AssertionError(f"Decodeè¾“å‡ºä¸ç­‰ä»·")
def test_training_mode(hf_sse_swa, vllm_sse_swa, sse_type='glah'):
    """æµ‹è¯•5ï¼šéªŒè¯è®­ç»ƒæ¨¡å¼"""
    print(f"\n=== æµ‹è¯•5ï¼šè®­ç»ƒæ¨¡å¼æµ‹è¯• ({sse_type.upper()}) ===")

    if not vllm_sse_swa:
        print("âš ï¸ vLLMæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒæ¨¡å¼æµ‹è¯•")
        return

    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    hf_sse_swa.train()
    vllm_sse_swa.train()

    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)

    # åˆ›å»ºè¾“å…¥æ•°æ® - ä½¿ç”¨bfloat16
    batch_size = 2
    seq_len = 320
    hidden_states = torch.randn(batch_size, seq_len, hf_sse_swa.hidden_size, dtype=torch.bfloat16).to(device)
    attention_mask = torch.ones(batch_size, seq_len).to(device)

    # æ·»åŠ æ¢¯åº¦è®¡ç®—
    hidden_states.requires_grad_(True)

    # HuggingFaceè®­ç»ƒå‰å‘
    hf_output, hf_aux_loss, _ = hf_sse_swa(
        hidden_states=hidden_states,
        attention_mask=attention_mask,
        output_attentions=False,
        use_cache=False
    )

    # vLLMè®­ç»ƒå‰å‘
    positions = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0).repeat(batch_size, 1)

    # è·å–vllmé…ç½®å¹¶è®¾ç½®forward_context
    if vllm_available:
        vllm_config = get_current_vllm_config()
        attn_metadata = create_attention_metadata(
            batch_size=batch_size,
            seq_len=seq_len,
            use_cu_seqlens=False
        )

        with set_forward_context(
            attn_metadata=attn_metadata,
            vllm_config=vllm_config,
            num_tokens=attn_metadata["num_actual_tokens"],
            cudagraph_runtime_mode=CUDAGraphMode.NONE
        ):
            vllm_output, vllm_aux_loss, _ = vllm_sse_swa(
                hidden_states=hidden_states,
                positions=positions,
                attention_mask=attention_mask,
                output_attentions=False,
                use_cache=False
            )
    else:
        # å¦‚æœvLLMä¸å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
        vllm_output, vllm_aux_loss, _ = vllm_sse_swa(
            hidden_states=hidden_states,
            positions=positions,
            attention_mask=attention_mask,
            output_attentions=False,
            use_cache=False
        )

    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert hf_output.shape == vllm_output.shape, f"è®­ç»ƒæ¨¡å¼è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"

    # æ£€æŸ¥è¾“å‡ºå€¼
    max_diff = torch.max(torch.abs(hf_output - vllm_output))
    print(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºæœ€å¤§å·®å¼‚: {max_diff.item():.6f}")

    if max_diff < 1e-5:
        print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼å‰å‘éªŒè¯é€šè¿‡")
    else:
        print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼è¾“å‡ºä¸ç­‰ä»·")
        raise AssertionError(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºä¸ç­‰ä»·")

    # æµ‹è¯•åå‘ä¼ æ’­
    print("\næµ‹è¯•åå‘ä¼ æ’­...")

    # HuggingFaceåå‘ä¼ æ’­
    hf_loss = hf_output.sum()
    if hf_aux_loss and len(hf_aux_loss) > 1 and hf_aux_loss[1] is not None:
        hf_loss += hf_aux_loss[1]
    hf_loss.backward(retain_graph=True)

    # vLLMåå‘ä¼ æ’­
    vllm_loss = vllm_output.sum()
    if vllm_aux_loss is not None:
        vllm_loss += vllm_aux_loss
    vllm_loss.backward()

    # æ£€æŸ¥æ¢¯åº¦ï¼ˆä»…æ£€æŸ¥éƒ¨åˆ†å…³é”®å‚æ•°ï¼‰
    print("\næ¢¯åº¦æ£€æŸ¥:")

    # æ¯”è¾ƒSSEæŠ•å½±å±‚æƒé‡çš„æ¢¯åº¦
    proj_layers = ['sse_q_proj', 'sse_k_proj', 'sse_v_proj', 'swa_q_proj', 'swa_k_proj', 'swa_v_proj']
    found = False

    for layer_name in proj_layers:
        if hasattr(hf_sse_swa, layer_name) and hasattr(getattr(hf_sse_swa, layer_name), 'weight') and \
           hasattr(vllm_sse_swa, layer_name) and hasattr(getattr(vllm_sse_swa, layer_name), 'weight'):

            found = True
            hf_grad = getattr(hf_sse_swa, layer_name).weight.grad
            vllm_grad = getattr(vllm_sse_swa, layer_name).weight.grad

            if hf_grad is not None and vllm_grad is not None:
                grad_max_diff = torch.max(torch.abs(hf_grad - vllm_grad))
                print(f"{layer_name}æ¢¯åº¦æœ€å¤§å·®å¼‚: {grad_max_diff.item():.6f}")

                if grad_max_diff < 1e-5:
                    print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­éªŒè¯é€šè¿‡")
                else:
                    print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­ä¸ç­‰ä»·")
                    raise AssertionError(f"è®­ç»ƒæ¨¡å¼åå‘ä¼ æ’­ä¸ç­‰ä»·")
            else:
                print(f"âš ï¸ {layer_name}æ¢¯åº¦ä¸ºNoneï¼Œæ— æ³•æ¯”è¾ƒ")
            break

    if not found:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯æ¯”è¾ƒçš„æŠ•å½±å±‚ï¼Œä»…éªŒè¯è¾“å‡ºç­‰ä»·æ€§")
        if max_diff < 1e-5:
            print(f"âœ“ {sse_type.upper()} è®­ç»ƒæ¨¡å¼éªŒè¯é€šè¿‡ï¼ˆä»…éªŒè¯è¾“å‡ºç­‰ä»·æ€§ï¼‰")
        else:
            print(f"âœ— {sse_type.upper()} è®­ç»ƒæ¨¡å¼éªŒè¯å¤±è´¥")
            raise AssertionError(f"è®­ç»ƒæ¨¡å¼ä¸ç­‰ä»·")
def test_sse_swa_type(sse_type):
    """æµ‹è¯•ç‰¹å®šç±»å‹çš„SSE-SWA"""
    print(f"\n" + "="*70)
    print(f"å¼€å§‹æµ‹è¯• {sse_type.upper()} ç±»å‹çš„SSE-SWAå±‚")
    print("="*70)

    # æµ‹è¯•1ï¼šæƒé‡åŠ è½½
    hf_sse_swa, vllm_sse_swa = test_weight_loading(sse_type)

    if not torch.cuda.is_available():
        print(f"\nâš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œè·³è¿‡{sse_type.upper()}çš„å…¶ä»–æµ‹è¯•")
        return

    # æµ‹è¯•2ï¼šå‰å‘ä¼ æ’­ç­‰ä»·æ€§
    test_forward_equivalence(hf_sse_swa, vllm_sse_swa, sse_type)

    # æµ‹è¯•3ï¼šæ¨ç†prefillé˜¶æ®µ
    hf_past, vllm_past = test_inference_prefill(hf_sse_swa, vllm_sse_swa, sse_type)

    # æµ‹è¯•4ï¼šæ¨ç†decodeé˜¶æ®µ
    test_inference_decode(hf_sse_swa, vllm_sse_swa, hf_past, vllm_past, sse_type)

    # æµ‹è¯•5ï¼šè®­ç»ƒæ¨¡å¼
    test_training_mode(hf_sse_swa, vllm_sse_swa, sse_type)

    print(f"\n" + "="*70)
    print(f"ğŸ‰ {sse_type.upper()} ç±»å‹çš„SSE-SWAå±‚æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("="*70)
def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*70)
    print("SSE-SWA (Sparse State Expansion with Sliding Window Attention) å±‚ç­‰ä»·æ€§æµ‹è¯•")
    print("æ”¯æŒ SSEGLAH å’Œ SSEGDNH ä¸¤ç§ç±»å‹")
    print("="*70)

    try:
        # åˆå§‹åŒ–vLLMåˆ†å¸ƒå¼ç¯å¢ƒ
        if vllm_available:
            if initialize_vllm_distributed():
                # åˆå§‹åŒ–model parallel
                initialize_vllm_model_parallel()

        # æµ‹è¯•SSEGLAH
        test_sse_swa_type('glah')

        # æµ‹è¯•SSEGDNH
        test_sse_swa_type('gdnh')

        print("\n" + "="*70)
        print("ğŸ‰ æ‰€æœ‰SSE-SWAç±»å‹çš„æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        print("âœ“ SSEGLAH å’Œ SSEGDNH å®ç°å®Œå…¨ç­‰ä»·")
        print("="*70)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for sse_type in ['glah', 'gdnh']:
            weight_file = f"hf_{sse_type}_weights.pth"
            if os.path.exists(weight_file):
                os.remove(weight_file)
        print("\nâœ“ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

        # æ¸…ç†vLLMåˆ†å¸ƒå¼ç¯å¢ƒ
        if vllm_available:
            try:
                destroy_model_parallel()
                print("âœ“ vLLMæ¨¡å‹å¹¶è¡Œç¯å¢ƒå·²æ¸…ç†")
            except:
                pass
if __name__ == "__main__":
    main()